@article{Ahneman_2018,
  title     = {Predicting reaction performance in C–N cross-coupling using machine learning},
  volume    = {360},
  issn      = {1095-9203},
  url       = {http://dx.doi.org/10.1126/science.aar5169},
  doi       = {10.1126/science.aar5169},
  number    = {6385},
  journal   = {Science},
  publisher = {American Association for the Advancement of Science (AAAS)},
  author    = {Ahneman, Derek T. and Estrada, Jesús G. and Lin, Shishi and Dreher, Spencer D. and Doyle, Abigail G.},
  year      = {2018},
  month     = apr,
  pages     = {186–190}
}
@article{Chuang_2018,
  title     = {Comment on “Predicting reaction performance in C–N cross-coupling using machine learning”},
  volume    = {362},
  issn      = {1095-9203},
  url       = {http://dx.doi.org/10.1126/science.aat8603},
  doi       = {10.1126/science.aat8603},
  number    = {6416},
  journal   = {Science},
  publisher = {American Association for the Advancement of Science (AAAS)},
  author    = {Chuang, Kangway V. and Keiser, Michael J.},
  year      = {2018},
  month     = nov
}
@misc{vinyals2016order,
  title         = {Order Matters: Sequence to sequence for sets},
  author        = {Oriol Vinyals and Samy Bengio and Manjunath Kudlur},
  year          = {2016},
  eprint        = {1511.06391},
  archiveprefix = {arXiv},
  primaryclass  = {stat.ML}
}
@misc{shazeer2017outrageously,
  title         = {Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer},
  author        = {Noam Shazeer and Azalia Mirhoseini and Krzysztof Maziarz and Andy Davis and Quoc Le and Geoffrey Hinton and Jeff Dean},
  year          = {2017},
  eprint        = {1701.06538},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}
@misc{jiang2024mixtral,
  title         = {Mixtral of Experts},
  author        = {Albert Q. Jiang and Alexandre Sablayrolles and Antoine Roux and Arthur Mensch and Blanche Savary and Chris Bamford and Devendra Singh Chaplot and Diego de las Casas and Emma Bou Hanna and Florian Bressand and Gianna Lengyel and Guillaume Bour and Guillaume Lample and Lélio Renard Lavaud and Lucile Saulnier and Marie-Anne Lachaux and Pierre Stock and Sandeep Subramanian and Sophia Yang and Szymon Antoniak and Teven Le Scao and Théophile Gervet and Thibaut Lavril and Thomas Wang and Timothée Lacroix and William El Sayed},
  year          = {2024},
  eprint        = {2401.04088},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}
@misc{kingma2015variational,
  title         = {Variational Dropout and the Local Reparameterization Trick},
  author        = {Diederik P. Kingma and Tim Salimans and Max Welling},
  year          = {2015},
  eprint        = {1506.02557},
  archiveprefix = {arXiv},
  primaryclass  = {stat.ML}
}
@article{Chen_2024,
  title     = {Uncertainty-Aware Yield Prediction with Multimodal Molecular Features},
  volume    = {38},
  issn      = {2159-5399},
  url       = {http://dx.doi.org/10.1609/aaai.v38i8.28668},
  doi       = {10.1609/aaai.v38i8.28668},
  number    = {8},
  journal   = {Proceedings of the AAAI Conference on Artificial Intelligence},
  publisher = {Association for the Advancement of Artificial Intelligence (AAAI)},
  author    = {Chen, Jiayuan and Guo, Kehan and Liu, Zhen and Isayev, Olexandr and Zhang, Xiangliang},
  year      = {2024},
  month     = mar,
  pages     = {8274–8282}
}

@article{schwaller2020data,
  title   = {Data augmentation strategies to improve reaction yield predictions and estimate uncertainty},
  author  = {Schwaller, Philippe and Vaucher, Alain C and Laino, Teodoro and Reymond, Jean-Louis},
  journal = {Chemrxiv preprint},
  year    = {2020}
}
@article{schwaller2021prediction,
  title     = {Prediction of chemical reaction yields using deep learning},
  author    = {Schwaller, Philippe and Vaucher, Alain C and Laino, Teodoro and Reymond, Jean-Louis},
  journal   = {Machine learning: science and technology},
  volume    = {2},
  number    = {1},
  pages     = {015016},
  year      = {2021},
  publisher = {IOP Publishing}
}

@article{Kwon_2022,
  title     = {Uncertainty-aware prediction of chemical reaction yields with graph neural networks},
  volume    = {14},
  issn      = {1758-2946},
  url       = {http://dx.doi.org/10.1186/s13321-021-00579-z},
  doi       = {10.1186/s13321-021-00579-z},
  number    = {1},
  journal   = {Journal of Cheminformatics},
  publisher = {Springer Science and Business Media LLC},
  author    = {Kwon, Youngchun and Lee, Dongseon and Choi, Youn-Suk and Kang, Seokho},
  year      = {2022},
  month     = jan
}
@inproceedings{gal2016dropout,
  title        = {Dropout as a bayesian approximation: Representing model uncertainty in deep learning},
  author       = {Gal, Yarin and Ghahramani, Zoubin},
  booktitle    = {international conference on machine learning},
  pages        = {1050--1059},
  year         = {2016},
  organization = {PMLR}
}

@article{dagdelen_structured_2024,
  title     = {Structured information extraction from scientific text with large language models},
  volume    = {15},
  copyright = {2024 The Author(s)},
  issn      = {2041-1723},
  url       = {https://www.nature.com/articles/s41467-024-45563-x},
  doi       = {10.1038/s41467-024-45563-x},
  abstract  = {Extracting structured knowledge from scientific text remains a challenging task for machine learning models. Here, we present a simple approach to joint named entity recognition and relation extraction and demonstrate how pretrained large language models (GPT-3, Llama-2) can be fine-tuned to extract useful records of complex scientific knowledge. We test three representative tasks in materials chemistry: linking dopants and host materials, cataloging metal-organic frameworks, and general composition/phase/morphology/application information extraction. Records are extracted from single sentences or entire paragraphs, and the output can be returned as simple English sentences or a more structured format such as a list of JSON objects. This approach represents a simple, accessible, and highly flexible route to obtaining large databases of structured specialized scientific knowledge extracted from research papers.},
  language  = {en},
  number    = {1},
  urldate   = {2024-04-13},
  journal   = {Nature Communications},
  author    = {Dagdelen, John and Dunn, Alexander and Lee, Sanghoon and Walker, Nicholas and Rosen, Andrew S. and Ceder, Gerbrand and Persson, Kristin A. and Jain, Anubhav},
  month     = feb,
  year      = {2024},
  note      = {Publisher: Nature Publishing Group},
  keywords  = {Materials science, Theory and computation, Databases, Scientific data},
  pages     = {1418},
  file      = {Dagdelen et al_2024_Structured information extraction from scientific text with large language.pdf:C\:\\Users\\pepem\\Zotero\\storage\\IQJG3VH8\\Dagdelen et al_2024_Structured information extraction from scientific text with large language.pdf:application/pdf}
}

@article{Trewartha2022,
  title     = {Quantifying the advantage of domain-specific pre-training on named entity recognition tasks in materials science},
  volume    = {3},
  issn      = {2666-3899},
  url       = {http://dx.doi.org/10.1016/j.patter.2022.100488},
  doi       = {10.1016/j.patter.2022.100488},
  number    = {4},
  journal   = {Patterns},
  publisher = {Elsevier BV},
  author    = {Trewartha,  Amalie and Walker,  Nicholas and Huo,  Haoyan and Lee,  Sanghoon and Cruse,  Kevin and Dagdelen,  John and Dunn,  Alexander and Persson,  Kristin A. and Ceder,  Gerbrand and Jain,  Anubhav},
  year      = {2022},
  month     = apr,
  pages     = {100488}
}

@article{Guo2021,
  title     = {Automated Chemical Reaction Extraction from Scientific Literature},
  volume    = {62},
  issn      = {1549-960X},
  url       = {http://dx.doi.org/10.1021/acs.jcim.1c00284},
  doi       = {10.1021/acs.jcim.1c00284},
  number    = {9},
  journal   = {Journal of Chemical Information and Modeling},
  publisher = {American Chemical Society (ACS)},
  author    = {Guo,  Jiang and Ibanez-Lopez,  A. Santiago and Gao,  Hanyu and Quach,  Victor and Coley,  Connor W. and Jensen,  Klavs F. and Barzilay,  Regina},
  year      = {2021},
  month     = jun,
  pages     = {2035–2045}
}

@article{Kim2017,
  title     = {Materials Synthesis Insights from Scientific Literature via Text Extraction and Machine Learning},
  volume    = {29},
  issn      = {1520-5002},
  url       = {http://dx.doi.org/10.1021/acs.chemmater.7b03500},
  doi       = {10.1021/acs.chemmater.7b03500},
  number    = {21},
  journal   = {Chemistry of Materials},
  publisher = {American Chemical Society (ACS)},
  author    = {Kim,  Edward and Huang,  Kevin and Saunders,  Adam and McCallum,  Andrew and Ceder,  Gerbrand and Olivetti,  Elsa},
  year      = {2017},
  month     = oct,
  pages     = {9436–9444}
}

@misc{mysore2019materials,
  title         = {The Materials Science Procedural Text Corpus: Annotating Materials Synthesis Procedures with Shallow Semantic Structures},
  author        = {Sheshera Mysore and Zach Jensen and Edward Kim and Kevin Huang and Haw-Shiuan Chang and Emma Strubell and Jeffrey Flanigan and Andrew McCallum and Elsa Olivetti},
  year          = {2019},
  eprint        = {1905.06939},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@article{Kim2020,
  title     = {Inorganic Materials Synthesis Planning with Literature-Trained Neural Networks},
  volume    = {60},
  issn      = {1549-960X},
  url       = {http://dx.doi.org/10.1021/acs.jcim.9b00995},
  doi       = {10.1021/acs.jcim.9b00995},
  number    = {3},
  journal   = {Journal of Chemical Information and Modeling},
  publisher = {American Chemical Society (ACS)},
  author    = {Kim,  Edward and Jensen,  Zach and van Grootel,  Alexander and Huang,  Kevin and Staib,  Matthew and Mysore,  Sheshera and Chang,  Haw-Shiuan and Strubell,  Emma and McCallum,  Andrew and Jegelka,  Stefanie and Olivetti,  Elsa},
  year      = {2020},
  month     = jan,
  pages     = {1194–1201}
}

@article{Kononova2019,
  title     = {Text-mined dataset of inorganic materials synthesis recipes},
  volume    = {6},
  issn      = {2052-4463},
  url       = {http://dx.doi.org/10.1038/s41597-019-0224-1},
  doi       = {10.1038/s41597-019-0224-1},
  number    = {1},
  journal   = {Scientific Data},
  publisher = {Springer Science and Business Media LLC},
  author    = {Kononova,  Olga and Huo,  Haoyan and He,  Tanjin and Rong,  Ziqin and Botari,  Tiago and Sun,  Wenhao and Tshitoyan,  Vahe and Ceder,  Gerbrand},
  year      = {2019},
  month     = oct
}

@article{Huo2019,
  title     = {Semi-supervised machine-learning classification of materials synthesis procedures},
  volume    = {5},
  issn      = {2057-3960},
  url       = {http://dx.doi.org/10.1038/s41524-019-0204-1},
  doi       = {10.1038/s41524-019-0204-1},
  number    = {1},
  journal   = {npj Computational Materials},
  publisher = {Springer Science and Business Media LLC},
  author    = {Huo,  Haoyan and Rong,  Ziqin and Kononova,  Olga and Sun,  Wenhao and Botari,  Tiago and He,  Tanjin and Tshitoyan,  Vahe and Ceder,  Gerbrand},
  year      = {2019},
  month     = jul
}

@article{Swain2016,
  title     = {ChemDataExtractor: A Toolkit for Automated Extraction of Chemical Information from the Scientific Literature},
  volume    = {56},
  issn      = {1549-960X},
  url       = {http://dx.doi.org/10.1021/acs.jcim.6b00207},
  doi       = {10.1021/acs.jcim.6b00207},
  number    = {10},
  journal   = {Journal of Chemical Information and Modeling},
  publisher = {American Chemical Society (ACS)},
  author    = {Swain,  Matthew C. and Cole,  Jacqueline M.},
  year      = {2016},
  month     = oct,
  pages     = {1894–1904}
}

@article{Mavrai2021,
  title     = {ChemDataExtractor 2.0: Autopopulated Ontologies for Materials Science},
  volume    = {61},
  issn      = {1549-960X},
  url       = {http://dx.doi.org/10.1021/acs.jcim.1c00446},
  doi       = {10.1021/acs.jcim.1c00446},
  number    = {9},
  journal   = {Journal of Chemical Information and Modeling},
  publisher = {American Chemical Society (ACS)},
  author    = {Mavračić,  Juraj and Court,  Callum J. and Isazawa,  Taketomo and Elliott,  Stephen R. and Cole,  Jacqueline M.},
  year      = {2021},
  month     = sep,
  pages     = {4280–4289}
}

@article{Nugmanov2024,
  title     = {PaCh (Packed Chemicals): Computationally Effective Binary Format for Chemical Structure Encoding},
  volume    = {64},
  issn      = {1549-9596},
  url       = {https://doi.org/10.1021/acs.jcim.3c01720},
  doi       = {10.1021/acs.jcim.3c01720},
  number    = {8},
  journal   = {Journal of Chemical Information and Modeling},
  publisher = {American Chemical Society (ACS)},
  author    = {Nugmanov, Ramil},
  year      = {2024},
  month     = mar,
  pages     = {3173-3179}
}

@article{Tran2017,
  author   = {Ngoc Hieu Tran  and Xianglilan Zhang  and Lei Xin  and Baozhen Shan  and Ming Li },
  title    = {De novo peptide sequencing by deep learning},
  journal  = {Proceedings of the National Academy of Sciences},
  volume   = {114},
  number   = {31},
  pages    = {8247-8252},
  year     = {2017},
  doi      = {10.1073/pnas.1705691114},
  url      = {https://www.pnas.org/doi/abs/10.1073/pnas.1705691114},
  eprint   = {https://www.pnas.org/doi/pdf/10.1073/pnas.1705691114},
  abstract = {De novo peptide sequencing from tandem MS data is the key technology in proteomics for the characterization of proteins, especially for new sequences, such as mAbs. In this study, we propose a deep neural network model, DeepNovo, for de novo peptide sequencing. DeepNovo architecture combines recent advances in convolutional neural networks and recurrent neural networks to learn features of tandem mass spectra, fragment ions, and sequence patterns of peptides. The networks are further integrated with local dynamic programming to solve the complex optimization task of de novo sequencing. We evaluated the method on a wide variety of species and found that DeepNovo considerably outperformed state of the art methods, achieving 7.7–22.9\% higher accuracy at the amino acid level and 38.1–64.0\% higher accuracy at the peptide level. We further used DeepNovo to automatically reconstruct the complete sequences of antibody light and heavy chains of mouse, achieving 97.5–100\% coverage and 97.2–99.5\% accuracy, without assisting databases. Moreover, DeepNovo is retrainable to adapt to any sources of data and provides a complete end-to-end training and prediction solution to the de novo sequencing problem. Not only does our study extend the deep learning revolution to a new field, but it also shows an innovative approach in solving optimization problems by using deep learning and dynamic programming.}
}

@article{wang2022molecular,
  title     = {Molecular contrastive learning of representations via graph neural networks},
  author    = {Wang, Yuyang and Wang, Jianren and Cao, Zhonglin and Barati Farimani, Amir},
  journal   = {Nature Machine Intelligence},
  volume    = {4},
  number    = {3},
  pages     = {279--287},
  year      = {2022},
  publisher = {Nature Publishing Group UK London}
}

@article{le2020contrastive,
  title     = {Contrastive representation learning: A framework and review},
  author    = {Le-Khac, Phuc H and Healy, Graham and Smeaton, Alan F},
  journal   = {Ieee Access},
  volume    = {8},
  pages     = {193907--193934},
  year      = {2020},
  publisher = {IEEE}
}

@article{zhou2020graph,
  title     = {Graph neural networks: A review of methods and applications},
  author    = {Zhou, Jie and Cui, Ganqu and Hu, Shengding and Zhang, Zhengyan and Yang, Cheng and Liu, Zhiyuan and Wang, Lifeng and Li, Changcheng and Sun, Maosong},
  journal   = {AI open},
  volume    = {1},
  pages     = {57--81},
  year      = {2020},
  publisher = {Elsevier}
}

@article{Allen2016,
  author  = {Allen, Felicity and Pon, Allison and Greiner, Russ and Wishart, David},
  title   = {Computational Prediction of Electron Ionization Mass Spectra to Assist in GC/MS Compound Identification},
  journal = {Analytical Chemistry},
  volume  = {88},
  number  = {15},
  pages   = {7689-7697},
  year    = {2016},
  doi     = {10.1021/acs.analchem.6b01622},
  note    = {PMID: 27381172}
}
@book{Gross2011,
  title     = {Mass Spectrometry—A Textbook},
  author    = {Gross, J. H.},
  publisher = {Springer},
  year      = {2011},
  doi       = {https://doi.org/10.1007/978-3-319-54398-7}
}

@inbook{Niessen2015,
  author    = {Niessen, W. M. A. and Falck, D.},
  title     = {Chapter 1 in Analyzing Biomolecular Interactions by Mass Spectrometry},
  booktitle = {Analyzing Biomolecular Interactions by Mass Spectrometry},
  editor    = {Kool, J. and Niessen, W. M. A.},
  publisher = {Wiley},
  year      = {2015},
  doi       = {https://doi.org/10.1002/9783527673391}
}

@article{Aebersold2016,
  author  = {Aebersold, R. and Mann, M.},
  title   = {Mass-spectrometric exploration of proteome structure and function},
  journal = {Nature},
  volume  = {537},
  pages   = {347--355},
  year    = {2016}
}

@article{Gowda2014,
  author  = {Gowda, G. A. N. and Djukovic, D.},
  title   = {Overview of mass spectrometry-based metabolomics: opportunities and challenges},
  journal = {Methods Mol. Biol.},
  volume  = {1198},
  pages   = {3--12},
  year    = {2014}
}

@article{DeVijlder2018,
  author  = {De Vijlder, T. and Cuyckens, F.},
  title   = {A tutorial in small molecule identification via electrospray ionization-mass spectrometry: the practical art of structural elucidation},
  journal = {Mass Spectrom. Rev.},
  volume  = {37},
  pages   = {607--629},
  year    = {2018}
}

@article{Peters2011,
  author  = {Peters, F. T.},
  title   = {Recent advances of liquid chromatography-(tandem) mass spectrometry in clinical and forensic toxicology},
  journal = {Clin. Biochem.},
  volume  = {44},
  pages   = {54--65},
  year    = {2011}
}

@article{VanBocxlaer2000,
  author  = {Van Bocxlaer, J. F. et al.},
  title   = {Liquid chromatography-mass spectrometry in forensic toxicology},
  journal = {Mass Spectrom. Rev.},
  volume  = {19},
  pages   = {165--214},
  year    = {2000}
}

@article{Lebedev2013,
  author  = {Lebedev, A. T.},
  title   = {Environmental mass spectrometry},
  journal = {Ann. Rev. Anal. Chem.},
  volume  = {6},
  pages   = {163--189},
  year    = {2013}
}

@article{Ghiandoni2020,
  author  = {Ghiandoni, G. M. et al.},
  journal = {Journal of Computer-Aided Molecular Design},
  title   = {Enhancing reaction-based de novo design using a multi-label reaction class recommender},
  volume  = {34},
  year    = {2020},
  pages   = {783–803}
}
@article{pernaa2023open,
  title={Open-Source Software Development in Cheminformatics: A Qualitative Analysis of Rationales},
  author={Pernaa, Johannes and Takala, Aleksi and Ciftci, Veysel and Hern{\'a}ndez-Ramos, Jos{\'e} and C{\'a}ceres-Jensen, Lizethly and Rodr{\'\i}guez-Becerra, Jorge},
  journal={Applied Sciences},
  volume={13},
  number={17},
  pages={9516},
  year={2023},
  publisher={MDPI}
}

@article{chen2006chemoinformatics,
  title={Chemoinformatics: past, present, and future},
  author={Chen, William Lingran},
  journal={Journal of Chemical Information and Modeling},
  volume={46},
  number={6},
  pages={2230--2255},
  year={2006},
  publisher={ACS Publications}
}

@article{king1946asymmetric,
  title={The Asymmetric Rotor III. Punched-Card Methods of Constructing Band Spectra},
  author={King, Gilbert W and Cross, Paul C and Thomas, George B},
  journal={The Journal of Chemical Physics},
  volume={14},
  number={1},
  pages={35--42},
  year={1946},
  publisher={American Institute of Physics}
}

@article{ray1957finding,
  title={Finding chemical records by digital computers},
  author={Ray, Louis C and Kirsch, Russell A},
  journal={Science},
  volume={126},
  number={3278},
  pages={814--819},
  year={1957},
  publisher={American Association for the Advancement of Science}
}

@article{willett2011chemoinformatics,
  title={Chemoinformatics: a history},
  author={Willett, Peter},
  journal={Wiley Interdisciplinary Reviews: Computational Molecular Science},
  volume={1},
  number={1},
  pages={46--56},
  year={2011},
  publisher={Wiley Online Library}
}

@article{peironcely2012omg,
  title={OMG: open molecule generator},
  author={Peironcely, Julio E and Rojas-Chert{\'o}, Miguel and Fichera, Davide and Reijmers, Theo and Coulier, Leon and Faulon, Jean-Loup and Hankemeier, Thomas},
  journal={Journal of cheminformatics},
  volume={4},
  pages={1--13},
  year={2012},
  publisher={Springer}
}

@article{cao2013chemopy,
  title={ChemoPy: freely available python package for computational biology and chemoinformatics},
  author={Cao, Dong-Sheng and Xu, Qing-Song and Hu, Qian-Nan and Liang, Yi-Zeng},
  journal={Bioinformatics},
  volume={29},
  number={8},
  pages={1092--1094},
  year={2013},
  publisher={Oxford University Press}
}


@inproceedings{satorras2021n,
  title        = {E (n) equivariant graph neural networks},
  author       = {Satorras, V{\i}ctor Garcia and Hoogeboom, Emiel and Welling, Max},
  booktitle    = {International conference on machine learning},
  pages        = {9323--9332},
  year         = {2021},
  organization = {PMLR}
}

@article{orsi2024one,
  title={One chiral fingerprint to find them all},
  author={Orsi, Markus and Reymond, Jean-Louis},
  journal={Journal of cheminformatics},
  volume={16},
  number={1},
  pages={53},
  year={2024},
  publisher={Springer}
}

@article{capecchi2020one,
  title={One molecular fingerprint to rule them all: drugs, biomolecules, and the metabolome},
  author={Capecchi, Alice and Probst, Daniel and Reymond, Jean-Louis},
  journal={Journal of cheminformatics},
  volume={12},
  pages={1--15},
  year={2020},
  publisher={Springer}
}

@article{rogers2010extended,
  title={Extended-connectivity fingerprints},
  author={Rogers, David and Hahn, Mathew},
  journal={Journal of chemical information and modeling},
  volume={50},
  number={5},
  pages={742--754},
  year={2010},
  publisher={ACS Publications}
}

@article{carhart1985atom,
  title={Atom pairs as molecular features in structure-activity studies: definition and applications},
  author={Carhart, Raymond E and Smith, Dennis H and Venkataraghavan, RENGACHARI},
  journal={Journal of Chemical Information and Computer Sciences},
  volume={25},
  number={2},
  pages={64--73},
  year={1985},
  publisher={ACS Publications}
}

@article{probst2018probabilistic,
  title={A probabilistic molecular fingerprint for big data settings},
  author={Probst, Daniel and Reymond, Jean-Louis},
  journal={Journal of cheminformatics},
  volume={10},
  pages={1--12},
  year={2018},
  publisher={Springer}
}

@article{landrum_lwreg_2024,
    title = {lwreg: {A} {Lightweight} {System} for {Chemical} {Registration} and {Data} {Storage}},
    volume = {64},
    copyright = {https://creativecommons.org/licenses/by/4.0/},
    issn = {1549-9596, 1549-960X},
    shorttitle = {lwreg},
    url = {https://pubs.acs.org/doi/10.1021/acs.jcim.4c01133},
    doi = {10.1021/acs.jcim.4c01133},
    language = {en},
    number = {16},
    urldate = {2024-10-21},
    journal = {Journal of Chemical Information and Modeling},
    author = {Landrum, Gregory A. and Braun, Jessica and Katzberger, Paul and Lehner, Marc T. and Riniker, Sereina},
    month = aug,
    year = {2024},
    pages = {6247--6252},
}

@article{bento_open_2020,
    title = {An open source chemical structure curation pipeline using {RDKit}},
    volume = {12},
    issn = {1758-2946},
    url = {https://jcheminf.biomedcentral.com/articles/10.1186/s13321-020-00456-1},
    doi = {10.1186/s13321-020-00456-1},
    language = {en},
    number = {1},
    urldate = {2023-01-12},
    journal = {Journal of Cheminformatics},
    author = {Bento, A. Patrícia and Hersey, Anne and Félix, Eloy and Landrum, Greg and Gaulton, Anna and Atkinson, Francis and Bellis, Louisa J. and De Veij, Marleen and Leach, Andrew R.},
    month = dec,
    year = {2020},
    pages = {51},
}

@article{lehner_dash_2023,
    title = {{DASH}: {Dynamic} {Attention}-{Based} {Substructure} {Hierarchy} for {Partial} {Charge} {Assignment}},
    volume = {63},
    copyright = {https://creativecommons.org/licenses/by-nc-nd/4.0/},
    issn = {1549-9596, 1549-960X},
    shorttitle = {{DASH}},
    url = {https://pubs.acs.org/doi/10.1021/acs.jcim.3c00800},
    doi = {10.1021/acs.jcim.3c00800},
    language = {en},
    number = {19},
    urldate = {2024-10-22},
    journal = {Journal of Chemical Information and Modeling},
    author = {Lehner, Marc T. and Katzberger, Paul and Maeder, Niels and Schiebroek, Carl C.G. and Teetz, Jakob and Landrum, Gregory A. and Riniker, Sereina},
    month = oct,
    year = {2023},
    pages = {6014--6028},
    file = {Full Text:C\:\\Users\\jonas\\Zotero\\storage\\CATGSF9S\\Lehner et al. - 2023 - DASH Dynamic Attention-Based Substructure Hierarc.pdf:application/pdf},
}

@misc{https://doi.org/10.48550/arxiv.2410.11527,
  doi = {10.48550/ARXIV.2410.11527},
  url = {https://arxiv.org/abs/2410.11527},
  author = {Guo,  Jeff and Schwaller,  Philippe},
  keywords = {Biomolecules (q-bio.BM),  Machine Learning (cs.LG),  FOS: Biological sciences,  FOS: Biological sciences,  FOS: Computer and information sciences,  FOS: Computer and information sciences},
  title = {It Takes Two to Tango: Directly Optimizing for Constrained Synthesizability in Generative Molecular Design},
  publisher = {arXiv},
  year = {2024},
  copyright = {Creative Commons Attribution 4.0 International}
}

@misc{https://doi.org/10.48550/arxiv.2110.06389,
  doi = {10.48550/ARXIV.2110.06389},
  url = {https://arxiv.org/abs/2110.06389},
  author = {Gao,  Wenhao and Mercado,  Rocío and Coley,  Connor W.},
  keywords = {Machine Learning (cs.LG),  Quantitative Methods (q-bio.QM),  FOS: Computer and information sciences,  FOS: Computer and information sciences,  FOS: Biological sciences,  FOS: Biological sciences},
  title = {Amortized Tree Generation for Bottom-up Synthesis Planning and Synthesizable Molecular Design},
  publisher = {arXiv},
  year = {2021},
  copyright = {Creative Commons Attribution 4.0 International}
}

@misc{https://doi.org/10.48550/arxiv.2410.03494,
  doi = {10.48550/ARXIV.2410.03494},
  url = {https://arxiv.org/abs/2410.03494},
  author = {Gao,  Wenhao and Luo,  Shitong and Coley,  Connor W.},
  keywords = {Machine Learning (cs.LG),  Artificial Intelligence (cs.AI),  Chemical Physics (physics.chem-ph),  Biomolecules (q-bio.BM),  FOS: Computer and information sciences,  FOS: Computer and information sciences,  FOS: Physical sciences,  FOS: Physical sciences,  FOS: Biological sciences,  FOS: Biological sciences},
  title = {Generative Artificial Intelligence for Navigating Synthesizable Chemical Space},
  publisher = {arXiv},
  year = {2024},
  copyright = {Creative Commons Attribution 4.0 International}
}

@misc{siegel2024corebenchfosteringcredibilitypublished,
      title={CORE-Bench: Fostering the Credibility of Published Research Through a Computational Reproducibility Agent Benchmark}, 
      author={Zachary S. Siegel and Sayash Kapoor and Nitya Nagdir and Benedikt Stroebl and Arvind Narayanan},
      year={2024},
      eprint={2409.11363},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2409.11363}, 
}

@misc{kapoor2024aiagentsmatter,
      title={AI Agents That Matter}, 
      author={Sayash Kapoor and Benedikt Stroebl and Zachary S. Siegel and Nitya Nadgir and Arvind Narayanan},
      year={2024},
      eprint={2407.01502},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2407.01502}, 
}

@misc{jimenez2024swebenchlanguagemodelsresolve,
      title={SWE-bench: Can Language Models Resolve Real-World GitHub Issues?}, 
      author={Carlos E. Jimenez and John Yang and Alexander Wettig and Shunyu Yao and Kexin Pei and Ofir Press and Karthik Narasimhan},
      year={2024},
      eprint={2310.06770},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.06770}, 
}

@misc{laurent2024labbenchmeasuringcapabilitieslanguage,
      title={LAB-Bench: Measuring Capabilities of Language Models for Biology Research}, 
      author={Jon M. Laurent and Joseph D. Janizek and Michael Ruzo and Michaela M. Hinks and Michael J. Hammerling and Siddharth Narayanan and Manvitha Ponnapati and Andrew D. White and Samuel G. Rodriques},
      year={2024},
      eprint={2407.10362},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2407.10362}, 
}

@misc{chan2024mlebenchevaluatingmachinelearning,
      title={MLE-bench: Evaluating Machine Learning Agents on Machine Learning Engineering}, 
      author={Jun Shern Chan and Neil Chowdhury and Oliver Jaffe and James Aung and Dane Sherburn and Evan Mays and Giulio Starace and Kevin Liu and Leon Maksin and Tejal Patwardhan and Lilian Weng and Aleksander Mądry},
      year={2024},
      eprint={2410.07095},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.07095}, 
}

@misc{huang2024mlagentbenchevaluatinglanguageagents,
      title={MLAgentBench: Evaluating Language Agents on Machine Learning Experimentation}, 
      author={Qian Huang and Jian Vora and Percy Liang and Jure Leskovec},
      year={2024},
      eprint={2310.03302},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2310.03302}, 
}


@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}

@article{kojima2022large,
  title={Large language models are zero-shot reasoners},
  author={Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={22199--22213},
  year={2022}
}

@misc{fu2023complexitybasedpromptingmultistepreasoning,
      title={Complexity-Based Prompting for Multi-Step Reasoning}, 
      author={Yao Fu and Hao Peng and Ashish Sabharwal and Peter Clark and Tushar Khot},
      year={2023},
      eprint={2210.00720},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2210.00720}, 
}


@misc{yao2023treethoughtsdeliberateproblem,
      title={Tree of Thoughts: Deliberate Problem Solving with Large Language Models}, 
      author={Shunyu Yao and Dian Yu and Jeffrey Zhao and Izhak Shafran and Thomas L. Griffiths and Yuan Cao and Karthik Narasimhan},
      year={2023},
      eprint={2305.10601},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2305.10601}, 
}

@article{Fedorenko_2024, title={Language is primarily a tool for communication rather than thought}, volume={630}, ISSN={1476-4687}, url={http://dx.doi.org/10.1038/s41586-024-07522-w}, DOI={10.1038/s41586-024-07522-w}, number={8017}, journal={Nature}, publisher={Springer Science and Business Media LLC}, author={Fedorenko, Evelina and Piantadosi, Steven T. and Gibson, Edward A. F.}, year={2024}, month=jun, pages={575–586} }


@misc{qin2024o1replicationjourneystrategic,
      title={O1 Replication Journey: A Strategic Progress Report -- Part 1}, 
      author={Yiwei Qin and Xuefeng Li and Haoyang Zou and Yixiu Liu and Shijie Xia and Zhen Huang and Yixin Ye and Weizhe Yuan and Hector Liu and Yuanzhi Li and Pengfei Liu},
      year={2024},
      eprint={2410.18982},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2410.18982}, 
}

@article{rull2023nmr,
  title={NMR shift prediction from small data quantities},
  author={Rull, Herman and Fischer, Markus and Kuhn, Stefan},
  journal={Journal of Cheminformatics},
  volume={15},
  number={1},
  pages={114},
  year={2023},
  publisher={Springer}
}

@article{jonas2022prediction,
  title={Prediction of chemical shift in NMR: A review},
  author={Jonas, Eric and Kuhn, Stefan and Schl{\"o}rer, Nils},
  journal={Magnetic Resonance in Chemistry},
  volume={60},
  number={11},
  pages={1021--1031},
  year={2022},
  publisher={Wiley Online Library}
}

@article{duvenaud2015convolutional,
  title={Convolutional networks on graphs for learning molecular fingerprints},
  author={Duvenaud, David K and Maclaurin, Dougal and Iparraguirre, Jorge and Bombarell, Rafael and Hirzel, Timothy and Aspuru-Guzik, Al{\'a}n and Adams, Ryan P},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}

@article{gilmer2017neural,
  title={Neural message passing for quantum chemistry},
  author={Gilmer, Justin and Schoenholz, Samuel S and Riley, Patrick F and Vinyals, Oriol and Dahl, George E},
  booktitle={International conference on machine learning},
  pages={1263--1272},
  year={2017},
  organization={PMLR}
}

@article{bremser1978hose,
  title={Hose—a novel substructure code},
  author={Bremser, W},
  journal={Analytica Chimica Acta},
  volume={103},
  number={4},
  pages={355--365},
  year={1978},
  publisher={Elsevier}
}

@article{jonas2019rapid,
  title={Rapid prediction of NMR spectral properties with quantified uncertainty},
  author={Jonas, Eric and Kuhn, Stefan},
  journal={Journal of cheminformatics},
  volume={11},
  pages={1--7},
  year={2019},
  publisher={Springer}
}

@article{oellien20127th,
  title={7th German Conference on Chemoinformatics: 25 CIC-Workshop Goslar, Germany. 6-8 November 2011. Abstracts},
  author={Oellien, Frank and Fechner, Uli and Engel, Thomas},
  journal={Journal of cheminformatics},
  volume={4},
  number={Suppl 1},
  pages={A1--P62},
  year={2012}
}

@article{steyvers2025large,
  title={What large language models know and what people think they know},
  author={Steyvers, Mark and Tejeda, Heliodoro and Kumar, Aakriti and Belem, Catarina and Karny, Sheer and Hu, Xinyue and Mayer, Lukas W and Smyth, Padhraic},
  journal={Nature Machine Intelligence},
  pages={1--11},
  year={2025},
  publisher={Nature Publishing Group UK London}
}
@article{starace2025paperbench0,
  title   = {PaperBench: Evaluating AI's Ability to Replicate AI Research},
  author  = {Giulio Starace and Oliver Jaffe and Dane Sherburn and James Aung and Jun Shern Chan and Leon Maksin and Rachel Dias and Evan Mays and Benjamin Kinsella and Wyatt Thompson and Johannes Heidecke and Amelia Glaese and Tejal Patwardhan},
  year    = {2025},
  journal = {arXiv preprint arXiv: 2504.01848}
}
@article{lu2024ai,
  title   = {The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery},
  author  = {Chris Lu and Cong Lu and Robert Tjarko Lange and Jakob Foerster and Jeff Clune and David Ha},
  year    = {2024},
  journal = {arXiv preprint arXiv: 2408.06292}
}
@article{tian2024scicode0,
  title     = {SciCode: A Research Coding Benchmark Curated by Scientists},
  author    = {Minyang Tian and Luyu Gao and Shizhuo Dylan Zhang and Xinan Chen and Cunwei Fan and Xuefei Guo and Roland Haas and Pan Ji and Kittithat Krongchon and Yao Li and Shengyan Liu and Di Luo and Yutao Ma and Hao Tong and Kha Trinh and Chenyu Tian and Zihan Wang and Bohao Wu and Yanyu Xiong and Shengzhu Yin and Min Zhu and K. Lieret and Yanxin Lu and Genglin Liu and Yufeng Du and Tianhua Tao and Ofir Press and Jamie Callan and E. Huerta and Hao Peng},
  journal   = {Neural Information Processing Systems},
  year      = {2024},
  doi       = {10.48550/arXiv.2407.13168},
}
@article{huang2023mlagentbench0,
  title     = {MLAgentBench: Evaluating Language Agents on Machine Learning Experimentation},
  author    = {Qian Huang and Jian Vora and Percy Liang and J. Leskovec},
  journal   = {International Conference on Machine Learning},
  year      = {2023},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/b8ee0b5322382807e687c95cc87b059d3f348495}
}
@article{chan2024mle0bench0,
  title   = {MLE-bench: Evaluating Machine Learning Agents on Machine Learning Engineering},
  author  = {Jun Shern Chan and Neil Chowdhury and Oliver Jaffe and James Aung and Dane Sherburn and Evan Mays and Giulio Starace and Kevin Liu and Leon Maksin and Tejal Patwardhan and Lilian Weng and Aleksander Mądry},
  year    = {2024},
  journal = {arXiv preprint arXiv: 2410.07095}
}
@article{shankar2024validates,
  title     = {Who Validates the Validators? Aligning LLM-Assisted Evaluation of LLM Outputs with Human Preferences},
  author    = {Shreya Shankar and J.D. Zamfirescu-Pereira and Bjorn Hartmann and Aditya G. Parameswaran and Ian Arawjo},
  journal   = {ACM Symposium on User Interface Software and Technology},
  year      = {2024},
  doi       = {10.48550/arXiv.2404.12272},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/6098ac103ba222f9c3b089714ef3100357993255}
}
@article{ding_nafm_2025,
	title = {{NaFM}: {Pre}-training a {Foundation} {Model} for {Small}-{Molecule} {Natural} {Products}},
	copyright = {Creative Commons Attribution 4.0 International},
	shorttitle = {{NaFM}},
	url = {https://arxiv.org/abs/2503.17656},
	doi = {10.48550/ARXIV.2503.17656},
	abstract = {Natural products, as metabolites from microorganisms, animals, or plants, exhibit diverse biological activities, making them crucial for drug discovery. Nowadays, existing deep learning methods for natural products research primarily rely on supervised learning approaches designed for specific downstream tasks. However, such one-model-for-a-task paradigm often lacks generalizability and leaves significant room for performance improvement. Additionally, existing molecular characterization methods are not well-suited for the unique tasks associated with natural products. To address these limitations, we have pre-trained a foundation model for natural products based on their unique properties. Our approach employs a novel pretraining strategy that is especially tailored to natural products. By incorporating contrastive learning and masked graph learning objectives, we emphasize evolutional information from molecular scaffolds while capturing side-chain information. Our framework achieves state-of-the-art (SOTA) results in various downstream tasks related to natural product mining and drug discovery. We first compare taxonomy classification with synthesized molecule-focused baselines to demonstrate that current models are inadequate for understanding natural synthesis. Furthermore, by diving into a fine-grained analysis at both the gene and microbial levels, NaFM demonstrates the ability to capture evolutionary information. Eventually, our method is experimented with virtual screening, illustrating informative natural product representations that can lead to more effective identification of potential drug candidates.},
	urldate = {2025-04-22},
	publisher = {arXiv},
	author = {Ding, Yuheng and Wang, Yusong and Qiang, Bo and Yu, Jie and Li, Qi and Zhou, Yiran and Liu, Zhenmin},
	year = {2025},
	note = {Version Number: 1},
	keywords = {Artificial Intelligence (cs.AI), FOS: Biological sciences, FOS: Computer and information sciences, Machine Learning (cs.LG), Quantitative Methods (q-bio.QM)},
}
@article{xu_composite_2024,
	title = {Composite machine learning strategy for natural products taxonomical classification and structural insights},
	volume = {3},
	issn = {2635-098X},
	url = {https://xlink.rsc.org/?DOI=D4DD00155A},
	doi = {10.1039/D4DD00155A},
	abstract = {A composite machine learning model combining graph and decision tree-based architectures achieved high accuracy in taxonomical classification of natural products and uncovered key structure–taxonomy relationships.
          ,
            Taxonomical classification of natural products (NPs) can assist in genomic and phylogenetic analysis of source organisms and facilitate streamlining of bioprospecting efforts. Here, a composite machine learning strategy marrying graph convolutional neural networks (GCNNs) and eXteme Gradient boosting (XGB) is proposed and validated for taxonomical classification of NPs in five kingdoms (Animalia, Bacteria, Chromista, Fungi, and Plantae). Our composite model, trained on 133 092 NPs from the LOTUS database, achieved five-fold cross-validated classification accuracy of 97.4\%. When employed to classify out-of-sample NPs from the NP Atlas database, accuracies of 82.8\% for bacteria and 86.6\% for fungi were obtained. Dimensionality-reduced representations of the molecular embeddings from our composite model revealed distinct clusters of NPs that suggest a basis for enhanced classification performance. The top critical substructures from the NPs of each kingdom were also identified and compared to provide insights on structure–taxonomy relationships. Overall, this study showcases the potential of composite machine learning models for robust taxonomical classification of NPs, which can streamline discovery of NPs.},
	language = {en},
	number = {11},
	urldate = {2025-04-22},
	journal = {Digital Discovery},
	author = {Xu, Qisong and Tan, Alan K. X. and Guo, Liangfeng and Lim, Yee Hwee and Tay, Dillon W. P. and Ang, Shi Jun},
	year = {2024},
	pages = {2192--2200},
	file = {Full Text PDF:C\:\\Users\\jonas\\Zotero\\storage\\IS7MQPQ7\\Xu et al. - 2024 - Composite machine learning strategy for natural products taxonomical classification and structural i.pdf:application/pdf},
}
@article{stokes_deep_2020,
	title = {A {Deep} {Learning} {Approach} to {Antibiotic} {Discovery}},
	volume = {180},
	issn = {00928674},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0092867420301021},
	doi = {10.1016/j.cell.2020.01.021},
	language = {en},
	number = {4},
	urldate = {2025-04-22},
	journal = {Cell},
	author = {Stokes, Jonathan M. and Yang, Kevin and Swanson, Kyle and Jin, Wengong and Cubillos-Ruiz, Andres and Donghia, Nina M. and MacNair, Craig R. and French, Shawn and Carfrae, Lindsey A. and Bloom-Ackermann, Zohar and Tran, Victoria M. and Chiappino-Pepe, Anush and Badran, Ahmed H. and Andrews, Ian W. and Chory, Emma J. and Church, George M. and Brown, Eric D. and Jaakkola, Tommi S. and Barzilay, Regina and Collins, James J.},
	month = feb,
	year = {2020},
	pages = {688--702.e13},
	file = {PubMed Central Full Text PDF:C\:\\Users\\jonas\\Zotero\\storage\\9MFWEVEV\\Stokes et al. - 2020 - A Deep Learning Approach to Antibiotic Discovery.pdf:application/pdf},
}
@article{kim_npclassifier_2021,
	title = {{NPClassifier}: {A} {Deep} {Neural} {Network}-{Based} {Structural} {Classification} {Tool} for {Natural} {Products}},
	volume = {84},
	copyright = {https://creativecommons.org/licenses/by-nc-nd/4.0/},
	issn = {0163-3864, 1520-6025},
	shorttitle = {{NPClassifier}},
	url = {https://pubs.acs.org/doi/10.1021/acs.jnatprod.1c00399},
	doi = {10.1021/acs.jnatprod.1c00399},
	language = {en},
	number = {11},
	urldate = {2025-04-22},
	journal = {Journal of Natural Products},
	author = {Kim, Hyun Woo and Wang, Mingxun and Leber, Christopher A. and Nothias, Louis-Félix and Reher, Raphael and Kang, Kyo Bin and Van Der Hooft, Justin J. J. and Dorrestein, Pieter C. and Gerwick, William H. and Cottrell, Garrison W.},
	month = nov,
	year = {2021},
	pages = {2795--2807},
	file = {Full Text PDF:C\:\\Users\\jonas\\Zotero\\storage\\BRKQUDES\\Kim et al. - 2021 - NPClassifier A Deep Neural Network-Based Structural Classification Tool for Natural Products.pdf:application/pdf},
}
@article{rutz_lotus_2022,
	title = {The {LOTUS} initiative for open knowledge management in natural products research},
	volume = {11},
	issn = {2050-084X},
	url = {https://elifesciences.org/articles/70780},
	doi = {10.7554/eLife.70780},
	abstract = {Contemporary bioinformatic and chemoinformatic capabilities hold promise to reshape knowledge management, analysis and interpretation of data in natural products research. Currently, reliance on a disparate set of non-standardized, insular, and specialized databases presents a series of challenges for data access, both within the discipline and for integration and interoperability between related fields. The fundamental elements of exchange are referenced structure-organism pairs that establish relationships between distinct molecular structures and the living organisms from which they were identified. Consolidating and sharing such information via an open platform has strong transformative potential for natural products research and beyond. This is the ultimate goal of the newly established LOTUS initiative, which has now completed the first steps toward the harmonization, curation, validation and open dissemination of 750,000+ referenced structure-organism pairs. LOTUS data is hosted on Wikidata and regularly mirrored on
              https://lotus.naturalproducts.net
              . Data sharing within the Wikidata framework broadens data access and interoperability, opening new possibilities for community curation and evolving publication models. Furthermore, embedding LOTUS data into the vast Wikidata knowledge graph will facilitate new biological and chemical insights. The LOTUS initiative represents an important advancement in the design and deployment of a comprehensive and collaborative natural products knowledge base.},
	language = {en},
	urldate = {2024-12-12},
	journal = {eLife},
	author = {Rutz, Adriano and Sorokina, Maria and Galgonek, Jakub and Mietchen, Daniel and Willighagen, Egon and Gaudry, Arnaud and Graham, James G and Stephan, Ralf and Page, Roderic and Vondrášek, Jiří and Steinbeck, Christoph and Pauli, Guido F and Wolfender, Jean-Luc and Bisson, Jonathan and Allard, Pierre-Marie},
	month = may,
	year = {2022},
	pages = {e70780},
	file = {Full Text:C\:\\Users\\jonas\\Zotero\\storage\\UPLQDVDL\\Rutz et al. - 2022 - The LOTUS initiative for open knowledge management.pdf:application/pdf},
}
@article{chandrasekhar_coconut_2024,
	title = {{COCONUT} 2.0: a comprehensive overhaul and curation of the collection of open natural products database},
	copyright = {https://creativecommons.org/licenses/by/4.0/},
	issn = {0305-1048, 1362-4962},
	shorttitle = {{COCONUT} 2.0},
	url = {https://academic.oup.com/nar/advance-article/doi/10.1093/nar/gkae1063/7908792},
	doi = {10.1093/nar/gkae1063},
	abstract = {Abstract
            The COCONUT (COlleCtion of Open Natural prodUcTs) database was launched in 2021 as an aggregation of openly available natural product datasets and has been one of the biggest open natural product databases since. Apart from the chemical structures of natural products, COCONUT contains information about names and synonyms, species and organism parts in which the natural product has been found, geographic information about where the respective sample has been collected and literature references, where available. COCONUT is openly accessible at https://coconut.naturalproducts.net. Users can search textual information and perform structure, substructure, and similarity searches. The data in COCONUT are available for bulk download as SDF, CSV and a database dump. The web application for accessing the data is open-source. Here, we describe COCONUT 2.0, for which the web application has been completely rewritten, and the data have been newly assembled and extensively curated. New features include data submissions by users and community curation facilitated in various ways.},
	language = {en},
	urldate = {2024-12-12},
	journal = {Nucleic Acids Research},
	author = {Chandrasekhar, Venkata and Rajan, Kohulan and Kanakam, Sri Ram Sagar and Sharma, Nisha and Weißenborn, Viktor and Schaub, Jonas and Steinbeck, Christoph},
	month = nov,
	year = {2024},
	pages = {gkae1063},
}
@article{probst_visualization_2020,
	title = {Visualization of very large high-dimensional data sets as minimum spanning trees},
	volume = {12},
	issn = {1758-2946},
	url = {https://jcheminf.biomedcentral.com/articles/10.1186/s13321-020-0416-x},
	doi = {10.1186/s13321-020-0416-x},
	abstract = {Abstract

              The chemical sciences are producing an unprecedented amount of large, high-dimensional data sets containing chemical structures and associated properties. However, there are currently no algorithms to visualize such data while preserving both global and local features with a sufficient level of detail to allow for human inspection and interpretation. Here, we propose a solution to this problem with a new data visualization method, TMAP, capable of representing data sets of up to millions of data points and arbitrary high dimensionality as a two-dimensional tree (
              http://tmap.gdb.tools
              ). Visualizations based on TMAP are better suited than t-SNE or UMAP for the exploration and interpretation of large data sets due to their tree-like nature, increased local and global neighborhood and structure preservation, and the transparency of the methods the algorithm is based on. We apply TMAP to the most used chemistry data sets including databases of molecules such as ChEMBL, FDB17, the Natural Products Atlas, DSSTox, as well as to the MoleculeNet benchmark collection of data sets. We also show its broad applicability with further examples from biology, particle physics, and literature.},
	language = {en},
	number = {1},
	urldate = {2025-04-22},
	journal = {Journal of Cheminformatics},
	author = {Probst, Daniel and Reymond, Jean-Louis},
	month = dec,
	year = {2020},
	pages = {12},
	file = {Volltext:C\:\\Users\\jonas\\Zotero\\storage\\S25J627G\\Probst und Reymond - 2020 - Visualization of very large high-dimensional data sets as minimum spanning trees.pdf:application/pdf},
}
@article{terlouw_mibig_2023,
	title = {{MIBiG} 3.0: a community-driven effort to annotate experimentally validated biosynthetic gene clusters},
	volume = {51},
	copyright = {https://creativecommons.org/licenses/by/4.0/},
	issn = {0305-1048, 1362-4962},
	shorttitle = {{MIBiG} 3.0},
	url = {https://academic.oup.com/nar/article/51/D1/D603/6833236},
	doi = {10.1093/nar/gkac1049},
	abstract = {Abstract
            With an ever-increasing amount of (meta)genomic data being deposited in sequence databases, (meta)genome mining for natural product biosynthetic pathways occupies a critical role in the discovery of novel pharmaceutical drugs, crop protection agents and biomaterials. The genes that encode these pathways are often organised into biosynthetic gene clusters (BGCs). In 2015, we defined the Minimum Information about a Biosynthetic Gene cluster (MIBiG): a standardised data format that describes the minimally required information to uniquely characterise a BGC. We simultaneously constructed an accompanying online database of BGCs, which has since been widely used by the community as a reference dataset for BGCs and was expanded to 2021 entries in 2019 (MIBiG 2.0). Here, we describe MIBiG 3.0, a database update comprising large-scale validation and re-annotation of existing entries and 661 new entries. Particular attention was paid to the annotation of compound structures and biological activities, as well as protein domain selectivities. Together, these new features keep the database up-to-date, and will provide new opportunities for the scientific community to use its freely available data, e.g. for the training of new machine learning models to predict sequence-structure-function relationships for diverse natural products. MIBiG 3.0 is accessible online at https://mibig.secondarymetabolites.org/.},
	language = {en},
	number = {D1},
	urldate = {2025-04-22},
	journal = {Nucleic Acids Research},
	author = {Terlouw, Barbara R and Blin, Kai and Navarro-Muñoz, Jorge C and Avalon, Nicole E and Chevrette, Marc G and Egbert, Susan and Lee, Sanghoon and Meijer, David and Recchia, Michael J J and Reitz, Zachary L and van Santen, Jeffrey A and Selem-Mojica, Nelly and Tørring, Thomas and Zaroubi, Liana and Alanjary, Mohammad and Aleti, Gajender and Aguilar, César and Al-Salihi, Suhad A A and Augustijn, Hannah E and Avelar-Rivas, J Abraham and Avitia-Domínguez, Luis A and Barona-Gómez, Francisco and Bernaldo-Agüero, Jordan and Bielinski, Vincent A and Biermann, Friederike and Booth, Thomas J and Carrion Bravo, Victor J and Castelo-Branco, Raquel and Chagas, Fernanda O and Cruz-Morales, Pablo and Du, Chao and Duncan, Katherine R and Gavriilidou, Athina and Gayrard, Damien and Gutiérrez-García, Karina and Haslinger, Kristina and Helfrich, Eric J N and van der Hooft, Justin J J and Jati, Afif P and Kalkreuter, Edward and Kalyvas, Nikolaos and Kang, Kyo Bin and Kautsar, Satria and Kim, Wonyong and Kunjapur, Aditya M and Li, Yong-Xin and Lin, Geng-Min and Loureiro, Catarina and Louwen, Joris J R and Louwen, Nico L L and Lund, George and Parra, Jonathan and Philmus, Benjamin and Pourmohsenin, Bita and Pronk, Lotte J U and Rego, Adriana and Rex, Devasahayam Arokia Balaya and Robinson, Serina and Rosas-Becerra, L Rodrigo and Roxborough, Eve T and Schorn, Michelle A and Scobie, Darren J and Singh, Kumar Saurabh and Sokolova, Nika and Tang, Xiaoyu and Udwary, Daniel and Vigneshwari, Aruna and Vind, Kristiina and Vromans, Sophie P J M and Waschulin, Valentin and Williams, Sam E and Winter, Jaclyn M and Witte, Thomas E and Xie, Huali and Yang, Dong and Yu, Jingwei and Zdouc, Mitja and Zhong, Zheng and Collemare, Jérôme and Linington, Roger G and Weber, Tilmann and Medema, Marnix H},
	month = jan,
	year = {2023},
	pages = {D603--D610},
	file = {Volltext:C\:\\Users\\jonas\\Zotero\\storage\\K5DBDVHT\\Terlouw et al. - 2023 - MIBiG 3.0 a community-driven effort to annotate experimentally validated biosynthetic gene clusters.pdf:application/pdf},
}
@article{mistry_pfam_2021,
	title = {Pfam: {The} protein families database in 2021},
	volume = {49},
	copyright = {http://creativecommons.org/licenses/by/4.0/},
	issn = {0305-1048, 1362-4962},
	shorttitle = {Pfam},
	url = {https://academic.oup.com/nar/article/49/D1/D412/5943818},
	doi = {10.1093/nar/gkaa913},
	abstract = {Abstract
            The Pfam database is a widely used resource for classifying protein sequences into families and domains. Since Pfam was last described in this journal, over 350 new families have been added in Pfam 33.1 and numerous improvements have been made to existing entries. To facilitate research on COVID-19, we have revised the Pfam entries that cover the SARS-CoV-2 proteome, and built new entries for regions that were not covered by Pfam. We have reintroduced Pfam-B which provides an automatically generated supplement to Pfam and contains 136 730 novel clusters of sequences that are not yet matched by a Pfam family. The new Pfam-B is based on a clustering by the MMseqs2 software. We have compared all of the regions in the RepeatsDB to those in Pfam and have started to use the results to build and refine Pfam repeat families. Pfam is freely available for browsing and download at http://pfam.xfam.org/.},
	language = {en},
	number = {D1},
	urldate = {2025-04-22},
	journal = {Nucleic Acids Research},
	author = {Mistry, Jaina and Chuguransky, Sara and Williams, Lowri and Qureshi, Matloob and Salazar, Gustavo A and Sonnhammer, Erik L L and Tosatto, Silvio C E and Paladin, Lisanna and Raj, Shriya and Richardson, Lorna J and Finn, Robert D and Bateman, Alex},
	month = jan,
	year = {2021},
	pages = {D412--D419},
	file = {Volltext:C\:\\Users\\jonas\\Zotero\\storage\\G4BCHCLS\\Mistry et al. - 2021 - Pfam The protein families database in 2021.pdf:application/pdf},
e=======

@inproceedings{DBLP:conf/nips/Wei0SBIXCLZ22,
  author    = {Jason Wei and Xuezhi Wang and Dale Schuurmans and Maarten Bosma and Brian Ichter and Fei Xia and Ed H. Chi and Quoc V. Le and Denny Zhou},
  editor    = {Sanmi Koyejo and S. Mohamed and A. Agarwal and Danielle Belgrave and K. Cho and A. Oh},
  title     = {Chain-of-Thought Prompting Elicits Reasoning in Large Language Models},
  booktitle = {Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022},
  year      = {2022},
  url       = {http://papers.nips.cc/paper_files/paper/2022/hash/9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html},
  timestamp = {Tue, 12 Nov 2024 16:50:49 +0100},
  biburl    = {https://dblp.org/rec/conf/nips/Wei0SBIXCLZ22.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{ling2017program,
  title     = {Program Induction by Rationale Generation : Learning to Solve and Explain Algebraic Word Problems},
  author    = {Wang Ling and Dani Yogatama and Chris Dyer and Phil Blunsom},
  journal   = {Annual Meeting of the Association for Computational Linguistics},
  year      = {2017},
  doi       = {10.18653/v1/P17-1015},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/b123a0d46ad917b79c43c5ae981e03ed2458ed11}
}

@article{nye2021show,
  title   = {Show Your Work: Scratchpads for Intermediate Computation with Language Models},
  author  = {Maxwell Nye and Anders Johan Andreassen and Guy Gur-Ari and Henryk Michalewski and Jacob Austin and David Bieber and David Dohan and Aitor Lewkowycz and Maarten Bosma and David Luan and Charles Sutton and Augustus Odena},
  year    = {2021},
  journal = {arXiv preprint arXiv: 2112.00114}
}

@article{kojima2022large,
  title={Large language models are zero-shot reasoners},
  author={Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={22199--22213},
  year={2022}
}

@article{zelikman2022star,
  title   = {Star: Bootstrapping reasoning with reasoning},
  author  = {Zelikman, Eric and Wu, Yuhuai and Mu, Jesse and Goodman, Noah},
  journal = {Advances in Neural Information Processing Systems},
  volume  = {35},
  pages   = {15476-15488},
  year    = {2022}
}

@article{wang2023math0shepherd0,
  title     = {Math-Shepherd: Verify and Reinforce LLMs Step-by-step without Human Annotations},
  author    = {Peiyi Wang and Lei Li and Zhihong Shao and R. Xu and Damai Dai and Yifei Li and Deli Chen and Y.Wu and Zhifang Sui},
  journal   = {Annual Meeting of the Association for Computational Linguistics},
  year      = {2023},
  doi       = {10.48550/arXiv.2312.08935},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/4ba57555bef02f988f2ed3bab2f102733dc55221}
}
@article{guo2023can,
  title={What can large language models do in chemistry? a comprehensive benchmark on eight tasks},
  author={Guo, Taicheng and Nan, Bozhao and Liang, Zhenwen and Guo, Zhichun and Chawla, Nitesh and Wiest, Olaf and Zhang, Xiangliang and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={59662--59688},
  year={2023}
}
@article{cai2024internlm2,
  title   = {InternLM2 Technical Report},
  author  = {Zheng Cai and Maosong Cao and Haojiong Chen and Kai Chen and Keyu Chen and Xin Chen and Xun Chen and Zehui Chen and Zhi Chen and Pei Chu and Xiaoyi Dong and Haodong Duan and Qi Fan and Zhaoye Fei and Yang Gao and Jiaye Ge and Chenya Gu and Yuzhe Gu and Tao Gui and Aijia Guo and Qipeng Guo and Conghui He and Yingfan Hu and Ting Huang and Tao Jiang and Penglong Jiao and Zhenjiang Jin and Zhikai Lei and Jiaxing Li and Jingwen Li and Linyang Li and Shuaibin Li and Wei Li and Yining Li and Hongwei Liu and Jiangning Liu and Jiawei Hong and Kaiwen Liu and Kuikun Liu and Xiaoran Liu and Chengqi Lv and Haijun Lv and Kai Lv and Li Ma and Runyuan Ma and Zerun Ma and Wenchang Ning and Linke Ouyang and Jiantao Qiu and Yuan Qu and Fukai Shang and Yunfan Shao and Demin Song and Zifan Song and Zhihao Sui and Peng Sun and Yu Sun and Huanze Tang and Bin Wang and Guoteng Wang and Jiaqi Wang and Jiayu Wang and Rui Wang and Yudong Wang and Ziyi Wang and Xingjian Wei and Qizhen Weng and Fan Wu and Yingtong Xiong and Chao Xu and Ruiliang Xu and Hang Yan and Yirong Yan and Xiaogui Yang and Haochen Ye and Huaiyuan Ying and Jia Yu and Jing Yu and Yuhang Zang and Chuyu Zhang and Li Zhang and Pan Zhang and Peng Zhang and Ruijie Zhang and Shuo Zhang and Songyang Zhang and Wenjian Zhang and Wenwei Zhang and Xingcheng Zhang and Xinyue Zhang and Hui Zhao and Qian Zhao and Xiaomeng Zhao and Fengzhe Zhou and Zaida Zhou and Jingming Zhuo and Yicheng Zou and Xipeng Qiu and Yu Qiao and Dahua Lin},
  year    = {2024},
  journal = {arXiv preprint arXiv: 2403.17297}
}
@article{xie2023darwin,
  title   = {DARWIN Series: Domain Specific Large Language Models for Natural Science},
  author  = {Tong Xie and Yuwei Wan and Wei Huang and Zhenyu Yin and Yixuan Liu and Shaozhou Wang and Qingyuan Linghu and Chunyu Kit and Clara Grazian and Wenjie Zhang and Imran Razzak and Bram Hoex},
  year    = {2023},
  journal = {arXiv preprint arXiv: 2308.13565}
}
@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}
@article{zhang2024chemllm0,
  title   = {ChemLLM: A Chemical Large Language Model},
  author  = {Di Zhang and Wei Liu and Qian Tan and Jingdan Chen and Hang Yan and Yuliang Yan and Jiatong Li and Weiran Huang and Xiangyu Yue and Wanli Ouyang and Dongzhan Zhou and Shufei Zhang and Mao Su and Han-Sen Zhong and Yuqiang Li},
  year    = {2024},
  journal = {arXiv preprint arXiv: 2402.06852}
}

@article{liu2023improving,
  title   = {Improving Large Language Model Fine-tuning for Solving Math Problems},
  author  = {Yixin Liu and Avi Singh and C. Daniel Freeman and John D. Co-Reyes and Peter J. Liu},
  year    = {2023},
  journal = {arXiv preprint arXiv: 2310.10047}
}

[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Lamalab Tool and Paper Notes",
    "section": "",
    "text": "In our group seminars, we have a tradition of dedicating a few minutes to showcase tools/software/tricks/methods that we find useful. This repository is a collection of these tool minutes.\n\n\n\n\nGuo, Jeff, and Philippe Schwaller. 2024. “It Takes Two to Tango: Directly Optimizing for Constrained Synthesizability in Generative Molecular Design.” arXiv. https://doi.org/10.48550/ARXIV.2410.11527.\n\n\nLandrum, Gregory A., Jessica Braun, Paul Katzberger, Marc T. Lehner, and Sereina Riniker. 2024. “Lwreg: A Lightweight System for Chemical Registration and Data Storage.” Journal of Chemical Information and Modeling 64 (16): 6247–52. https://doi.org/10.1021/acs.jcim.4c01133.\n\n\nOrsi, Markus, and Jean-Louis Reymond. 2024. “One Chiral Fingerprint to Find Them All.” Journal of Cheminformatics 16 (1): 53.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Tool and paper minutes</span>"
    ]
  },
  {
    "objectID": "tools/hydra.html",
    "href": "tools/hydra.html",
    "title": "Hydra",
    "section": "",
    "text": "Getting started\nHydra is an open-source Python framework that simplifies the development of research and other complex applications. The key feature is the ability to dynamically create a hierarchical configuration by composition and override it through config files and the command line. The name Hydra comes from its ability to run multiple similar jobs - much like a Hydra with multiple heads.",
    "crumbs": [
      "Tools",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Hydra</span>"
    ]
  },
  {
    "objectID": "tools/hydra.html#getting-started",
    "href": "tools/hydra.html#getting-started",
    "title": "Hydra",
    "section": "",
    "text": "Key features:\n\nHierarchical configuration composable from multiple sources\nConfiguration can be specified or overridden from the command line\nDynamic command line tab completion\nRun your application locally or launch it to run remotely\nRun multiple jobs with different arguments with a single command\n\n\n\nInstallation\npip install hydra-core --upgrade\n\n\nBasic example\nConfig, e.g., in conf/config.yaml:\ndb:\ndriver: mysql\nuser: omry\npass: secret\n\n\n\n\nGuo, Jeff, and Philippe Schwaller. 2024. “It Takes Two to Tango: Directly Optimizing for Constrained Synthesizability in Generative Molecular Design.” arXiv. https://doi.org/10.48550/ARXIV.2410.11527.\n\n\nLandrum, Gregory A., Jessica Braun, Paul Katzberger, Marc T. Lehner, and Sereina Riniker. 2024. “Lwreg: A Lightweight System for Chemical Registration and Data Storage.” Journal of Chemical Information and Modeling 64 (16): 6247–52. https://doi.org/10.1021/acs.jcim.4c01133.\n\n\nOrsi, Markus, and Jean-Louis Reymond. 2024. “One Chiral Fingerprint to Find Them All.” Journal of Cheminformatics 16 (1): 53.",
    "crumbs": [
      "Tools",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Hydra</span>"
    ]
  },
  {
    "objectID": "tools/ip_rotator.html",
    "href": "tools/ip_rotator.html",
    "title": "IP Rotator",
    "section": "",
    "text": "GitHub repository\niq-requests-rotator",
    "crumbs": [
      "Tools",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>IP Rotator</span>"
    ]
  },
  {
    "objectID": "tools/ip_rotator.html#github-repository",
    "href": "tools/ip_rotator.html#github-repository",
    "title": "IP Rotator",
    "section": "",
    "text": "Example usage:\nimport requests\nfrom requests_ip_rotator import ApiGatewaywith ApiGateway(\"https://site.com\") as g:\n    session = requests.Session()\n    session.mount(\"https://site.com\", g)    response = session.get(\"https://site.com/index.php\")\n    print(response.status_code)\n\n\n\n\nGuo, Jeff, and Philippe Schwaller. 2024. “It Takes Two to Tango: Directly Optimizing for Constrained Synthesizability in Generative Molecular Design.” arXiv. https://doi.org/10.48550/ARXIV.2410.11527.\n\n\nLandrum, Gregory A., Jessica Braun, Paul Katzberger, Marc T. Lehner, and Sereina Riniker. 2024. “Lwreg: A Lightweight System for Chemical Registration and Data Storage.” Journal of Chemical Information and Modeling 64 (16): 6247–52. https://doi.org/10.1021/acs.jcim.4c01133.\n\n\nOrsi, Markus, and Jean-Louis Reymond. 2024. “One Chiral Fingerprint to Find Them All.” Journal of Cheminformatics 16 (1): 53.",
    "crumbs": [
      "Tools",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>IP Rotator</span>"
    ]
  },
  {
    "objectID": "tools/polars.html",
    "href": "tools/polars.html",
    "title": "Polars",
    "section": "",
    "text": "An alternative to pandas\nThe advantages of polars can be directly seen in the image above. It is clear from the graph that Polars perform faster than Pandas for most operations. This is particularly true for the GroupBy operation, where Polars is nearly 20 times faster than Pandas. The Filter operation is also significantly faster in Polars, while Create operations are somewhat faster in Pandas. Overall, Polars seems to be a more performant library for data manipulation, particularly for large datasets.",
    "crumbs": [
      "Tools",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Polars</span>"
    ]
  },
  {
    "objectID": "tools/polars.html#syntax-example",
    "href": "tools/polars.html#syntax-example",
    "title": "Polars",
    "section": "Syntax example",
    "text": "Syntax example\nimport polars as pl\n\nq = (\n    pl.scan_csv(\"docs/data/iris.csv\")\n    .filter(pl.col(\"sepal_length\") &gt; 5)\n    .group_by(\"species\")\n    .agg(pl.all().sum())\n)\n\ndf = q.collect()\n\n\n\n\nGuo, Jeff, and Philippe Schwaller. 2024. “It Takes Two to Tango: Directly Optimizing for Constrained Synthesizability in Generative Molecular Design.” arXiv. https://doi.org/10.48550/ARXIV.2410.11527.\n\n\nLandrum, Gregory A., Jessica Braun, Paul Katzberger, Marc T. Lehner, and Sereina Riniker. 2024. “Lwreg: A Lightweight System for Chemical Registration and Data Storage.” Journal of Chemical Information and Modeling 64 (16): 6247–52. https://doi.org/10.1021/acs.jcim.4c01133.\n\n\nOrsi, Markus, and Jean-Louis Reymond. 2024. “One Chiral Fingerprint to Find Them All.” Journal of Cheminformatics 16 (1): 53.",
    "crumbs": [
      "Tools",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Polars</span>"
    ]
  },
  {
    "objectID": "tools/thunder_client.html",
    "href": "tools/thunder_client.html",
    "title": "Thunder Client",
    "section": "",
    "text": "Installation\nThunder Client is a lightweight alternative to Postman that can be used directly from VSCode.\nYou can use it to test your API endpoints.\nFor an example, see this video.\nInstall the Thunder client extension from the marketplace.",
    "crumbs": [
      "Tools",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Thunder Client</span>"
    ]
  },
  {
    "objectID": "tools/thunder_client.html#installation",
    "href": "tools/thunder_client.html#installation",
    "title": "Thunder Client",
    "section": "",
    "text": "Guo, Jeff, and Philippe Schwaller. 2024. “It Takes Two to Tango: Directly Optimizing for Constrained Synthesizability in Generative Molecular Design.” arXiv. https://doi.org/10.48550/ARXIV.2410.11527.\n\n\nLandrum, Gregory A., Jessica Braun, Paul Katzberger, Marc T. Lehner, and Sereina Riniker. 2024. “Lwreg: A Lightweight System for Chemical Registration and Data Storage.” Journal of Chemical Information and Modeling 64 (16): 6247–52. https://doi.org/10.1021/acs.jcim.4c01133.\n\n\nOrsi, Markus, and Jean-Louis Reymond. 2024. “One Chiral Fingerprint to Find Them All.” Journal of Cheminformatics 16 (1): 53.",
    "crumbs": [
      "Tools",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Thunder Client</span>"
    ]
  },
  {
    "objectID": "tools/tmux.html",
    "href": "tools/tmux.html",
    "title": "tmux",
    "section": "",
    "text": "Installation\ntmux is a terminal multiplexer. It lets you switch easily between several programs in one terminal, detach them (they keep running in the background) and reattach them to a different terminal. And do a lot more.\nor on Mac",
    "crumbs": [
      "Tools",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>tmux</span>"
    ]
  },
  {
    "objectID": "tools/tmux.html#installation",
    "href": "tools/tmux.html#installation",
    "title": "tmux",
    "section": "",
    "text": "sudo apt install tmux\n\nbrew install tmux",
    "crumbs": [
      "Tools",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>tmux</span>"
    ]
  },
  {
    "objectID": "tools/tmux.html#usage",
    "href": "tools/tmux.html#usage",
    "title": "tmux",
    "section": "Usage",
    "text": "Usage\nLet’s assume you are via ssh on a remote server and you want to run a long running process. You can use tmux to run the process in a session and then detach from it. You can then log out and log back in later to check on the process. Your process will still be running, even if your ssh session is closed.\n\nOn the remote server\ntmux new -s myprocess\nThen run your process. When you are done, detach from the session by pressing Ctrl+b and then d.\n\n\nOn the remote server later\ntmux ls\nThis will list all the sessions. You can then reattach to the session you want by typing:\ntmux attach -t myprocess\n\n\nPanes\nYou can split your terminal into panes. This is useful if you want to run multiple processes in the same terminal. You can split the terminal vertically by pressing Ctrl+b and then \" or horizontally by pressing Ctrl+b and then %.\nTo move panes around, you can use Ctrl+b and then o to cycle through the panes.\n\n\n\n\nGuo, Jeff, and Philippe Schwaller. 2024. “It Takes Two to Tango: Directly Optimizing for Constrained Synthesizability in Generative Molecular Design.” arXiv. https://doi.org/10.48550/ARXIV.2410.11527.\n\n\nLandrum, Gregory A., Jessica Braun, Paul Katzberger, Marc T. Lehner, and Sereina Riniker. 2024. “Lwreg: A Lightweight System for Chemical Registration and Data Storage.” Journal of Chemical Information and Modeling 64 (16): 6247–52. https://doi.org/10.1021/acs.jcim.4c01133.\n\n\nOrsi, Markus, and Jean-Louis Reymond. 2024. “One Chiral Fingerprint to Find Them All.” Journal of Cheminformatics 16 (1): 53.",
    "crumbs": [
      "Tools",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>tmux</span>"
    ]
  },
  {
    "objectID": "tools/trimean.html",
    "href": "tools/trimean.html",
    "title": "Robust statistics and Trimean",
    "section": "",
    "text": "from scipy.stats import skewnorm\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nLet’s generate some data that might be something we find in the real world.\n\nskew_magnitude = -6\narr = skewnorm.rvs(skew_magnitude, loc=0, scale=1000, size=100000)\n\n(The skew is a third-order moment.)\n\nplt.hist(arr, bins=100, density=True)\nplt.show()\n\n\n\n\n\n\n\n\nLet’s get a very common measure of central tendency:\n\nnp.mean(arr)\n\n-789.5809069979605\n\n\nThe mean overstates the central tendency because of the skew.\nThe mean is defined as\n\\[\\bar{x} = \\frac{1}{n} \\sum_{i=1}^n x_i\\]\nand treats all numbers equally. No matter how big or small.\nOne can “fix” this by looking at “robust” statistics that are often rank based. Rank based means that we sort the data and then base our statistics on the rank of the data. In this way, they are no longer sensitive to outliers.\n\ndef interquartile_range(arr):\n    q1 = np.percentile(arr, 25)\n    q3 = np.percentile(arr, 75)\n    return q3 - q1\n\nprint(\"Median\", np.percentile(arr, 50))\nprint(\"Interquartile Range\", interquartile_range(arr))\nprint(\"Mean\", arr.mean())\nprint(\"Standard Deviation\", arr.std())\n\nMedian -679.7024551978025\nInterquartile Range 834.2816858677052\nMean -789.5809069979605\nStandard Deviation 614.9363837309692\n\n\nA very nice measure of centrality is the so-called trimean.\n\n“An advantage of the trimean as a measure of the center (of a distribution) is that it combines the median’s emphasis on center values with the midhinge’s attention to the extremes.”\n— Herbert F. Weisberg, Central Tendency and Variability\n\nIt is defined as\n\\[\n\\text{trimean} = \\frac{Q_1 + 2Q_2 + Q_3}{4}\n\\]\nwhere \\(Q_1\\) is the first quartile, \\(Q_2\\) is the median, and \\(Q_3\\) is the third quartile.\n\ndef trimean(arr):\n    q1 = np.percentile(arr, 25)\n    q3 = np.percentile(arr, 75)\n    median = np.percentile(arr, 50)\n    return (q1 + 2*median + q3)/4\n\nprint(\"Trimean\", trimean(arr))\n\nTrimean -708.4430042323374\n\n\n\n\n\n\nGuo, Jeff, and Philippe Schwaller. 2024. “It Takes Two to Tango: Directly Optimizing for Constrained Synthesizability in Generative Molecular Design.” arXiv. https://doi.org/10.48550/ARXIV.2410.11527.\n\n\nLandrum, Gregory A., Jessica Braun, Paul Katzberger, Marc T. Lehner, and Sereina Riniker. 2024. “Lwreg: A Lightweight System for Chemical Registration and Data Storage.” Journal of Chemical Information and Modeling 64 (16): 6247–52. https://doi.org/10.1021/acs.jcim.4c01133.\n\n\nOrsi, Markus, and Jean-Louis Reymond. 2024. “One Chiral Fingerprint to Find Them All.” Journal of Cheminformatics 16 (1): 53.",
    "crumbs": [
      "Tools",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Robust statistics and Trimean</span>"
    ]
  },
  {
    "objectID": "tools/pandarallel.html",
    "href": "tools/pandarallel.html",
    "title": "Easy fast .apply for pandas",
    "section": "",
    "text": "apply in pandas is slow. This is the case because it does not take advantage of vectorization. That means, in general, if you have something for which there is a built-in pandas (or numpy) function, you should use that instead of apply, because those functions will be optimized and typically vectorized.\nThe pandarallel package allows you to parallelize apply on a pandas DataFrame or Series object. It does this by using multiprocessing. However, since it uses multiple processes, it will use more memory than a simple apply.\nIf your data just barley fits in memory, you should not use pandarallel. However, if it does fit in memory, and you have a lot of cores, then pandarallel can speed up your code significantly with just changing one line of code.\n\nfrom pandarallel import pandarallel\n\npandarallel.initialize(progress_bar=True)\n\n# df.apply(func)\ndf.parallel_apply(func)\n\n\n\n\n\nGuo, Jeff, and Philippe Schwaller. 2024. “It Takes Two to Tango: Directly Optimizing for Constrained Synthesizability in Generative Molecular Design.” arXiv. https://doi.org/10.48550/ARXIV.2410.11527.\n\n\nLandrum, Gregory A., Jessica Braun, Paul Katzberger, Marc T. Lehner, and Sereina Riniker. 2024. “Lwreg: A Lightweight System for Chemical Registration and Data Storage.” Journal of Chemical Information and Modeling 64 (16): 6247–52. https://doi.org/10.1021/acs.jcim.4c01133.\n\n\nOrsi, Markus, and Jean-Louis Reymond. 2024. “One Chiral Fingerprint to Find Them All.” Journal of Cheminformatics 16 (1): 53.",
    "crumbs": [
      "Tools",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Easy fast `.apply` for pandas</span>"
    ]
  },
  {
    "objectID": "tools/bfg.html",
    "href": "tools/bfg.html",
    "title": "BFG Repo-Cleaner",
    "section": "",
    "text": "If you did not take with your .gitignore or just used git add . you might have by accident committed large files. This might lead to an error like\nremote: error: See https://gh.io/lfs for more information.\nremote: error: File reports/gemini-pro/.langchain.db is 123.01 MB; this exceeds GitHub's file size limit of 100.00 MB\nremote: error: GH001: Large files detected. You may want to try Git Large File Storage - https://git-lfs.github.com.\nTo github.com:lamalab-org/chem-bench.git\n ! [remote rejected]     kjappelbaum/issue258 -&gt; kjappelbaum/issue258 (pre-receive hook declined)\nerror: failed to push some refs to 'github.com:lamalab-org/chem-bench.git'\nTo fix this, you need to remove the large files. A convenient tool for doing this is BFG.\nOnce you download the file you can run it using something like\njava -jar ~/Downloads/bfg-1.14.0.jar --strip-blobs-bigger-than 100M --no-blob-protection\nto remove large files.\nNote that this here uses --no-blob-protection as BFG defaults to not touching the last commit.\nAfter the BFG run, it will prompt you to run something like\ngit reflog expire --expire=now --all && git gc --prune=now --aggressive\n\n\n\n\nGuo, Jeff, and Philippe Schwaller. 2024. “It Takes Two to Tango: Directly Optimizing for Constrained Synthesizability in Generative Molecular Design.” arXiv. https://doi.org/10.48550/ARXIV.2410.11527.\n\n\nLandrum, Gregory A., Jessica Braun, Paul Katzberger, Marc T. Lehner, and Sereina Riniker. 2024. “Lwreg: A Lightweight System for Chemical Registration and Data Storage.” Journal of Chemical Information and Modeling 64 (16): 6247–52. https://doi.org/10.1021/acs.jcim.4c01133.\n\n\nOrsi, Markus, and Jean-Louis Reymond. 2024. “One Chiral Fingerprint to Find Them All.” Journal of Cheminformatics 16 (1): 53.",
    "crumbs": [
      "Tools",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>BFG Repo-Cleaner</span>"
    ]
  },
  {
    "objectID": "tools/showyourwork.html",
    "href": "tools/showyourwork.html",
    "title": "showyourwork",
    "section": "",
    "text": "showyourwork : https://github.com/showyourwork is a framework for building reproducible papers. The package works on a combination of Tex and Python code, where you can on the fly modify your plots.\nThe pre-requisites are: 1. define a conda environment with the packages are that necessary for plotting 2. use the \\script{}, \\variable{} and other commands to link your figures/tables to a Python script. 3. compile the paper\n\n\n\n\nGuo, Jeff, and Philippe Schwaller. 2024. “It Takes Two to Tango: Directly Optimizing for Constrained Synthesizability in Generative Molecular Design.” arXiv. https://doi.org/10.48550/ARXIV.2410.11527.\n\n\nLandrum, Gregory A., Jessica Braun, Paul Katzberger, Marc T. Lehner, and Sereina Riniker. 2024. “Lwreg: A Lightweight System for Chemical Registration and Data Storage.” Journal of Chemical Information and Modeling 64 (16): 6247–52. https://doi.org/10.1021/acs.jcim.4c01133.\n\n\nOrsi, Markus, and Jean-Louis Reymond. 2024. “One Chiral Fingerprint to Find Them All.” Journal of Cheminformatics 16 (1): 53.",
    "crumbs": [
      "Tools",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>showyourwork</span>"
    ]
  },
  {
    "objectID": "papers/llms_review.html",
    "href": "papers/llms_review.html",
    "title": "LLMs in Chemistry",
    "section": "",
    "text": "1. Introduction\n\nThree key challenges in chemistry:\n\nProperty prediction: Predicting physical/chemical properties of compounds\nProperty-directed molecule generation: Creating molecules with desired properties\nSynthesis prediction: Determining optimal ways to make target molecules\n\nChemical space is vast: Estimated 10^180 possible stable compounds – requires AI to explore effectively\nAutomation: The fourth challenge connecting the other three\n\n\n\n2. Large Language Models (LLMs) Fundamentals\n\nWhat are LLMs?: Deep neural networks trained on vast data, primarily using transformer architecture\nTransformer architecture consists of:\n\nInput embeddings: Converts the input tokens into a numerical/vector representation\nAttention mechanism: Allows the model to focus on relevant parts of input\nPositional encoding: Helps maintain sequence order information\n\nTraining:\n\nPretraining Approaches\n\n\nSelf-supervised learning: Learning patterns from unlabeled chemical data (like SMILES strings) without explicit annotations\nDomain-specific Vocabulary: Creating specialized tokenization for chemical notation rather than using general language vocabularies\n\n\nFine-tuning Approaches\n\n\nSupervised Fine-tuning: Training on labeled datasets of specific chemical properties or reactions\nInstruction Tuning: Teaching models to follow specific chemistry-related instructions through examples\nParameter-Efficient Fine-Tuning (PEFT): Updating only a small subset of model parameters for chemistry tasks\n\nThree main transformer architectures:\n\nEncoder-only (bidirectional context) (e.g., BERT): Good for classification and property prediction (understanding)\nDecoder-only (causal context) (e.g., GPT): Excellent for generative tasks and molecule design (generation)\nEncoder-decoder (e.g., BART): Ideal for translation tasks like reaction prediction\n\n\n\n\n3. The Data Challenge in Chemistry\n\nLimited chemical data: Chemistry datasets are orders of magnitude smaller than general language datasets\n\nGeneral LLMs (e.g., LLaMA2): Trained on trillions of tokens\nChemical datasets: Only billions of tokens (mostly hypothetical compounds)\nVerified synthesized compounds: ~5 orders of magnitude less data than general LLMs\n\nData quality issues:\n\nCurrent benchmarks like MoleculeNet have errors and inconsistencies\nMany datasets include theoretical rather than experimental values\nNeed for more reliable, experimentally-verified data\n\nChemical representation: Models use text-based notations like SMILES, SELFIES, and InChI\n\n\n\n4. LLMs for Chemistry Applications\n\nProperty Prediction (Encoder-only Models)\n\n\n\n\n\n\n\n\n\n\n\nName\nArchitecture\nTraining Data\nAim\nAchievements\nInnovations\n\n\n\n\nMol-BERT\nBERT\n4B SMILES from ZINC15/ChEMBL27\nProperty prediction\nOutperformed existing methods by 2%+ on benchmark datasets\nUsed Morgan fingerprint fragments as “words” and molecules as “sentences”\n\n\nMolFormer\nBERT\n1.1B molecules from ZINC/PubChem\nMolecular property prediction\nOutperformed GNNs on MoleculeNet tasks\nImplemented rotary positional embeddings for better sequence relationships\n\n\nChemBERTa\nRoBERTa\n10M SMILES from PubChem\nProperty prediction\nShowed performance improved with dataset size\nExplored impact of tokenization strategies and representation types\n\n\nChemBERTa-2\nRoBERTa\n77M SMILES from PubChem\nFoundational model for multiple tasks\nMatched state-of-the-art on MoleculeNet\nAdded Multi-Task Regression to pretraining\n\n\nSELFormer\nRoBERTa\n2M drug-like compounds\nProperty prediction\nOutperformed competitors on several tasks\nUsed SELFIES instead of SMILES for better robustness\n\n\nSolvBERT\nBERT\n1M solute-solvent pairs\nSolubility/solvation prediction\nComparable to graph-based models\nPredicted 3D-dependent properties from 1D representations\n\n\nMatSciBERT\nBERT\n150K materials science papers\nMaterial property extraction\n3% improvement over SciBERT\nFine-tuned for materials science domain\n\n\n\n\n\nMolecule Generation (Decoder-only Models)\n\n\n\n\n\n\n\n\n\n\n\nName\nArchitecture\nTraining Data\nAim\nAchievements\nInnovations\n\n\n\n\nMolGPT\nGPT\nMOSES/GuacaMol datasets\nMolecular generation\nBetter performance than VAE approaches\nUsed masked self-attention for long-range dependencies in SMILES\n\n\nChemGPT\nGPT-neo\n10M molecules from PubChem\nExplore hyperparameter tuning\nRefined generative models for specific domains\nExplored impact of data scale on generation\n\n\ncMolGPT\nGPT\nMOSES\nTarget-specific molecule generation\nGenerated compounds with binding activity\nIntegrated protein-ligand interactions with conditional generation\n\n\niupacGPT\nGPT\nIUPAC names\nGenerate molecules using IUPAC\nHuman-readable molecular generation\nUsed IUPAC names directly instead of SMILES\n\n\nTaiga\nGPT + RL\nVarious\nProperty-directed molecule generation\nOptimized molecules for targeted properties\nEmployed REINFORCE for property optimization\n\n\nLlaSMol\nLLaMA/Mistral\nSMolInstruct\nMulti-task molecular modeling\nState-of-the-art on property prediction\nUsed parameter-efficient fine-tuning on pretrained models\n\n\nGPTChem\nGPT-3\nMultiple benchmarks\nProperty prediction and inverse design\nCompeted with specialized models\nShowed generalist models can perform chemical tasks\n\n\n\n\n\nSynthesis Prediction (Encoder-decoder Models)\n\nEvolution from template-based to template-free approaches:\n\nTemplate-based: Limited to known reaction types\nSemi-template-based: More flexible but still constrained\nTemplate-free: Learn directly from data without predefined rules\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nArchitecture\nTraining Data\nAim\nAchievements\nInnovations\n\n\n\n\nMolecular Transformer\nTransformer\nUSPTO datasets\nSynthesis prediction\nOutperformed prior algorithms\nRequired no handcrafted rules for chemical transformations\n\n\nChemformer\nBART\n100M SMILES from ZINC-15\nMulti-task chemical modeling\nState-of-the-art in synthesis and property tasks\nEmphasized computational efficiency through pretraining\n\n\nText+Chem T5\nT5\n11.5M-33.5M samples\nMulti-task chemical processing\nEffective in diverse synthesis tasks\nCombined reaction data from multiple sources\n\n\nMOLGEN\nBART\n100M molecules (SELFIES)\nMolecular generation\nAddressed bias against natural product-like molecules\nUsed domain-specific molecular prefix tuning\n\n\nReactionT5\nT5\nZINC and ORD\nReaction prediction\nImproved synthesis prediction\nCombined property and reaction prediction\n\n\nTSChem\nT5\nUSPTO\nVersatile chemical modeling\nStrong performance on multiple tasks\nIntegrated property and synthesis prediction\n\n\n\n\n\n\n5. Multi-modal Approaches\n\nText2Mol (2021): Connects molecular representations with text descriptions\nMolT5 (2022): Generates molecular captions and predicts structures from descriptions\nChallenges:\n\nBuilding quality datasets pairing chemical structures with descriptions\nMultiple valid ways to describe molecules (therapeutic effects, structure, etc.)\nNeed for chemical domain expertise to evaluate outputs\n\nApplications: Molecular retrieval from text queries, structure generation from descriptions ## Multi-modal Models\n\n\n\n\n\n\n\n\n\n\n\n\nName\nArchitecture\nTraining Data\nAim\nAchievements\nInnovations\n\n\n\n\nText2Mol\nSciBERT w/ decoder\nCheBI-20\nConnecting text with molecules\nEnabled retrieval of molecules using text\nCreated paired dataset linking molecules with descriptions\n\n\nMolT5\nT5\nC4 dataset\nMolecule captioning and generation\nGenerated structures from descriptions\nUsed denoising objective to handle complex chemical descriptions\n\n\nBioT5+\nT5\nZINC20, UniRef50, PubMed, etc.\nBridging text and molecules\nMulti-task performance across domains\nIntegrated diverse biological and chemical data sources\n\n\nCLAMP\nCombined model\nBiochemical data\nPredict biochemical activity\nEnhanced prediction with language\nCombined separate chemical and language modules\n\n\nGIT-Mol\nMulti-modal\nGraphs, images, text\nIntegrated chemical understanding\nImproved accuracy with multiple data types\nCombined three modalities for better chemical modeling\n\n\n\n\n\n6. Conclusion and Future Directions\n\nLLMs show tremendous potential for transforming chemical research\nIntegration of multiple models can address different aspects of chemical discovery\nKey to progress: Better data, improved benchmarks, and enhanced interpretability\nMoving toward autonomous systems that combine the strengths of different models\n\n\n\n\n\nGuo, Jeff, and Philippe Schwaller. 2024. “It Takes Two to Tango: Directly Optimizing for Constrained Synthesizability in Generative Molecular Design.” arXiv. https://doi.org/10.48550/ARXIV.2410.11527.\n\n\nLandrum, Gregory A., Jessica Braun, Paul Katzberger, Marc T. Lehner, and Sereina Riniker. 2024. “Lwreg: A Lightweight System for Chemical Registration and Data Storage.” Journal of Chemical Information and Modeling 64 (16): 6247–52. https://doi.org/10.1021/acs.jcim.4c01133.\n\n\nOrsi, Markus, and Jean-Louis Reymond. 2024. “One Chiral Fingerprint to Find Them All.” Journal of Cheminformatics 16 (1): 53.",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>LLMs in Chemistry</span>"
    ]
  },
  {
    "objectID": "papers/llm4mat.html",
    "href": "papers/llm4mat.html",
    "title": "Leveraging language representation for materials exploration and discovery",
    "section": "",
    "text": "Why discussing this paper?\nI chose Jiaxing et al.’s paper for our journal club because",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Leveraging language representation for materials exploration and discovery</span>"
    ]
  },
  {
    "objectID": "papers/llm4mat.html#why-discussing-this-paper",
    "href": "papers/llm4mat.html#why-discussing-this-paper",
    "title": "Leveraging language representation for materials exploration and discovery",
    "section": "",
    "text": "LLMs successfully applied in other domain, interesting to see what can be done in material science\nOne among the few paper where LLMs are applied in materials science for actual material discovery.\nNice embedding figures",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Leveraging language representation for materials exploration and discovery</span>"
    ]
  },
  {
    "objectID": "papers/llm4mat.html#context",
    "href": "papers/llm4mat.html#context",
    "title": "Leveraging language representation for materials exploration and discovery",
    "section": "Context",
    "text": "Context\n\nMaterial space is not completely explored. And there is possibility of finding better materials in many applications.\nML recommender systems for exploring material spaces are already there but not many using “LLMs”\nLLM framework for recommending prototype crystal structures and later validate through first-principles calculations and experiments\nWhy LLMs? - Universal task agnostic representations",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Leveraging language representation for materials exploration and discovery</span>"
    ]
  },
  {
    "objectID": "papers/llm4mat.html#some-previous-llm-models",
    "href": "papers/llm4mat.html#some-previous-llm-models",
    "title": "Leveraging language representation for materials exploration and discovery",
    "section": "Some Previous LLM Models",
    "text": "Some Previous LLM Models\n\nMatSciBERT\nMatsciBERT was pretrained on whole sections of more than 1 million materials science articles with masked language modelling.\n\n\nMatBERT\nMatBERT was trained by sampling 50 million paragraphs from 2 million articles masked language modelling.\n\n\nWord2Vec\nMat2Vec was trained similarly as Word2vec training through skip-gram with negative sampling. Each word is embedded into a 200-dimensional vector.",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Leveraging language representation for materials exploration and discovery</span>"
    ]
  },
  {
    "objectID": "papers/llm4mat.html#problem-setting",
    "href": "papers/llm4mat.html#problem-setting",
    "title": "Leveraging language representation for materials exploration and discovery",
    "section": "Problem setting",
    "text": "Problem setting\n\nHand-crafted features and specialized structural models have limitations in providing universal and task-agnostic representations within the vast material space.\nAdditional contexts are also very useful. for eg: (doping, temperature, synthesis conditions)\nIn materials exploration and discovery context:\n\n\neffective representations of both chemical and structural complexity, (ii) successful recall of relevant candidates to property of interest\naccurate candidate ranking based on multiple desired functional properties.",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Leveraging language representation for materials exploration and discovery</span>"
    ]
  },
  {
    "objectID": "papers/llm4mat.html#approach",
    "href": "papers/llm4mat.html#approach",
    "title": "Leveraging language representation for materials exploration and discovery",
    "section": "Approach",
    "text": "Approach\nThe authors propose a two-step funnel based approach\n\n\nRECALL - Given a material finding similar material from a set of materials\n\n\nRANKING - Based on functional properties rank the recalled materials\n\n\n\n\n\nFunnel based recommender framework\n\n\n\nRecalling similar materials\nThe authors use Robocrystallographer representation to describe the material. Encode the material description using pretrained MatBERT (compared other encoders as well), and use this as a feature vector\n\nUse a Query material (a well studied known material with property of interest).\nEncode all material in database and Query material (Robocrystallographer + MatBERT)\nLook at cosine similarity of feature vectors (material in database with Query material)\n\n\n\n\nRecall material based on cosine similarity\n\n\n\n\nRanking potential materials\nBased on multiple properties the recalled materials are ranked.\nUsually for any application, and in this paper, for thermoelectric material as well many properties are important hence a ranker based on performance on different functional aspects.\n\nAuthor train a Multitask Mixture of Expert Model (MMoEM), using multitask learning to rank the materials.\n\n\n\n\nRank material based on cosine similarity",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Leveraging language representation for materials exploration and discovery</span>"
    ]
  },
  {
    "objectID": "papers/llm4mat.html#results",
    "href": "papers/llm4mat.html#results",
    "title": "Leveraging language representation for materials exploration and discovery",
    "section": "Results",
    "text": "Results\nThe authors perform ablations to understand the importance of the different components of their model. While there are some differences, the differences are not drastic.\n\nAblation on Representation suitable for RECALL step\nTwo set of models 1. Uses only composition of materials Baseline: Mat2Vec\nA(Composition)–&gt; B(MatBERT)\n\nUses Both composition and structure Baseline: CrystalNN Fingerprint\n\nA(Material)–&gt; B(RoboCrystallographer)–&gt; C(MatBERT)\n\nEmbeddings from composition only and Composition + Structure\nStructure level representations exhibit more distinct separation (well-defined domains) by material groups\n\n\n\nUMAP of embeddings from different representations\n\n\nFor further evaluation, authors evaluated material embedding performance on downstream property prediction tasks.\nThe task models were multi-layer perceptrons (MLPs) with meanabsolute-error (MAE) training loss.\nThe tasks consisted of band gap, energy per atom, bulk modulus, shear modulus, Debye temperature, and coefficient of thermal expansion from AFLOW dataset.\n\n\n\nProperty prediction using embeddings from different representations\n\n\n\n\n\nFinding similar materials\nStarting with known materials with favorable properties for TEs such as PbTe, we analyzed the top recalled candidates and found significantly different predicted figure-of-merit zT distributions from selected baseline representations.\n\n\n\nDistributions of predicted zT of the top-100 recalled candidates for PbTe as the query material predicted by MatBERT\n\n\n\n\nRanking potential materials\nLearning from multiple related tasks provides superior performance over single-task learning by modeling task-specific objectives and cross-task relationships.\nIn addition to the embeddings derived from language models, the authors added further information based on context (one hot encoded temperature)\n\n\n\nMulti-task learning framework for material property prediction.\n\n\n\n\n\nPerformance for 6 material property prediction tasks between single-task models and MMoE using composition or structure embeddings.",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Leveraging language representation for materials exploration and discovery</span>"
    ]
  },
  {
    "objectID": "papers/llm4mat.html#takeaways",
    "href": "papers/llm4mat.html#takeaways",
    "title": "Leveraging language representation for materials exploration and discovery",
    "section": "Takeaways",
    "text": "Takeaways\n\nMight not need a Language model for this task\nGood to see that some of the materials where later tested in lab\nComposition vs Composition + structure not convincing.\n\n\n\n\n\nGuo, Jeff, and Philippe Schwaller. 2024. “It Takes Two to Tango: Directly Optimizing for Constrained Synthesizability in Generative Molecular Design.” arXiv. https://doi.org/10.48550/ARXIV.2410.11527.\n\n\nLandrum, Gregory A., Jessica Braun, Paul Katzberger, Marc T. Lehner, and Sereina Riniker. 2024. “Lwreg: A Lightweight System for Chemical Registration and Data Storage.” Journal of Chemical Information and Modeling 64 (16): 6247–52. https://doi.org/10.1021/acs.jcim.4c01133.\n\n\nOrsi, Markus, and Jean-Louis Reymond. 2024. “One Chiral Fingerprint to Find Them All.” Journal of Cheminformatics 16 (1): 53.",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Leveraging language representation for materials exploration and discovery</span>"
    ]
  },
  {
    "objectID": "papers/chem_yield_prediction_2024.html",
    "href": "papers/chem_yield_prediction_2024.html",
    "title": "Uncertainty-Aware Yield Prediction with Multimodal Molecular Features",
    "section": "",
    "text": "Why discussing this paper?\nI chose Chen et al.’s paper (Chen et al. 2024) for our journal club because",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Uncertainty-Aware Yield Prediction with Multimodal Molecular Features</span>"
    ]
  },
  {
    "objectID": "papers/chem_yield_prediction_2024.html#why-discussing-this-paper",
    "href": "papers/chem_yield_prediction_2024.html#why-discussing-this-paper",
    "title": "Uncertainty-Aware Yield Prediction with Multimodal Molecular Features",
    "section": "",
    "text": "An important and interesting problem in chemistry\nUses many of the techniques we care about in our group",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Uncertainty-Aware Yield Prediction with Multimodal Molecular Features</span>"
    ]
  },
  {
    "objectID": "papers/chem_yield_prediction_2024.html#context",
    "href": "papers/chem_yield_prediction_2024.html#context",
    "title": "Uncertainty-Aware Yield Prediction with Multimodal Molecular Features",
    "section": "Context",
    "text": "Context\nPredicting the yield of chemical reactions is a crucial task in organic chemistry. It can help to optimize the synthesis of new molecules, reduce the number of experiments needed, and save time and resources. However, predicting the yield of a reaction is challenging due to the complexity of chemical reactions and the large number of factors that can influence the outcome.",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Uncertainty-Aware Yield Prediction with Multimodal Molecular Features</span>"
    ]
  },
  {
    "objectID": "papers/chem_yield_prediction_2024.html#prior-work",
    "href": "papers/chem_yield_prediction_2024.html#prior-work",
    "title": "Uncertainty-Aware Yield Prediction with Multimodal Molecular Features",
    "section": "Prior work",
    "text": "Prior work\n\nAhneman et al. (2018)\nAhneman et al. (Ahneman et al. 2018) reported in Science a random forest model that predicts the yield of chemical reactions in a high-throughput dataset (palladium-catalyzed Buchwald-Hartwig cross-coupling reactions). For this, the authors created a set of features using computational techniques.\nA very interesting aspect of this work is the subsequent exchange with Chuang and Keiser (Chuang and Keiser 2018) who point out that the chemical features used in the work by Ahneman et al. perform not distinguishably better than non-meaningful features.\n\n\n\nFigure taken from Chuang and Keiser’s paper(Chuang and Keiser 2018) illustrating models trained with various featurization approaches.\n\n\n\n\nSchwaller et al. (2020, 2021)\nSchwaller et al. (Schwaller et al. 2020, 2021) utilized BERT models with a regression head to predict yields based on reaction SMILES.\nThey observed multiple interesting effects:\n\nThe performance on high-throughput datasets is good, on USPTO datasets the models are not predictive (\\(R^2\\) on a random split of 0.117 for the gram scale)\nThe yield distribution depends on the scale, which might be due to reaction at larger scale being better optimized\n\n\n\n\nFigure taken from Schwaller et al. (Schwaller et al. 2021) illustrating the distribution of yields on different scales.\n\n\n\n\nKwon et al. (2022)\nKwon et al. (Kwon et al. 2022), in contrast, used graph neural networks to predict yields. They pass reactants and products through a graph neural network and concatenate the embeddings to predict the yield. They train on a similar loss as the work at hand (but use also use dropout Monte-Carlo (Gal and Ghahramani 2016) to estimate the epistemic uncertainty).",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Uncertainty-Aware Yield Prediction with Multimodal Molecular Features</span>"
    ]
  },
  {
    "objectID": "papers/chem_yield_prediction_2024.html#problem-setting",
    "href": "papers/chem_yield_prediction_2024.html#problem-setting",
    "title": "Uncertainty-Aware Yield Prediction with Multimodal Molecular Features",
    "section": "Problem setting",
    "text": "Problem setting\n\nprior works perform well on high-throughput datasets but not on real-world datasets\nthis is partially due to a lot of noise in datasets\nof course, reaction conditions are important, too\n\nAdditionally, the authors propose that the previous representations might not be “rich” enough to capture the complexity of chemical reactions.",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Uncertainty-Aware Yield Prediction with Multimodal Molecular Features</span>"
    ]
  },
  {
    "objectID": "papers/chem_yield_prediction_2024.html#approach",
    "href": "papers/chem_yield_prediction_2024.html#approach",
    "title": "Uncertainty-Aware Yield Prediction with Multimodal Molecular Features",
    "section": "Approach",
    "text": "Approach\nThe authors propose to fuse multiple features. In addition, they also use a special loss function and a mixture of experts (MoE) model used to transform human-designed features.\n\n\n\nOverview of the model architecture. Figure taken from Chem et al. (Chen et al. 2024)\n\n\n\nGraph encoder and SMILES encoder\nThe authors pretrain the graph and SMILES encoders using a contrastive loss. The graph encoder is a GNN, the SMILES encoder is a transformer.\n\nGraph convolutional neural network\nTheir graph encoder is basically a message graph convolutional neural network. The authors use the DGL library to implement this.\nThe forward pass looks like this:\nfor _ in range(self.num_step_message_passing):\n    node_feats = self.activation(self.gnn_layer(g, node_feats, edge_feats)).unsqueeze(0)\n    node_feats, hidden_feats = self.gru(node_feats, hidden_feats)\n    node_feats = node_feats.squeeze(0)\nWhere the GNN layer performs a simple operation such as\n\\[\n\\mathbf{x}_i^{\\prime}=\\boldsymbol{\\Theta}^{\\top} \\sum_{j \\in \\mathcal{N}(i) \\cup\\{i\\}} \\frac{e_{j, i}}{\\sqrt{\\hat{d}_j \\hat{d}_i}} \\mathbf{x}_j\n\\]\nwhere \\(\\hat{d}_i\\) is the degree of node \\(i\\) and \\(\\boldsymbol{\\Theta}\\) is a learnable weight matrix. \\(\\mathcal{N}(i)\\) is the set of neighbors of node \\(i\\). \\(\\mathbf{x}_i\\) is the node embedding of node \\(i\\), \\(e_{j, i}\\) is the edge feature between node \\(i\\) and \\(j\\).\nThe node embeddings are then aggregated using Set2Set pooling (Vinyals, Bengio, and Kudlur 2016).\n\n\nSMILES encoder\nFor encoding SMILES, the use a transformer model. In their code, they seem to pass through only one transformer layer.\nThe forward pass looks like this:\nx = self.token_embedding(text)\nx = x + self.positional_embedding\nx = x.permute(1, 0, 2)  # NLD -&gt; LND\nx = self.transformer(x)\nx = x.permute(1, 0, 2)  # LND -&gt; NLD\nx = self.ln_final(x)\nx = self.pooler(x[:,0,:])\nThey take the first token of the sequence and pass it through a linear layer to get the final representation.\n\n\nContrastive training\nThe authors use a contrastive loss to train the encoders.\n\\[\n\\mathcal{L}_c=-\\frac{1}{2} \\log \\frac{e^{\\left\\langle f_G^j, f_S^j\\right\\rangle / \\tau}}{\\sum_{k=1}^N e^{\\left\\langle f_G^j, f_S^k\\right\\rangle / \\tau}}-\\frac{1}{2} \\log \\frac{e^{\\left\\langle f_G^j, f_S^j\\right\\rangle / \\tau}}{\\sum_{k=1}^N e^{\\left\\langle f_G^k, f_S^j\\right\\rangle / \\tau}},\n\\]\nIn contrastive training, we try to maximize the similarity between positive pairs and minimize the similarity between negative pairs. In the equation above, \\(f_G^j\\) and \\(f_S^j\\) are the representations of the graph and SMILES of the same reaction, respectively. \\(\\tau\\) is a temperature parameter.\nSuch contrastive training allows to pretrain the encoders on a large dataset without labels.\n\n\n\n\n\n\nNote\n\n\n\nContrastive learning is one of the most popular methods in self-supervised learning. A good overview can be found in Lilian Weng’s amazing blog.\n\n\n\n\n\nHuman-features encoder\nThe authors also encode additional features with feedforward networks in a mixture of experts (MoE) model. The key idea behind MoE is that we replace “conventional layers” with “MoE layers” which are copies of the same layer. A gating network decides, based on the input, which layer to use. This is powerful if we sparsely select the experts-then only a subset of all weights are used in a given forward pass.\n\\[\n\\operatorname{MoE}\\left(x_H\\right)=\\sum_{i=1}^t \\mathcal{G}\\left(x_H\\right)_i \\cdot E_i\\left(x_H\\right)\n\\]\nThis is a mixture of experts model. The authors use a gating network \\(\\mathcal{G}\\) to decide which expert to use. The experts \\(E_i\\) are simple feedforward networks. The gating network might be a simple softmax layer:\n\\[\nG_\\sigma(x)=\\operatorname{Softmax}\\left(x \\cdot W_g\\right)\n\\]\nin practice, one can improve that by adding sparsity (e.g. selecting top-k).\n\n\n\n\n\n\nNote\n\n\n\nMoE (Shazeer et al. 2017) has become popular recently as a way to scale LLMs. You might have across model names like Mixtral-8x7B (Jiang et al. 2024), which indicates that the model is a mixture of 8 experts, each of which is a 7B parameter model. The total number of parameters is 47B parameters, but the inference cost is similar to the one of a 14B parameter model. (Note however, that memory consumption is still high as all experts need to be loaded into memory.)\nThis blog by Cameron Wolfe gives a good overview. You might also find Yannic Kilcher’s video about Mixtral of Experts useful.\n\n\n\n\nFusion\nThe fusion of the different features is done by concatenating them\nThe complete forward pass looks like this:\nr_graph_feats = torch.sum(torch.stack([self.clme.mpnn(mol) for mol in rmols]), 0)\np_graph_feats = self.clme.mpnn(pmols)\nfeats, a_loss = self.mlp(input_feats)\nseq_feats = self.clme.transformer(smiles)\nconcat_feats = torch.cat([r_graph_feats, p_graph_feats, feats, seq_feats], 1)\nout = self.predict(concat_feats)\nwhere the mpnn method is the graph encoder, the transformer method is the SMILES encoder, and the mlp method is the human-features encoder.\n\n\nUncertainty (quantification)\nThe authors define the prediction as\n\\[\n\\hat{y}=\\mu(\\boldsymbol{x})+\\epsilon * \\sigma(\\boldsymbol{x})\n\\]\nwhere \\(\\mu(\\boldsymbol{x})\\) is the prediction, \\(\\sigma(\\boldsymbol{x})\\) is the uncertainty, and \\(\\epsilon\\) is a random variable sampled from a normal distribution.\nThe model is trained with a loss function that includes the uncertainty:\n\\[\n\\mathcal{L}_u=\\frac{1}{N} \\sum_{i=1}^N\\left[\\frac{1}{\\sigma\\left(\\boldsymbol{x}_i\\right)^2}\\left\\|y_i-\\mu\\left(\\boldsymbol{x}_i\\right)\\right\\|^2+\\log \\sigma\\left(\\boldsymbol{x}_i\\right)^2\\right]\n\\]\nThe \\(\\sigma\\) term is capturing observation noise (aleatoric uncertainty).\n\n\n\n\n\n\nNote\n\n\n\nThis loss comes from the idea of variational inference.\n\\[\n\\mathcal{L}(\\boldsymbol{\\lambda})=-\\mathbb{E}_{q(\\boldsymbol{\\theta} ; \\boldsymbol{\\lambda})}[\\log p(\\mathbf{y} \\mid \\mathbf{x}, \\boldsymbol{\\theta})]+\\mathrm{KL}(q(\\boldsymbol{\\theta} ; \\boldsymbol{\\lambda}) \\| p(\\boldsymbol{\\theta}))\n\\]\nIn this equation, the first term is the negative log-likelihood, and the second term is the KL divergence between the approximate posterior \\(q(\\boldsymbol{\\theta} ; \\boldsymbol{\\lambda})\\) and the prior \\(p(\\boldsymbol{\\theta})\\). The KL divergence is a measure of how much the approximate posterior diverges from the prior. The idea is to minimize the negative log-likelihood while keeping the approximate posterior close to the prior. This is a way to quantify the uncertainty in the model.\nThe idea comes from Bayesian inference, where we want to estimate the posterior distribution over the parameters of the model. In practice, this is intractable, so we use variational inference to approximate the posterior with a simpler distribution. The posterior (which quantifies uncertainty) is typically computationally expensive to compute, so we use variational inference to approximate it with a simpler distribution, this is called variational inference. Since during training, we do some sampling, we need to perform a reparametrization trick (Kingma, Salimans, and Welling 2015) to make the gradients flow through the sampling operation.",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Uncertainty-Aware Yield Prediction with Multimodal Molecular Features</span>"
    ]
  },
  {
    "objectID": "papers/chem_yield_prediction_2024.html#results",
    "href": "papers/chem_yield_prediction_2024.html#results",
    "title": "Uncertainty-Aware Yield Prediction with Multimodal Molecular Features",
    "section": "Results",
    "text": "Results\nAs in most ML papers, we have tables with bold numbers, e.g. for a dataset with amide coupling reactions:\n\n\n\n\n\n\n\n\n\nModel\nMAE \\(\\downarrow\\)\nRMSE \\(\\downarrow\\)\n\\(R^2 \\uparrow\\)\n\n\n\n\nMordred\n\\(15.99 \\pm 0.14\\)\n\\(21.08 \\pm 0.16\\)\n\\(0.168 \\pm 0.010\\)\n\n\nYieldBert\n\\(16.52 \\pm 0.20\\)\n\\(21.12 \\pm 0.13\\)\n\\(0.172 \\pm 0.016\\)\n\n\nYieldGNN\n\\(\\underline{15.27 \\pm 0.18}\\)\n\\(\\underline{19.82} \\pm 0.08\\)\n\\(\\underline{0.216} \\pm 0.013\\)\n\n\nMPNN\n\\(16.31 \\pm 0.22\\)\n\\(20.86 \\pm 0.27\\)\n\\(0.188 \\pm 0.021\\)\n\n\nOurs\n\\(\\mathbf{1 4 . 7 6} \\pm \\mathbf{0 . 1 5}\\)\n\\(\\mathbf{1 9 . 3 3} \\pm \\mathbf{0 . 1 0}\\)\n\\(\\mathbf{0 . 2 6 2} \\pm \\mathbf{0 . 0 0 9}\\)\n\n\n\nHere, their model outperforms the baselines. But it is also interesting to see how well the Mordred baseline performs compared to much more complex models.\nThe pattern of their model being bold in tables is persistent across datasets.\n\nAblations\nThe authors perform ablations to understand the importance of the different components of their model. While there are some differences, the differences are not drastic (partially overlapping errorbars).\n\n\n\n\n\n\n\n\n\nModel\nMAE \\(\\downarrow\\)\nRMSE \\(\\downarrow\\)\n\\(R^2 \\uparrow\\)\n\n\n\n\nOurs\n\\(14.76 \\pm 0.15\\)\n\\(19.33 \\pm 0.10\\)\n\\(0.262 \\pm 0.009\\)\n\n\nw/o UQ\n\\(15.08 \\pm 0.13\\)\n\\(19.63 \\pm 0.09\\)\n\\(0.249 \\pm 0.009\\)\n\n\nw/o \\(\\mathcal{L}_r\\)\n\\(14.80 \\pm 0.16\\)\n\\(19.51 \\pm 0.10\\)\n\\(0.261 \\pm 0.010\\)\n\n\nw/o MoE\n\\(15.12 \\pm 0.18\\)\n\\(20.03 \\pm 0.13\\)\n\\(0.230 \\pm 0.012\\)\n\n\nw/o Seq.\n\\(14.97 \\pm 0.16\\)\n\\(19.55 \\pm 0.11\\)\n\\(0.261 \\pm 0.010\\)\n\n\nw/o Graph\n\\(15.06 \\pm 0.15\\)\n\\(19.59 \\pm 0.10\\)\n\\(0.260 \\pm 0.009\\)\n\n\nw/o H.\n\\(15.83 \\pm 0.20\\)\n\\(20.46 \\pm 0.18\\)\n\\(0.212 \\pm 0.016\\)",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Uncertainty-Aware Yield Prediction with Multimodal Molecular Features</span>"
    ]
  },
  {
    "objectID": "papers/chem_yield_prediction_2024.html#take-aways",
    "href": "papers/chem_yield_prediction_2024.html#take-aways",
    "title": "Uncertainty-Aware Yield Prediction with Multimodal Molecular Features",
    "section": "Take aways",
    "text": "Take aways\n\nA lot of machinery, but not a drastic improvement\nIt is the data, stupid! 😉 (It is not really clear how this is even supposed to work with information about the conditions)\nInterestingly, they didn’t test USPTO or other datasets\nTheir approach with frozen encoders is interesting, it would have been interesting to see learning curves to better understand the data efficiency of the approach",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Uncertainty-Aware Yield Prediction with Multimodal Molecular Features</span>"
    ]
  },
  {
    "objectID": "papers/chem_yield_prediction_2024.html#references",
    "href": "papers/chem_yield_prediction_2024.html#references",
    "title": "Uncertainty-Aware Yield Prediction with Multimodal Molecular Features",
    "section": "References",
    "text": "References\n\n\n\n\nAhneman, Derek T., Jesús G. Estrada, Shishi Lin, Spencer D. Dreher, and Abigail G. Doyle. 2018. “Predicting Reaction Performance in c–n Cross-Coupling Using Machine Learning.” Science 360 (6385): 186–90. https://doi.org/10.1126/science.aar5169.\n\n\nChen, Jiayuan, Kehan Guo, Zhen Liu, Olexandr Isayev, and Xiangliang Zhang. 2024. “Uncertainty-Aware Yield Prediction with Multimodal Molecular Features.” Proceedings of the AAAI Conference on Artificial Intelligence 38 (8): 8274–82. https://doi.org/10.1609/aaai.v38i8.28668.\n\n\nChuang, Kangway V., and Michael J. Keiser. 2018. “Comment on ‘Predicting Reaction Performance in c–n Cross-Coupling Using Machine Learning’.” Science 362 (6416). https://doi.org/10.1126/science.aat8603.\n\n\nGal, Yarin, and Zoubin Ghahramani. 2016. “Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning.” In International Conference on Machine Learning, 1050–59. PMLR.\n\n\nGuo, Jeff, and Philippe Schwaller. 2024. “It Takes Two to Tango: Directly Optimizing for Constrained Synthesizability in Generative Molecular Design.” arXiv. https://doi.org/10.48550/ARXIV.2410.11527.\n\n\nJiang, Albert Q., Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, et al. 2024. “Mixtral of Experts.” https://arxiv.org/abs/2401.04088.\n\n\nKingma, Diederik P., Tim Salimans, and Max Welling. 2015. “Variational Dropout and the Local Reparameterization Trick.” https://arxiv.org/abs/1506.02557.\n\n\nKwon, Youngchun, Dongseon Lee, Youn-Suk Choi, and Seokho Kang. 2022. “Uncertainty-Aware Prediction of Chemical Reaction Yields with Graph Neural Networks.” Journal of Cheminformatics 14 (1). https://doi.org/10.1186/s13321-021-00579-z.\n\n\nLandrum, Gregory A., Jessica Braun, Paul Katzberger, Marc T. Lehner, and Sereina Riniker. 2024. “Lwreg: A Lightweight System for Chemical Registration and Data Storage.” Journal of Chemical Information and Modeling 64 (16): 6247–52. https://doi.org/10.1021/acs.jcim.4c01133.\n\n\nOrsi, Markus, and Jean-Louis Reymond. 2024. “One Chiral Fingerprint to Find Them All.” Journal of Cheminformatics 16 (1): 53.\n\n\nSchwaller, Philippe, Alain C Vaucher, Teodoro Laino, and Jean-Louis Reymond. 2020. “Data Augmentation Strategies to Improve Reaction Yield Predictions and Estimate Uncertainty.” Chemrxiv Preprint.\n\n\n———. 2021. “Prediction of Chemical Reaction Yields Using Deep Learning.” Machine Learning: Science and Technology 2 (1): 015016.\n\n\nShazeer, Noam, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. 2017. “Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer.” https://arxiv.org/abs/1701.06538.\n\n\nVinyals, Oriol, Samy Bengio, and Manjunath Kudlur. 2016. “Order Matters: Sequence to Sequence for Sets.” https://arxiv.org/abs/1511.06391.",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Uncertainty-Aware Yield Prediction with Multimodal Molecular Features</span>"
    ]
  },
  {
    "objectID": "papers/dagdelen_data_extraction.html",
    "href": "papers/dagdelen_data_extraction.html",
    "title": "Structured information extraction from scientific text with large language models",
    "section": "",
    "text": "Why discussing this paper?\nI chose Dagdelen et al.’s paper (Dagdelen et al. 2024) for our journal club because:",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Structured information extraction from scientific text with large language models</span>"
    ]
  },
  {
    "objectID": "papers/dagdelen_data_extraction.html#why-discussing-this-paper",
    "href": "papers/dagdelen_data_extraction.html#why-discussing-this-paper",
    "title": "Structured information extraction from scientific text with large language models",
    "section": "",
    "text": "It is one of the last published papers to fine-tune a model for the data extraction task for materials science.\nIt presents a very robust fine-tuning and evaluation process.\nFurthermore, they show how the current models can help with a tedious task such as it is annotating data.",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Structured information extraction from scientific text with large language models</span>"
    ]
  },
  {
    "objectID": "papers/dagdelen_data_extraction.html#context",
    "href": "papers/dagdelen_data_extraction.html#context",
    "title": "Structured information extraction from scientific text with large language models",
    "section": "Context",
    "text": "Context\nExtracting the unstructured scientific information from the articles that contain it can be a really arduos and time-consuming task. In the recent years, several works have shown the great potential that LLMs have to greatly accelerate this task. However, for some research fields or harder extraction schemas, the general pre-training of these models might not be enough to archieve the desired results. For such cases, fine-tuning have shown to be the adequate technique.",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Structured information extraction from scientific text with large language models</span>"
    ]
  },
  {
    "objectID": "papers/dagdelen_data_extraction.html#prior-work",
    "href": "papers/dagdelen_data_extraction.html#prior-work",
    "title": "Structured information extraction from scientific text with large language models",
    "section": "Prior work",
    "text": "Prior work\n\nOld ages\nSeveral works from the Ceder group showed the complete tedious process. First, Huo et al. (Huo et al. 2019) use LDA + RF to classify text. To train the RF model they had to manually label 6000 materials paragraphs as they contain synthesis information or not.\nIn a following work, Kononova et al. (Kononova et al. 2019) trained a Word2Vec model, to then feed the embeddings to a BiLSTM-CRF. To train this NN they manually annotated more than 800 paragraphs word-by-word with tags about solid-state synthesis role (material, target, precursor or other). Furthermore, to classify the synthesis operations (NOT OPERATION, MIXING, HEATING, etc) they trained another NN with more annotated data. To this step they also had to LEMMATIZED the sentences and obtain each token’s POS. Amazing hard work!\nSimilar works by Kim et al. (Kim et al. 2017, 2020; Mysore et al. 2019) in which they applied similar techniques such as word embeddings from language models, then fed to a named entity recognition model.\n\n\n\nFigure taken from Mysore et al. paper (Mysore et al. 2019) illustrating how they labeled the data for the NER task.\n\n\n\n\nChemDataExtractor 1.0 and 2.0\nCole et al. (Swain and Cole 2016; Mavračić et al. 2021) developed ChemDataExtractor which is build from the combination of traditional ML techniques for each NLP task such as lemmatazion, tokenization, POS tagging, it even include Table Parsing. All of these models trained in chemical text, which made this tool a really good option for extracting chemical data from text.\n\n\nTrewartha et al. (2022)\nTrewartha el al. (Trewartha et al. 2022) compared the performance of a simpler model such as a BiLSTM RNN with three more complex transformer models, BERT, SciBERT and MatBERT for the NER task. For that, they used data from three different NER datasets, each one related with different materials synthesis.\nThe results, showed that the more specialized BERT models were able to better recognize the different entities. However, it is important to remark that the BERT models were fine-tuned for the task.\n\n\n\nFigure taken from Trewartha et al. (Trewartha et al. 2022) summarizing the results that they obtained with each model.",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Structured information extraction from scientific text with large language models</span>"
    ]
  },
  {
    "objectID": "papers/dagdelen_data_extraction.html#problem-setting",
    "href": "papers/dagdelen_data_extraction.html#problem-setting",
    "title": "Structured information extraction from scientific text with large language models",
    "section": "Problem setting",
    "text": "Problem setting\n\nAlmost all the scientific knowledge is contained in scientific texts in an unstructured way.\nThe classical approaches include a lot of different techniques, each of them has to be trained independently.\nFor those classical techniques, a lot manually labeled data is needed for each task and technique.\nLLMs appear to simplify a lot all the previous options by allowing to perform all the different NLP tasks with one unique model.",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Structured information extraction from scientific text with large language models</span>"
    ]
  },
  {
    "objectID": "papers/dagdelen_data_extraction.html#approach",
    "href": "papers/dagdelen_data_extraction.html#approach",
    "title": "Structured information extraction from scientific text with large language models",
    "section": "Approach",
    "text": "Approach\nThey proposed to fine-tune two models, one open-source Llama-2 70B model and a close-source one such as GPT-3, for the NER and RE tasks applied to solid-state materials. As output, they compared two different options: JSON and plain text. They proposed this for three different specificities of data: Doping, MOF and general materials data.\n\n\n\nTask\nTraining samples\nCompletion format\n\n\n\n\nDoping\n413 sentences\nJSON\n\n\nDoping\n413 sentences\nEnglish sentences\n\n\nMOFs\n507 abstracts\nJSON\n\n\nGeneral materials\n634 abstracts\nJSON",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Structured information extraction from scientific text with large language models</span>"
    ]
  },
  {
    "objectID": "papers/dagdelen_data_extraction.html#results",
    "href": "papers/dagdelen_data_extraction.html#results",
    "title": "Structured information extraction from scientific text with large language models",
    "section": "Results",
    "text": "Results\nThe results showed first of all that both models performed similar for the tasks. For the exact match, GPT-3 performed slightly better than the Llama-2 model, with overall results for both models around 50% considering all the tasks.\n\n\n\nTask\nRelation\nE.M. F1 GPT-3\nE.M. Llama-3\n\n\n\n\nDoping\nhost-dopant\n0.726\n0.821\n\n\nGeneral\nformula-name\n0.456\n0.367\n\n\nGeneral\nformula-acronym\n0.333\n0.286\n\n\nGeneral\nformula-structure/phase\n0.482\n0.470\n\n\nGeneral\nformula-application\n0.537\n0.516\n\n\nGeneral\nformula-description\n0.354\n0.340\n\n\nMOFs\nname-formula\n0.483\n0.276\n\n\nMOFs\nname-guest specie\n0.616\n0.408\n\n\nMOFs\nname-application\n0.573\n0.531\n\n\nMOFs\nname-description\n0.404\n0.389\n\n\n\n\n\n\nE.M. stands for exact match\n\n\n\nThe results presented in this table include for both NER and RE NLP tasks.\n\n\nIt is important to comment that the exact match is an approximate lower bound on information extraction performance, since it not consider some cases such as “Lithium ion” named as “Li-ion”, or MOF names such as “ZIF-8” that are described as “mesostructured MOFs formed by Cu2+ and 5hydroxy-1,3-benzenedicarboxylic acid”.\nFor correctly measure those ambiguities, they did a manual evaluation on a randomly sampled 10% of the test set. These results showed that the score for the extraction was much better than the showed by the exact match. This also showed that some kind of normallization proccess is needed to correctly evaluate this type of extraction tasks.\nFor the Doping task, three different output schema were consider, DopingEnglish, DopingJSON and DopingExtra-English. They compared the results for the three schema GPT-3 and Llama-2 fine-tuned models with other older models such as MatBERT and Seq2rel.\n\n\nThe difference between DopingEnglish and DopingExtra-English is that the last one include some additional information and not only the host-entity relation.\nThe results showed that the Llama-2 model return the best results for this task, which are slightly better than the GPT-3 ones. Both LLMs improved by far the other two models.\n\n\n\n\n\n\n\n\n\n\nModel\nSchema\nE.M. Precision\nE.M. Recall\nE.M. F1\n\n\n\n\nMatBERT\nn/a\n0.377\n0.403\n0.390\n\n\nSeq2rel\nn/a\n0.420\n0.605\n0.496\n\n\nGPT-3\nDoping-JSON\n0.772\n0.684\n0.725\n\n\nGPT-3\nDoping-English\n0.803\n0.754\n0.778\n\n\nGPT-3\nDopingExtra-English\n0.820\n0.798\n0.809\n\n\nLlama-2\nDoping-JSON\n0.836\n0.807\n0.821\n\n\nLlama-2\nDoping-English\n0.787\n0.842\n0.814\n\n\nLlama-2\nDopingExtra-English\n0.694\n0.815\n0.750\n\n\n\nA limitation of the method could be that for each of the three extractions tasks, they have to annotate between 100 and 500 text passages. This can be a tedious work. However, to overcome this limitation, they proposed to include human-in-the-loop annotation.\n\nHuman-in-the-loop\nTo overcome the limitation of having to manually annotate all the data needed for the fine-tuning process, they sucesfully implemented human-in-the-loop annotation. For that, they fine-tune the model with a small amount of manually labelled data. Then the model is asked to extract data from the other text passages. The returned data by the model is corrected by an human annotator and is feed into the model to further fine-tune it.\n\n\n\nFigure showing the process used to implement human-in-the-loop annotation.\n\n\nBy using this technique, they greatly reduce the amount of time needed to annotate the last pieces of text compared with the first ones.\n\n\n\nFigure showing the time reduction across the process of annotation using the human-in-the-loop technique.\n\n\nBy using this annotation method they greatly improve the annotation time solving one of the main drawbacks of fine-tuning an LLM. This great limitation can be seen in another works such as the one by Guo el al. (Jiang Guo et al. 2021) in which they employed 13 graduate and postdoc students to annotate about chemical reactions. After that, they have to even check all the annotation. They estimate that this process took them almost 300 hours.",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Structured information extraction from scientific text with large language models</span>"
    ]
  },
  {
    "objectID": "papers/dagdelen_data_extraction.html#take-aways",
    "href": "papers/dagdelen_data_extraction.html#take-aways",
    "title": "Structured information extraction from scientific text with large language models",
    "section": "Take aways",
    "text": "Take aways\n\nOpen source models with proper tuning can yield high-quality results similar to those of closed source models.\nDespite some labeled data is needed, the process is simplified a lot with the use of LLMs.\nWith the fasst and continuous development of the current models, maybe fine-tuning for a simpler task such as data extraction is no furhter needed.",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Structured information extraction from scientific text with large language models</span>"
    ]
  },
  {
    "objectID": "papers/dagdelen_data_extraction.html#references",
    "href": "papers/dagdelen_data_extraction.html#references",
    "title": "Structured information extraction from scientific text with large language models",
    "section": "References",
    "text": "References\n\n\n\n\nDagdelen, John, Alexander Dunn, Sanghoon Lee, Nicholas Walker, Andrew S. Rosen, Gerbrand Ceder, Kristin A. Persson, and Anubhav Jain. 2024. “Structured Information Extraction from Scientific Text with Large Language Models.” Nature Communications 15 (1): 1418. https://doi.org/10.1038/s41467-024-45563-x.\n\n\nGuo, Jeff, and Philippe Schwaller. 2024. “It Takes Two to Tango: Directly Optimizing for Constrained Synthesizability in Generative Molecular Design.” arXiv. https://doi.org/10.48550/ARXIV.2410.11527.\n\n\nGuo, Jiang, A. Santiago Ibanez-Lopez, Hanyu Gao, Victor Quach, Connor W. Coley, Klavs F. Jensen, and Regina Barzilay. 2021. “Automated Chemical Reaction Extraction from Scientific Literature.” Journal of Chemical Information and Modeling 62 (9): 2035–45. https://doi.org/10.1021/acs.jcim.1c00284.\n\n\nHuo, Haoyan, Ziqin Rong, Olga Kononova, Wenhao Sun, Tiago Botari, Tanjin He, Vahe Tshitoyan, and Gerbrand Ceder. 2019. “Semi-Supervised Machine-Learning Classification of Materials Synthesis Procedures.” Npj Computational Materials 5 (1). https://doi.org/10.1038/s41524-019-0204-1.\n\n\nKim, Edward, Kevin Huang, Adam Saunders, Andrew McCallum, Gerbrand Ceder, and Elsa Olivetti. 2017. “Materials Synthesis Insights from Scientific Literature via Text Extraction and Machine Learning.” Chemistry of Materials 29 (21): 9436–44. https://doi.org/10.1021/acs.chemmater.7b03500.\n\n\nKim, Edward, Zach Jensen, Alexander van Grootel, Kevin Huang, Matthew Staib, Sheshera Mysore, Haw-Shiuan Chang, et al. 2020. “Inorganic Materials Synthesis Planning with Literature-Trained Neural Networks.” Journal of Chemical Information and Modeling 60 (3): 1194–1201. https://doi.org/10.1021/acs.jcim.9b00995.\n\n\nKononova, Olga, Haoyan Huo, Tanjin He, Ziqin Rong, Tiago Botari, Wenhao Sun, Vahe Tshitoyan, and Gerbrand Ceder. 2019. “Text-Mined Dataset of Inorganic Materials Synthesis Recipes.” Scientific Data 6 (1). https://doi.org/10.1038/s41597-019-0224-1.\n\n\nLandrum, Gregory A., Jessica Braun, Paul Katzberger, Marc T. Lehner, and Sereina Riniker. 2024. “Lwreg: A Lightweight System for Chemical Registration and Data Storage.” Journal of Chemical Information and Modeling 64 (16): 6247–52. https://doi.org/10.1021/acs.jcim.4c01133.\n\n\nMavračić, Juraj, Callum J. Court, Taketomo Isazawa, Stephen R. Elliott, and Jacqueline M. Cole. 2021. “ChemDataExtractor 2.0: Autopopulated Ontologies for Materials Science.” Journal of Chemical Information and Modeling 61 (9): 4280–89. https://doi.org/10.1021/acs.jcim.1c00446.\n\n\nMysore, Sheshera, Zach Jensen, Edward Kim, Kevin Huang, Haw-Shiuan Chang, Emma Strubell, Jeffrey Flanigan, Andrew McCallum, and Elsa Olivetti. 2019. “The Materials Science Procedural Text Corpus: Annotating Materials Synthesis Procedures with Shallow Semantic Structures.” https://arxiv.org/abs/1905.06939.\n\n\nOrsi, Markus, and Jean-Louis Reymond. 2024. “One Chiral Fingerprint to Find Them All.” Journal of Cheminformatics 16 (1): 53.\n\n\nSwain, Matthew C., and Jacqueline M. Cole. 2016. “ChemDataExtractor: A Toolkit for Automated Extraction of Chemical Information from the Scientific Literature.” Journal of Chemical Information and Modeling 56 (10): 1894–904. https://doi.org/10.1021/acs.jcim.6b00207.\n\n\nTrewartha, Amalie, Nicholas Walker, Haoyan Huo, Sanghoon Lee, Kevin Cruse, John Dagdelen, Alexander Dunn, Kristin A. Persson, Gerbrand Ceder, and Anubhav Jain. 2022. “Quantifying the Advantage of Domain-Specific Pre-Training on Named Entity Recognition Tasks in Materials Science.” Patterns 3 (4): 100488. https://doi.org/10.1016/j.patter.2022.100488.",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Structured information extraction from scientific text with large language models</span>"
    ]
  },
  {
    "objectID": "papers/MolCLR_2024.html",
    "href": "papers/MolCLR_2024.html",
    "title": "Molecular contrastive learning of representations (MolCLR) via graph neural networks",
    "section": "",
    "text": "Why discussing this paper?\nI chose Wang’s et al.’s paper (Wang et al. 2022) for our journal club because",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Molecular contrastive learning of representations (MolCLR) via graph neural networks</span>"
    ]
  },
  {
    "objectID": "papers/MolCLR_2024.html#why-discussing-this-paper",
    "href": "papers/MolCLR_2024.html#why-discussing-this-paper",
    "title": "Molecular contrastive learning of representations (MolCLR) via graph neural networks",
    "section": "",
    "text": "Supervised learning relies heavily on labeled training data, which can be expensive and time-consuming to obtain.\nLabeled datasets are limited; training models on such datasets might lead to overfitting and poor generalization.\nSelf-Supervised Learning (SSL) provides a promising alternative. It enables learning from unlabeled data, which is much easier to acquire in real-world applications and is part of a large research effort.\nSSL learns the inherent structure and patterns in unlabeled data. By doing so, self-supervised models can acquire rich representations and knowledge that can be transferred to downstream tasks, even with limited labeled data.\nContrastive self-supervised learning, as the name implies, is a self-supervised method that learns representations by contrasting positive and negative pairs.",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Molecular contrastive learning of representations (MolCLR) via graph neural networks</span>"
    ]
  },
  {
    "objectID": "papers/MolCLR_2024.html#context",
    "href": "papers/MolCLR_2024.html#context",
    "title": "Molecular contrastive learning of representations (MolCLR) via graph neural networks",
    "section": "Context",
    "text": "Context\nThe paper introduces MolCLR, a self-supervised learning framework that uses graph encoders to learn differentiable molecular representations.\nThe authors used a large unlabeled dataset with 10 million unique molecule SMILES from ChemBERTa and PubChem. This framework involves two important steps. First, in the pre-training phase, they build molecule graphs and develop graph neural network (GNN) encoders to learn differentiable representations. Second, the pretrained GNN backbone is used for supervised learning tasks.",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Molecular contrastive learning of representations (MolCLR) via graph neural networks</span>"
    ]
  },
  {
    "objectID": "papers/MolCLR_2024.html#main-idea-behind-contrastive-learning",
    "href": "papers/MolCLR_2024.html#main-idea-behind-contrastive-learning",
    "title": "Molecular contrastive learning of representations (MolCLR) via graph neural networks",
    "section": "Main idea behind contrastive learning",
    "text": "Main idea behind contrastive learning\nThe aim of contrastive representation learning is to develop an embedding space where similar samples are positioned closely together, whereas dissimilar samples are kept distant from each other.\n\n\n\nFigure taken from the blog post by Ekin Tiu illustrating the idea behind contrastive learning.",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Molecular contrastive learning of representations (MolCLR) via graph neural networks</span>"
    ]
  },
  {
    "objectID": "papers/MolCLR_2024.html#nt-xent-loss",
    "href": "papers/MolCLR_2024.html#nt-xent-loss",
    "title": "Molecular contrastive learning of representations (MolCLR) via graph neural networks",
    "section": "NT-Xent loss",
    "text": "NT-Xent loss\nThe authors use a NT-Xent loss for the contrastive learning.\n\\[\nL_{ij} = -\\log \\left( \\frac{\\exp(\\text{sim}(z_i, z_j)/\\tau)}{\\sum_{k=1}^{2N} \\mathbf{1}_{\\{k \\neq i\\}} \\exp(\\text{sim}(z_i, z_k)/\\tau)} \\right)\n\\]\n\nz_i and z_j: Latent vectors extracted from a positive data pair.\nN: Batch size, indicating the number of data pairs used.\nsim(⋅): Function measuring the similarity between two vectors.\nτ: The temperature parameter, used to scale the similarity measures in the function.\n\nTo get more understanding, (Le-Khac, Healy, and Smeaton 2020) is a review paper on contrastive learning.",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Molecular contrastive learning of representations (MolCLR) via graph neural networks</span>"
    ]
  },
  {
    "objectID": "papers/MolCLR_2024.html#overview-of-molclr-framework",
    "href": "papers/MolCLR_2024.html#overview-of-molclr-framework",
    "title": "Molecular contrastive learning of representations (MolCLR) via graph neural networks",
    "section": "Overview of MolCLR framework",
    "text": "Overview of MolCLR framework\n\nThe SMILES are converted into graphs in a batch. The graphs consist of nodes and edges. Nodes represent atoms, and edges represent the chemical bonds of a molecule.\nThe authors studied three types of augmentations: atom masking, bond deletion, and sub-graph removal. These augmentations introduce variance to the model, allowing it to learn different substructures/topologies of the same molecule and generalize well to unseen molecules.\nThe GNNs operate based on a message-passing framework. At each node, the local neighborhood information is aggregated and updated iteratively, resulting in node embeddings of the molecule.\nThese embeddings are fed into a Multi-Layer Perceptron (MLP) with hidden units to obtain a latent representation of the molecule. The MLP serves as a projection head, mapping the high-dimensional representations from the encoder (GNN) to a lower-dimensional latent space. This projection helps in learning more compact and discriminative representations for the contrastive loss.\nThe key difference between supervised GNNs and self-supervised contrastive GNNs is the training objective. In a supervised setup, where we have access to ground truth labels, we can train the network to optimize standard loss functions such as Binary Cross Entropy (BCE) for classification or Mean Absolute Error/Mean Squared Error (MAE/MSE) for regression. In self-supervised contrastive learning, we aim to learn a latent space where positive pairs are closer and negative pairs are further apart.\nThe goal is to score the agreement between positive pairs higher than that of negative pairs. For a given graph, its positive pair is constructed using data augmentations, while all other graphs in the batch constitute negative pairs.\n\n\n\n\nOverview figure for the MolCLR framework\n\n\nFor more information about GNNs and their applications, check out this review paper [zhou2020graph].",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Molecular contrastive learning of representations (MolCLR) via graph neural networks</span>"
    ]
  },
  {
    "objectID": "papers/MolCLR_2024.html#results",
    "href": "papers/MolCLR_2024.html#results",
    "title": "Molecular contrastive learning of representations (MolCLR) via graph neural networks",
    "section": "Results",
    "text": "Results\nThe authors tested the performance of MolCLR framework on seven benchmarks for classification tasks and six benchmarks for regression tasks.\nThe authors claim that on classification tasks with other self-supervised learning or pre-training strategies, their MolCLR framework achieves the best performance on five out of seven benchmarks, with an average improvement of 4.0 percent in (ROC-AUC (%)).\n\n\n\nTable 1 shows the test performance on seven benchmarks on classification task\n\n\nThe authors also claim that on regression tasks, MolCLR surpasses other pre-training baselines in five out of six benchmarks and achieves almost the same performance on the remaining ESOL benchmark. For the QM9 dataset, MolCLR does not have comparable performancee with SchNet and MGCN supervised models.\n\n\n\nTable 2 shows the test performance on seven benchmarks on classification task",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Molecular contrastive learning of representations (MolCLR) via graph neural networks</span>"
    ]
  },
  {
    "objectID": "papers/MolCLR_2024.html#analysis-of-molecule-graph-augmentations",
    "href": "papers/MolCLR_2024.html#analysis-of-molecule-graph-augmentations",
    "title": "Molecular contrastive learning of representations (MolCLR) via graph neural networks",
    "section": "Analysis of molecule graph augmentations",
    "text": "Analysis of molecule graph augmentations\nThe authors employed four augmentation strategies on the classification benchmarks. Out of four augmentations, sub-graph removal with probability 0.25 achieved good (ROC-AUC %) on all of the classification benchmarks except in the BBBP benchmark as shown in the figure 2. The reason might be that model structures are sensitive in BBBP benchmark.\nThey also tested GIN supervised models with and without molecular graph augmentations. With augmentations (sub-graph removal with a probability of 0.25), the model achieved superior performance compared to without augmentations.\n\n\n\nFigure 2 shows the investigation of molecule graph augmentations on classification benchmarks",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Molecular contrastive learning of representations (MolCLR) via graph neural networks</span>"
    ]
  },
  {
    "objectID": "papers/MolCLR_2024.html#investigation-of-molclr-representation",
    "href": "papers/MolCLR_2024.html#investigation-of-molclr-representation",
    "title": "Molecular contrastive learning of representations (MolCLR) via graph neural networks",
    "section": "Investigation of MolCLR representation",
    "text": "Investigation of MolCLR representation\nThe authors examined the representations learned by pretrained MOlCLR using t-SNE embedding. This method groups similar chemicals together in two-dimensional space. In Figure 3, they show a picture of 100,000 molecules from the validation set of PubChem database showing similar/dissimilar molecules learned by MolCLR pretraining.\n\n\n\nFigure 3 Visualization of molecular representations learned by MolCLR via t-sne\n\n\nFor instance, the three molecules on the top possess carbonyl groups connected with aryls. The two molecules on the bottom left have similar structures, where a halogen atom (fluorine or chlorine) is connected to benzene.\nTherefore, learned representations are not random but they are meaningful.\nIn addition to this, the authors also query molecule from the PubcChem with ID 42953211 and the closest nine similar molecules are retrevied along with RDKFP and ECFP similarities labelled. It is observed that these selected molecules share the same functional groups, including alkyl halides (chlorine), tertiary amines, ketones and aromatics. A thiophene structure can also be found in all the molecules.\nMore examples of query molecules can be found in supplementary information of this paper.\n\n\n\nFigure shows Comparison of MolCLR-learned representations and conventional FPs using the query molecule (PubChem ID 42953211)",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Molecular contrastive learning of representations (MolCLR) via graph neural networks</span>"
    ]
  },
  {
    "objectID": "papers/MolCLR_2024.html#take-aways",
    "href": "papers/MolCLR_2024.html#take-aways",
    "title": "Molecular contrastive learning of representations (MolCLR) via graph neural networks",
    "section": "Take aways",
    "text": "Take aways\n\nGood molecular representations are important for better predictions\nSelf-supervised contrastive learning would be an advantage for generalizable machine learning over supervised learning",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Molecular contrastive learning of representations (MolCLR) via graph neural networks</span>"
    ]
  },
  {
    "objectID": "papers/MolCLR_2024.html#references",
    "href": "papers/MolCLR_2024.html#references",
    "title": "Molecular contrastive learning of representations (MolCLR) via graph neural networks",
    "section": "References",
    "text": "References\n\n\n\n\nGuo, Jeff, and Philippe Schwaller. 2024. “It Takes Two to Tango: Directly Optimizing for Constrained Synthesizability in Generative Molecular Design.” arXiv. https://doi.org/10.48550/ARXIV.2410.11527.\n\n\nLandrum, Gregory A., Jessica Braun, Paul Katzberger, Marc T. Lehner, and Sereina Riniker. 2024. “Lwreg: A Lightweight System for Chemical Registration and Data Storage.” Journal of Chemical Information and Modeling 64 (16): 6247–52. https://doi.org/10.1021/acs.jcim.4c01133.\n\n\nLe-Khac, Phuc H, Graham Healy, and Alan F Smeaton. 2020. “Contrastive Representation Learning: A Framework and Review.” Ieee Access 8: 193907–34.\n\n\nOrsi, Markus, and Jean-Louis Reymond. 2024. “One Chiral Fingerprint to Find Them All.” Journal of Cheminformatics 16 (1): 53.\n\n\nWang, Yuyang, Jianren Wang, Zhonglin Cao, and Amir Barati Farimani. 2022. “Molecular Contrastive Learning of Representations via Graph Neural Networks.” Nature Machine Intelligence 4 (3): 279–87.",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Molecular contrastive learning of representations (MolCLR) via graph neural networks</span>"
    ]
  },
  {
    "objectID": "papers/PaCh.html",
    "href": "papers/PaCh.html",
    "title": "PaCh (Packed Chemicals): Computationally Effective Binary Format for Chemical Structure Encoding",
    "section": "",
    "text": "Why discuss this paper?\nI chose the PaCh (Packed Chemicals) paper (Nugmanov 2024) current topics in the cheminformatics seminar because",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>PaCh (Packed Chemicals): Computationally Effective Binary Format for Chemical Structure Encoding</span>"
    ]
  },
  {
    "objectID": "papers/PaCh.html#why-discuss-this-paper",
    "href": "papers/PaCh.html#why-discuss-this-paper",
    "title": "PaCh (Packed Chemicals): Computationally Effective Binary Format for Chemical Structure Encoding",
    "section": "",
    "text": "It aims to combine compact local stereochemistry encoding (in SMILES) with explicit bond connectivity and 2D layout (like in MDL MOL files).\nThis is an interesting approach to package these two pieces of information together, which makes our group think about possible real-world applications.\nBenchmarking shows PaCh outperforms SMILES, MOL, and CML in encoding/decoding speed while maintaining compact size.\nIntroduces a computationally effective format tailored for cheminformatics applications, deep learning tasks, and efficient data storage/processing.",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>PaCh (Packed Chemicals): Computationally Effective Binary Format for Chemical Structure Encoding</span>"
    ]
  },
  {
    "objectID": "papers/PaCh.html#context",
    "href": "papers/PaCh.html#context",
    "title": "PaCh (Packed Chemicals): Computationally Effective Binary Format for Chemical Structure Encoding",
    "section": "Context",
    "text": "Context\nThis paper proposes a new binary format called PaCh (Packed Chemicals) that aims to combine the advantages of compact string-based SMILES and explicit bond connectivity of MDL MOL files while addressing their shortcomings. PaCh introduces a condensed representation of packing atom properties, 2D coordinates, bidirectional connectivity, and relative stereochemistry coding in an efficient manner to enable fast processing for modern data analytics workflows.",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>PaCh (Packed Chemicals): Computationally Effective Binary Format for Chemical Structure Encoding</span>"
    ]
  },
  {
    "objectID": "papers/PaCh.html#key-features-of-pach",
    "href": "papers/PaCh.html#key-features-of-pach",
    "title": "PaCh (Packed Chemicals): Computationally Effective Binary Format for Chemical Structure Encoding",
    "section": "Key Features of PaCh",
    "text": "Key Features of PaCh\n\nBinary Format: Utilizes a binary format for compact and efficient storage of molecular data.\nExplicit Atom Connectivity: Stores atom connectivity explicitly, including implicit hydrogens, to ensure accurate representation.\nElectronic State and Stereochemistry: Encodes electronic states, stereochemistry, and other essential molecular attributes.\n2D Layout: Includes 2D layout information, similar to MOL files, for visualization and analysis.\nReaction Encoding: Supports the encoding of chemical reactions, facilitating reaction analysis and retrosynthesis.\nZlib Compression: Employs Zlib compression to further reduce storage size.",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>PaCh (Packed Chemicals): Computationally Effective Binary Format for Chemical Structure Encoding</span>"
    ]
  },
  {
    "objectID": "papers/PaCh.html#available-representations",
    "href": "papers/PaCh.html#available-representations",
    "title": "PaCh (Packed Chemicals): Computationally Effective Binary Format for Chemical Structure Encoding",
    "section": "Available representations",
    "text": "Available representations\n\nSMILES\n\nCompact linear string representation\n\nEncodes local stereochemistry and implicit hydrogens\nLimitations:\n\nRepresenting radicals, 2D/3D data, metal complexes\nRing closures beyond 100\nExplicit bonds\n\n\n\n\nINCHI\n\nComprehensive format endorsed by IUPAC\nEncodes connectivity, stereochemistry, tautomers\nIssues:\n\nFlexible hydrogen mapping\nAtomic charge determination\n\nLacks 2D/3D encoding in main part\n\n\n\nMDL MOL/RXN\n\nEncodes 2D/3D coordinates, explicit radicals/charges\n\nLimited stereochemistry support\nAmbiguity in implicit H counting\nLine length restrictions and optional zero fields hamper parsing\n\n\n\nCML/MRV\n\nXML-based, easier parsing than MDL\nEncodes 2D/3D\nImplicit H and stereochemistry inherit MOL ambiguities\n\nNote: The paper highlights the limitations of existing formats, motivating the need for a new format like PaCh that combines advantages while mitigating drawbacks.\n\n\nBinary Formats\nPrevious attempts at binary encoding of chemical structures, such as the Chemical JSON format, have aimed to improve efficiency but often fall short in terms of compression and speed. Nugmanov et al. (Nugmanov 2024) aim to overcome these limitations with Pach.",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>PaCh (Packed Chemicals): Computationally Effective Binary Format for Chemical Structure Encoding</span>"
    ]
  },
  {
    "objectID": "papers/PaCh.html#problem-setting",
    "href": "papers/PaCh.html#problem-setting",
    "title": "PaCh (Packed Chemicals): Computationally Effective Binary Format for Chemical Structure Encoding",
    "section": "Problem setting",
    "text": "Problem setting\n\nExisting chemical structure encoding formats are not optimized for computational efficiency.\nThere is a need for a format that balances compression, speed, and ease of use.",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>PaCh (Packed Chemicals): Computationally Effective Binary Format for Chemical Structure Encoding</span>"
    ]
  },
  {
    "objectID": "papers/PaCh.html#advantages-over-existing-formats",
    "href": "papers/PaCh.html#advantages-over-existing-formats",
    "title": "PaCh (Packed Chemicals): Computationally Effective Binary Format for Chemical Structure Encoding",
    "section": "Advantages over Existing Formats",
    "text": "Advantages over Existing Formats\n\nAddresses SMILES Limitations: Overcomes issues in SMILES related to representing radicals, complex stereochemistry, and disconnected components.\nResolves MOL Challenges: Avoids problems with implicit hydrogen counting and parsing difficulties associated with MOL files.\nCombines Strengths: Integrates the best features of SMILES (e.g., local stereo- and implicit hydrogen encoding) and MOL (e.g., explicit bond encoding and 2D layout).\n\nAdditionally, the authors propose that the previous representations might not be “rich” enough to capture the complexity of chemical reactions.",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>PaCh (Packed Chemicals): Computationally Effective Binary Format for Chemical Structure Encoding</span>"
    ]
  },
  {
    "objectID": "papers/PaCh.html#approach",
    "href": "papers/PaCh.html#approach",
    "title": "PaCh (Packed Chemicals): Computationally Effective Binary Format for Chemical Structure Encoding",
    "section": "Approach",
    "text": "Approach\n\nPaCh Format Overview\n\nHeader Byte (8 bit):\n\nDifferentiates format version and data type (molecular or reaction data).\nMolecule header byte: 2 (0x02).\nReaction header byte: 1 (0x01).\nThe remaining 253 values are reserved for future extensions.\nHard declaration of format version in the header to avoid compatibility issues seen in SMILES or MDL formats.\nChanges to specifications for 0x01 and 0x02 require a different header byte to maintain compatibility and parsing efficiency.\n\n\n\n\nMolecule Encoding\n\nHeader Byte: Starts with a byte equal to 2 (0x02).\n12-bit Atom Count: Encodes the number of atoms (max 4095 atoms).\n12-bit Chiral Cis–Trans Bond Count: Encodes the number of chiral cis-trans bonds.\n4-byte Header Block: Consists of the above-mentioned data.\nAtom Blocks: Each atom block is 9 bytes, repeated for each atom.\nAtom Connectivity Coding: Bidirectional coding of atom connectivity, preserving neighbour order essential for stereochemistry coding.\n\nFlattened adjacency matrix without zero connections and atom indices, coded bidirectionally for space efficiency.\n\nBond Orders: Stored without duplicates, representing the upper triangle of the adjacency matrix without zero connections.\n\nSingle bond: 0b000, double bond: 0b001, triple bond: 0b010, aromatic bond: 0b011, coordinate bond: 0b111.\n\nCis–Trans Bonds: Optional repeated 4-byte blocks for cis-trans bonds.\n\nCodes 12-bit numbers of cumulene terminal atoms and bytes with states 0x00 and 0x01 for trans- and cis-forms respectively.\n\n\n\n\nKey Points\n\nHeader Byte Values: 2 for molecules, 1 for reactions.\nFuture-Proofing: 253 values reserved for extensions.\nCompatibility: Hard declaration of version to avoid issues.\nEfficiency: Simplified parsing and implementation due to structured coding.\nConnectivity Block: Bidirectional, ensuring consistency in neighbour order.\nBond Orders: Stored efficiently without duplicates, utilizing up to 7 bits for alignment to full byte.\nCis–Trans Block: Codes cumulene terminal atoms and cis/trans states for bonds.\n\n\n\n\nFigure 1: Encoded molecule example from the paper by Nugmanov et al.:\n\n\n\nFigure 1 Overview:\n\n(a) Block overview: General structure of the molecule PaCh.\n(b) Atom (#2) block structure: Detailed structure of a specific atom block.\n(c) Adjacency matrix: Bond orders with connected atom indices and flattened connectivity.\n(d) Flattened bond orders for acetic acid: Uses 2 bytes with padding.\n(e) Ccis/Ttrans block: Bytes representation with white-gray row indicating the bytes.\n\nAtom Block Structure (Figure 1b):\n\n12-bit Atom ID: Unique label (1–4095) used in the connectivity block. Important for tasks like reaction analytics, database management, and property prediction.\n4-bit Atom Neighbor Count: Ranges from 0 (e.g., halides) to 15 (hypervalent complexes like ferrocenes).\n2-bit Tetrahedron Stereo Sign: Encodes one of four possible states: not chiral, unknown label, chirality sign.\n2-bit Allene Stereo Sign: Same notation as for tetrahedrons.\n5-bit Isotope Information: Difference from the standard isotope number +16 to avoid negative values. Special case for unspecified isotope. Covers a range from -15 to +15 relative to the standard.\n7-bit Atomic Number: Maximum value of 127, with the current last element being Og (118).\n4 Bytes for Coordinates: x,y coordinates of the atom in float16 format, sufficient for small molecules and saves space.\n3-bit Implicit Hydrogen Count: Ranges from 0 to 6, with a special case for unknown hydrogen count. Important for accurate analysis in cheminformatics. Proper representation requires Kekule form for structures with aromatic bonds.\n4-bit Atomic Charge: Ranges from -4 to 4, shifted by 4 to avoid negative numbers (e.g., 4 for neutral).\n1-bit Radical State: Binary coding for radical state (0b0 for no radical, 0b1 for radical). Suitable for most organic molecules with a single unpaired electron. Limitation in coding carbenes: 0 for singlet, 1 for triplet.\n\n\n\n\nReaction Encoding\n\nHeader Byte: Reaction PaCh begins with a header byte of 1 (0x01).\nByte Counts: Followed by 3 bytes representing the counts of reactants, reagents, and products.\nLimitation:Maximum of 255 molecules per category, totalling 765 molecules.\nContinuous Addition: Molecules are added continuously to the first 4 bytes without separators.\nMDL RXN Format: Similar to PaCh but differs in the order of molecules; reagents come last.\nSMILES Order: Uses the order of reactants-reagents-products with a “&gt;” as a separator.\nSize Information: Packed molecules must contain byte size information for proper restoration.\n\n\n\n\nFigure 2: Structure of the reaction PaCh with one reactant and one product from the paper by Nugmanov et al.:\n\n\nNote: PaCh is using Zlib compression for increasing memory effectiveness.\n\n\nStereochemistry Coding\n\nThe stereochemistry coding in PaCh is relative, meaning it depends on neighbours’ connectivity order. This is similar to SMILES notation and helps avoid misinterpretations compared to absolute stereochemistry coding.\n\nTetrahedron: The chirality of tetrahedral centers is determined by calculating the sign of the volume of a pyramid formed by the chiral atom and its three neighbouring atoms. A positive volume indicates clockwise rotation and is coded as 0b11, while a negative volume indicates counterclockwise rotation and is coded as 0b10. Cumulene cis/trans: The cis/trans configuration of cumulenes with an odd number of double bonds is determined by the relative orientation of the first neighbour of each terminal atom. If the first neighbours are on the same side of the double bond chain, it’s cis (0x01), and if they are on opposite sides, it’s trans (0x00). Allenes: The stereochemistry of allenes is determined by the rotation of vectors corresponding to the first neighbours of the terminal atoms across the double bond axis. Clockwise rotation is coded as 0b10, and counterclockwise rotation is coded as 0b11.",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>PaCh (Packed Chemicals): Computationally Effective Binary Format for Chemical Structure Encoding</span>"
    ]
  },
  {
    "objectID": "papers/PaCh.html#benchmarking",
    "href": "papers/PaCh.html#benchmarking",
    "title": "PaCh (Packed Chemicals): Computationally Effective Binary Format for Chemical Structure Encoding",
    "section": "Benchmarking",
    "text": "Benchmarking\nBenchmarking Results for Reading/Writing Operations on a 4200 Molecule Dataset in Milliseconds Measured in a Single Thread on Intel Xeon W-10885M CPU This is a direct comparison with RDKit to compare the speed of each tool.\n\n\n\n\n\n\n\n\n\n\n\nFormat\nRead (Chython)\nWrite (Chython)\nRead (RDKit)\nWrite (RDKit)\nSize (kB)\n\n\n\n\nMOL\n2446\n573\n404\n1256\n9634\n\n\nSMILES\n2475\n515\n1995\n268\n195\n\n\nPython pickle\n243\n72\n210\n123\n1857\n\n\n(RDKit) INCHI\n3760\n1720\n1060\n580\n-\n\n\nCML\n1710\n15894\n-\n-\n-\n\n\nPaCh\n107 (6)\n51\n-\n-\n1424\n\n\nPaCh + Zlib\n124\n122\n-\n-\n853\n\n\ncompressed MOL\n2454\n-\n-\n-\n-",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>PaCh (Packed Chemicals): Computationally Effective Binary Format for Chemical Structure Encoding</span>"
    ]
  },
  {
    "objectID": "papers/PaCh.html#feature-comparison-of-formats.",
    "href": "papers/PaCh.html#feature-comparison-of-formats.",
    "title": "PaCh (Packed Chemicals): Computationally Effective Binary Format for Chemical Structure Encoding",
    "section": "Feature Comparison of Formats.",
    "text": "Feature Comparison of Formats.\n\n– 2D Layout is Required; ** – Can be Calculated from a 2D Layout, Which Leads to False Labeling for Unknown Stereo\n\n\n\n\nFeature\nSMILE\nMOL\nCML/MRV\nPaCh\n\n\n\n\ncharge\nYes\nYes\nYes\nYes\n\n\nradical state\nNo\nYes\nYes\nYes\n\n\nimplicit hydrogens\nNo\nYes\nYes\nYes\n\n\nisotopes\nYes\nYes\nYes\nYes\n\n\n2D layout\nNo\nYes\nYes\nYes\n\n\ncoordinate bond\nNo\nYes\nYes\nYes\n\n\ntetrahedron stereo\nYes\nYes\nYes*\nYes\n\n\nallene stereo\nYes\nYes\nYes*\nYes\n\n\ncis–trans stereo\nYes\nYes\nYes**\nYes",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>PaCh (Packed Chemicals): Computationally Effective Binary Format for Chemical Structure Encoding</span>"
    ]
  },
  {
    "objectID": "papers/PaCh.html#example-of-usage",
    "href": "papers/PaCh.html#example-of-usage",
    "title": "PaCh (Packed Chemicals): Computationally Effective Binary Format for Chemical Structure Encoding",
    "section": "Example of Usage",
    "text": "Example of Usage\nThe PaCh format is implemented in a Chython–Python-based framework for molecule and reaction processing. Below is how to read and write PaCh using Chython.\n\nhttps://github.com/chython/chython\n\nfrom chython import MoleculeContainer, ReactionContainer, smiles\n# reaction example\nr = smiles('CC=O&gt;&gt;CCO')\np = r.pack()\nu = ReactionContainer.unpack(p)\nprint(r == u)\n\n\n# generate uncompressed PaCh\nm = smiles('CCN')\np = m.pack(compressed=False)\nu = MoleculeContainer.unpack(p, compressed=False)\nprint(m == u)",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>PaCh (Packed Chemicals): Computationally Effective Binary Format for Chemical Structure Encoding</span>"
    ]
  },
  {
    "objectID": "papers/PaCh.html#takeaways",
    "href": "papers/PaCh.html#takeaways",
    "title": "PaCh (Packed Chemicals): Computationally Effective Binary Format for Chemical Structure Encoding",
    "section": "Takeaways",
    "text": "Takeaways\n\nA new binary format, PaCh (Packed Chemicals), is introduced for encoding molecular and reaction.\nThis may be useful for deep learning where we have to use 2D coordinates (instead of CXSMILES) but have to test it in real-world scenarios.\nDesigned to be computationally efficient, combining the strengths of existing formats like SMILES and MDL MOL while addressing their limitations.\nPaCh is implemented in the Chython library and is used in the Chytorch deep learning framework for efficient handling of chemical data.",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>PaCh (Packed Chemicals): Computationally Effective Binary Format for Chemical Structure Encoding</span>"
    ]
  },
  {
    "objectID": "papers/PaCh.html#references",
    "href": "papers/PaCh.html#references",
    "title": "PaCh (Packed Chemicals): Computationally Effective Binary Format for Chemical Structure Encoding",
    "section": "References",
    "text": "References\n\n\n\n\nGuo, Jeff, and Philippe Schwaller. 2024. “It Takes Two to Tango: Directly Optimizing for Constrained Synthesizability in Generative Molecular Design.” arXiv. https://doi.org/10.48550/ARXIV.2410.11527.\n\n\nLandrum, Gregory A., Jessica Braun, Paul Katzberger, Marc T. Lehner, and Sereina Riniker. 2024. “Lwreg: A Lightweight System for Chemical Registration and Data Storage.” Journal of Chemical Information and Modeling 64 (16): 6247–52. https://doi.org/10.1021/acs.jcim.4c01133.\n\n\nNugmanov, Ramil. 2024. “PaCh (Packed Chemicals): Computationally Effective Binary Format for Chemical Structure Encoding.” Journal of Chemical Information and Modeling 64 (8): 3173–79. https://doi.org/10.1021/acs.jcim.3c01720.\n\n\nOrsi, Markus, and Jean-Louis Reymond. 2024. “One Chiral Fingerprint to Find Them All.” Journal of Cheminformatics 16 (1): 53.",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>PaCh (Packed Chemicals): Computationally Effective Binary Format for Chemical Structure Encoding</span>"
    ]
  },
  {
    "objectID": "papers/ms_prediction_graph_transformers/tandem_ms_prediction_graph_transformers.html",
    "href": "papers/ms_prediction_graph_transformers/tandem_ms_prediction_graph_transformers.html",
    "title": "Tandem mass spectrum prediction for small molecules using graph transformers",
    "section": "",
    "text": "Why discuss this paper?\nAuthors: Adamo Young, Hannes Röst & Bo Wang\nDOI: doi:10.1038/s42256-024-00816-8\nPublished: April 2024\nJournal: Nature Machine Intelligence",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Tandem mass spectrum prediction for small molecules using graph transformers</span>"
    ]
  },
  {
    "objectID": "papers/ms_prediction_graph_transformers/tandem_ms_prediction_graph_transformers.html#why-discuss-this-paper",
    "href": "papers/ms_prediction_graph_transformers/tandem_ms_prediction_graph_transformers.html#why-discuss-this-paper",
    "title": "Tandem mass spectrum prediction for small molecules using graph transformers",
    "section": "",
    "text": "I chose this article because, even after decades of work in the field, we still lack reference spectra, and the vast majority of small molecules lack experimental reference spectra, making the identification of new compounds a difficult task.\nIt tackles the fundamental challenge in Mass Spectrometry (MS) of accurately simulating the fragmentation process.\nThe results show promise for improved MS-based compound identification.",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Tandem mass spectrum prediction for small molecules using graph transformers</span>"
    ]
  },
  {
    "objectID": "papers/ms_prediction_graph_transformers/tandem_ms_prediction_graph_transformers.html#context",
    "href": "papers/ms_prediction_graph_transformers/tandem_ms_prediction_graph_transformers.html#context",
    "title": "Tandem mass spectrum prediction for small molecules using graph transformers",
    "section": "Context",
    "text": "Context\n\nOverview of Mass Spectrometry\n\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": true}} }%%\nflowchart LR;\n\nMass_Spectrometry_MS[\"&lt;b&gt;Mass Spectrometry (MS)&lt;/b&gt;\"]\nMass_Spectrometry_MS --&gt; Identifies_quantifies_chemicals[\"Identifies and quantifies chemicals in a mixture\"]\nMass_Spectrometry_MS --&gt; Ionized_detected_mass_analyzer[\"Molecules are ionized and detected by mass analyzer\"]\nMass_Spectrometry_MS --&gt; Records_mass_charge_ratio[\"Records mass-to-charge ratio (m/z)\"]\n\nTandem_MS_MS[\"&lt;b&gt;Tandem Mass Spectrometry (MS/MS)&lt;/b&gt;\"]\nTandem_MS_MS --&gt; Includes_fragmentation_step[\"Includes fragmentation step\"]\nTandem_MS_MS --&gt; Breaks_down_molecules[\"Breaks down molecules into smaller fragments\"]\nTandem_MS_MS --&gt; Infers_molecular_structure[\"Infers molecular structure of original molecule\"]\n\nLC_MS_MS[\"&lt;b&gt;LC-MS/MS&lt;/b&gt;\"]\nLC_MS_MS --&gt; Combines_liquid_chromatography[\"Combines liquid chromatography for separation\"]\nLC_MS_MS --&gt; Used_in_various_fields[\"Used in proteomics, metabolomics, forensics, environmental chemistry\"]\n\nMass_Spectrometry_MS --&gt; Tandem_MS_MS\nMass_Spectrometry_MS --&gt; LC_MS_MS\n\n\n\n\n\n\n\n\nChallenges in Mass Spectrometry\n\nMass spectrometry faces several significant challenges that impact its effectiveness.\n\n\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": true}} }%%\nflowchart LR;\n\nChallenges_in_MS[\"&lt;b&gt;Challenges in MS&lt;/b&gt;\"]\nChallenges_in_MS --&gt; Accurate_simulation_difficult[\"Accurate simulation of fragmentation is difficult\"]\nChallenges_in_MS --&gt; Simulations_slow_approximate[\"First principles simulations are slow and approximate\"]\nChallenges_in_MS --&gt; Incomplete_spectral_databases[\"Incomplete spectral databases hinder compound identification\"]\n\n\n\n\n\n\n\n\nOverview of Existing Solutions and Limitations\n\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": true}} }%%\nflowchart LR;\n\nExisting_Solutions_Limitations[\"&lt;b&gt;Existing Solutions and Limitations&lt;/b&gt;\"]\n\nDatabase_Searches[\"Database Searches\"]\nDatabase_Searches --&gt; Rely_large_libraries[\"Rely on large reference libraries and spectrum similarity functions\"]\nDatabase_Searches --&gt; Poor_coverage[\"Databases have poor coverage\"]\n\nIn_Silico_Spectra[\"In Silico Spectra\"]\nIn_Silico_Spectra --&gt; Augments_libraries[\"Augments spectral libraries with simulated spectra\"]\nIn_Silico_Spectra --&gt; Improves_coverage_match[\"Improves coverage and match finding\"]\n\nCompetitive_Fragmentation_Modelling[\"Competitive Fragmentation Modelling (CFM)\"]\nCompetitive_Fragmentation_Modelling[\"Competitive Fragmentation Modelling (CFM)\"] --&gt; Combinatorial_fragmentation[\"Combines combinatorial fragmentation and probabilistic modeling\"]\nCompetitive_Fragmentation_Modelling[\"Competitive Fragmentation Modelling (CFM)\"] --&gt; Slow_struggles_larger_compounds[\"Slow and struggles with larger compounds\"]\n\nDeep_Learning_Approaches[\"Deep Learning Approaches\"]\nDeep_Learning_Approaches --&gt; Fully_connected_networks[\"Use fully connected neural networks or graph neural networks\"]\nDeep_Learning_Approaches --&gt; Focus_local_structures[\"Focus on local structures\"]\nDeep_Learning_Approaches --&gt; Struggle_global_interactions[\"Struggle with modeling global interactions\"]\n\nExisting_Solutions_Limitations --&gt; Database_Searches\nExisting_Solutions_Limitations --&gt; In_Silico_Spectra\nExisting_Solutions_Limitations --&gt; Competitive_Fragmentation_Modelling\nExisting_Solutions_Limitations --&gt; Deep_Learning_Approaches",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Tandem mass spectrum prediction for small molecules using graph transformers</span>"
    ]
  },
  {
    "objectID": "papers/ms_prediction_graph_transformers/tandem_ms_prediction_graph_transformers.html#overview-of-massformer",
    "href": "papers/ms_prediction_graph_transformers/tandem_ms_prediction_graph_transformers.html#overview-of-massformer",
    "title": "Tandem mass spectrum prediction for small molecules using graph transformers",
    "section": "Overview of MassFormer",
    "text": "Overview of MassFormer\n\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": true}} }%%\nflowchart LR;\n\nDevelopment_of_MassFormer[\"&lt;b&gt;Development of MassFormer&lt;/b&gt;\"]\nDevelopment_of_MassFormer --&gt; Adapts_graph_transformer[\"Adapts graph transformer architecture for MS/MS spectrum prediction\"]\nDevelopment_of_MassFormer --&gt; Graph_transformers_model[\"Graph Transformers model pairwise interactions between all nodes\"]\nDevelopment_of_MassFormer --&gt; Captures_local_global[\"Captures both local and global structural information\"]\nDevelopment_of_MassFormer --&gt; Unique_Positional_Encoding[\"Unique Positional Encoding uses degree and shortest path information\"]\n\n\n\n\n\n\n\nOverview of Steps involved in MassFormer\n \nMolecule to Spectrum - Unlike traditional methods, MassFormer utilizes a graph-based approach to represent molecules. - Incorporating spectral metadata, such as collision energy used during the measurement, allows MassFormer to account for factors that might influence the fragmentation patterns observed in the final spectrum prediction. - Multihead self-attention : MHA - allows an input to attend to relevant parts of itself - Multilayer perceptrons : MLPs - capture non-linear relationships between the resulting representation and the final output.",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Tandem mass spectrum prediction for small molecules using graph transformers</span>"
    ]
  },
  {
    "objectID": "papers/ms_prediction_graph_transformers/tandem_ms_prediction_graph_transformers.html#results",
    "href": "papers/ms_prediction_graph_transformers/tandem_ms_prediction_graph_transformers.html#results",
    "title": "Tandem mass spectrum prediction for small molecules using graph transformers",
    "section": "Results",
    "text": "Results\n \nSpectrum similarity experiments: Predicted vs. Real spectra for two held-out compounds (Azlocillin and Flamprop-isopropyl) - Accurate Predictions: MassFormer can predict realistic mass spectra for held out compounds. The high cosine similarity values (around 0.6) indicate a close match between the predicted and actual spectra. - Outperforms Existing Models: When compared to other deep learning models (CFM, FP, WLN) on a larger dataset, MassFormer consistently performs better. It shows the highest average cosine similarity across different data splits. - NIST: National Institute of Standards and Technology - MoNA: MassBank of North America - FP: Fingerprint (FP) neural network model - WLN: Weisfeiler–Lehman (WLN) graph neural network model\n \nCollision energy experiments: Benzhydrylpiperazine - MassFormer can not only predict mass spectra accurately but also captures the influence of collision energy on the fragmentation patterns observed in the spectra. - This ability is important for real-world applications where collision energy is a variable factor during data acquisition. - NCEs: Normalized collision energies\n \nExplainability using gradient attributions: Mass spectrum of propranolol - Separation of Nitrogen-Containing Peaks: The gradient attribution maps reveal that nitrogen-containing peaks can be linearly separated from those without nitrogen, demonstrating that the model captures meaningful chemical information related to nitrogen content. - Improved Classification Accuracy: The linear classification accuracy is significantly higher when using nitrogen labeling compared to random labeling, with nearly half of the peaks being perfectly separable based on their gradient attributions, highlighting the effectiveness of the attributions in distinguishing chemical compositions.",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Tandem mass spectrum prediction for small molecules using graph transformers</span>"
    ]
  },
  {
    "objectID": "papers/ms_prediction_graph_transformers/tandem_ms_prediction_graph_transformers.html#advantages-of-massformer",
    "href": "papers/ms_prediction_graph_transformers/tandem_ms_prediction_graph_transformers.html#advantages-of-massformer",
    "title": "Tandem mass spectrum prediction for small molecules using graph transformers",
    "section": "Advantages of MassFormer",
    "text": "Advantages of MassFormer\n\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": true}} }%%\nflowchart LR;\n\nAdvantages_of_MassFormer[\"&lt;b&gt;Advantages of MassFormer&lt;/b&gt;\"]\n\nAccurate_Predictions[\"Accurate Predictions\"]\nAccurate_Predictions --&gt; State_of_the_art[\"State-of-the-art performance in spectrum prediction\"]\nAccurate_Predictions --&gt; Outperforms_existing_methods[\"Outperforms existing baseline methods\"]\n\nGlobal_Interactions[\"Global Interactions\"]\nGlobal_Interactions --&gt; Models_global_interactions[\"Models global interactions between distant atoms\"]\nGlobal_Interactions --&gt; Improves_understanding[\"Improves understanding of fragmentation events\"]\n\nEfficiency[\"Efficiency\"]\nEfficiency --&gt; Leverages_pretrained_models[\"Leverages pretrained graph transformer models\"]\nEfficiency --&gt; Accurate_predictions[\"Provides accurate predictions without excessive computational costs\"]\n\nReal_World_Applications[\"Real-World Applications\"]\nReal_World_Applications --&gt; Validated_spectrum_identification[\"Validated for spectrum identification problems\"]\nReal_World_Applications --&gt; Handles_collision_energy[\"Handles effects of collision energy on spectra\"]\n\nAdvantages_of_MassFormer --&gt; Accurate_Predictions\nAdvantages_of_MassFormer --&gt; Global_Interactions\nAdvantages_of_MassFormer --&gt; Efficiency\nAdvantages_of_MassFormer --&gt; Real_World_Applications",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Tandem mass spectrum prediction for small molecules using graph transformers</span>"
    ]
  },
  {
    "objectID": "papers/ms_prediction_graph_transformers/tandem_ms_prediction_graph_transformers.html#takeaways",
    "href": "papers/ms_prediction_graph_transformers/tandem_ms_prediction_graph_transformers.html#takeaways",
    "title": "Tandem mass spectrum prediction for small molecules using graph transformers",
    "section": "Takeaways",
    "text": "Takeaways\n\nThe current study introduces MassFormer, a novel method utilizing graph transformers for small molecule MS/MS spectra prediction.\nWhile demonstrating strong performance, MassFormer’s applicability is currently limited by compatibility with various data types.\nThe model offers explainability for its predictions; however, further development is needed for detailed peak annotations.\nMassFormer holds significant promise for MS-based compound identification, potentially enhancing existing tools and even aiding spectrum-to-structure generation. (Focuses on future potential and broader applications)",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Tandem mass spectrum prediction for small molecules using graph transformers</span>"
    ]
  },
  {
    "objectID": "papers/ms_prediction_graph_transformers/tandem_ms_prediction_graph_transformers.html#data-availability",
    "href": "papers/ms_prediction_graph_transformers/tandem_ms_prediction_graph_transformers.html#data-availability",
    "title": "Tandem mass spectrum prediction for small molecules using graph transformers",
    "section": "Data availability",
    "text": "Data availability\nAll public data from the study have been uploaded to Zenodo at https://doi.org/10.5281/zenodo.8399738. Some data that support the findings of this study are available from the National Institute of Standards and Technology (NIST). However, its access is subject to restrictions, requiring the purchase of an appropriate license or special permission from NIST.",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Tandem mass spectrum prediction for small molecules using graph transformers</span>"
    ]
  },
  {
    "objectID": "papers/ms_prediction_graph_transformers/tandem_ms_prediction_graph_transformers.html#code-availability",
    "href": "papers/ms_prediction_graph_transformers/tandem_ms_prediction_graph_transformers.html#code-availability",
    "title": "Tandem mass spectrum prediction for small molecules using graph transformers",
    "section": "Code availability",
    "text": "Code availability\nThe code used in this study is open-source (BSD-2-Clause license) and can be found in a GitHub repository (https://github.com/Roestlab/massformer/) with a DOI of https://doi.org/10.5281/zenodo.10558852.\n\n\n\n\nGuo, Jeff, and Philippe Schwaller. 2024. “It Takes Two to Tango: Directly Optimizing for Constrained Synthesizability in Generative Molecular Design.” arXiv. https://doi.org/10.48550/ARXIV.2410.11527.\n\n\nLandrum, Gregory A., Jessica Braun, Paul Katzberger, Marc T. Lehner, and Sereina Riniker. 2024. “Lwreg: A Lightweight System for Chemical Registration and Data Storage.” Journal of Chemical Information and Modeling 64 (16): 6247–52. https://doi.org/10.1021/acs.jcim.4c01133.\n\n\nOrsi, Markus, and Jean-Louis Reymond. 2024. “One Chiral Fingerprint to Find Them All.” Journal of Cheminformatics 16 (1): 53.",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Tandem mass spectrum prediction for small molecules using graph transformers</span>"
    ]
  },
  {
    "objectID": "papers/open-source-cheminformatics.html",
    "href": "papers/open-source-cheminformatics.html",
    "title": "Open-Source Software Development in Cheminformatics: A Qualitative Analysis of Rationales",
    "section": "",
    "text": "Why discuss this paper?\nI chose the Open-Source Software Development in Cheminformatics: A Qualitative Analysis of Rationales paper (Pernaa et al. 2023) for current topics in the cheminformatics seminar because:",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Open-Source Software Development in Cheminformatics: A Qualitative Analysis of Rationales</span>"
    ]
  },
  {
    "objectID": "papers/open-source-cheminformatics.html#why-discuss-this-paper",
    "href": "papers/open-source-cheminformatics.html#why-discuss-this-paper",
    "title": "Open-Source Software Development in Cheminformatics: A Qualitative Analysis of Rationales",
    "section": "",
    "text": "Introducing the concept of “qualitative content analysis” which might be unfamiliar to our group’s regular work, along with the used software ATLAS.ti 9.\nDiscuss Open-Source in Chemoinformatics due to its relevance in our work in NFDI4Chem.\nThe covered discussion over business, and business politics which might be of interest to us at this level of our careers.",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Open-Source Software Development in Cheminformatics: A Qualitative Analysis of Rationales</span>"
    ]
  },
  {
    "objectID": "papers/open-source-cheminformatics.html#context",
    "href": "papers/open-source-cheminformatics.html#context",
    "title": "Open-Source Software Development in Cheminformatics: A Qualitative Analysis of Rationales",
    "section": "Context",
    "text": "Context\nThe paper demonstrates and promotes the different rationales of open-source in cheminformatics by extracting them from different sources and categorizing them for rationales designing in future software development projects. Additionally, the authors transparently cover the arguments against this approach.",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Open-Source Software Development in Cheminformatics: A Qualitative Analysis of Rationales</span>"
    ]
  },
  {
    "objectID": "papers/open-source-cheminformatics.html#cheminformatics-in-chemistry-brief-history",
    "href": "papers/open-source-cheminformatics.html#cheminformatics-in-chemistry-brief-history",
    "title": "Open-Source Software Development in Cheminformatics: A Qualitative Analysis of Rationales",
    "section": "Cheminformatics in chemistry: Brief history",
    "text": "Cheminformatics in chemistry: Brief history\nCheminformatics has been used in chemistry since the 1940s (Chen 2006). King et al. (King, Cross, and Thomas 1946) may be the first scholars who applied computers in chemistry research. However, Ray and Kirsch (Ray and Kirsch 1957) described an algorithm for substructure searching in a paper published in 1957, which might be considered the first actual cheminformatics paper.\nAlthough cheminformatics as a field has been around for many decades, much of the cheminformatics research has been conducted in industrial laboratories, not academia. Because of this, many applications and methods are not published due to intellectual-property-rights issues (Willett 2011).",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Open-Source Software Development in Cheminformatics: A Qualitative Analysis of Rationales</span>"
    ]
  },
  {
    "objectID": "papers/open-source-cheminformatics.html#problem-setting",
    "href": "papers/open-source-cheminformatics.html#problem-setting",
    "title": "Open-Source Software Development in Cheminformatics: A Qualitative Analysis of Rationales",
    "section": "Problem setting",
    "text": "Problem setting\n\nThe development of the field requires open-source technology\nOpen-source software development in cheminformatics is necessary for cheminformatics education.\nOpen source and open data enable reproducibility which supports scientific reliability.\nOpen-source code lowers the research costs significantly.",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Open-Source Software Development in Cheminformatics: A Qualitative Analysis of Rationales</span>"
    ]
  },
  {
    "objectID": "papers/open-source-cheminformatics.html#approach",
    "href": "papers/open-source-cheminformatics.html#approach",
    "title": "Open-Source Software Development in Cheminformatics: A Qualitative Analysis of Rationales",
    "section": "Approach",
    "text": "Approach\nThe authors considered research articles with “open source”-related concepts that were then analyzed using qualitative content analysis and ATLAS.ti 9 software to extract relevant expressions and generate rationales subcategories, which were then classified into main categories.\n\n\n\n\n\n\n\n\nOriginal Expression\nSub-Category\nMain Category\n\n\n\n\n“Despite these efforts, no general purpose deterministic structure generator has been developed in an open source format so far.” (Peironcely et al. 2012)\nNo available open-source alternative\nDevelop New Software\n\n\n“The ChemoPy package aims at providing the user with comprehensive implementations of these descriptors in a unified framework to allow easy and transparent computation.” (Cao et al. 2013)\nClear workflow\nImprove Usability",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Open-Source Software Development in Cheminformatics: A Qualitative Analysis of Rationales</span>"
    ]
  },
  {
    "objectID": "papers/open-source-cheminformatics.html#results",
    "href": "papers/open-source-cheminformatics.html#results",
    "title": "Open-Source Software Development in Cheminformatics: A Qualitative Analysis of Rationales",
    "section": "Results",
    "text": "Results\nThe analysis produced six main rationale categories for open-source cheminformatics software development. The perspective of the rationale can be either general or specific. Most rationales produce technological outcomes, such as new software, frameworks, interfaces, and processes.\n\n\n\n\n\n\n\n\n\n#\nRationale\nPerspective\nOutcome\n\n\n\n\n1\nDevelop New Software\nGeneral/Specific\nTechnological\n\n\n2\nUpdate Current Features, Tools, or Processes\nSpecific\nTechnological\n\n\n3\nImprove Usability\nGeneral\nTechnological\n\n\n4\nSupport Open-source Development and Open Science\nGeneral/Specific\nTechnological/Political\n\n\n5\nFulfill Chemical Information Needs\nSpecific\nContent-driven\n\n\n6\nSupport Chemistry Learning and Teaching\nGeneral/Specific\nPedagogical",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Open-Source Software Development in Cheminformatics: A Qualitative Analysis of Rationales</span>"
    ]
  },
  {
    "objectID": "papers/open-source-cheminformatics.html#take-aways",
    "href": "papers/open-source-cheminformatics.html#take-aways",
    "title": "Open-Source Software Development in Cheminformatics: A Qualitative Analysis of Rationales",
    "section": "Take aways",
    "text": "Take aways\n\nThe most central challenge hindering the academic development of cheminformatics has been the industrial background.\nOpen source development can bridge academic and industrial stakeholders.",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Open-Source Software Development in Cheminformatics: A Qualitative Analysis of Rationales</span>"
    ]
  },
  {
    "objectID": "papers/open-source-cheminformatics.html#references",
    "href": "papers/open-source-cheminformatics.html#references",
    "title": "Open-Source Software Development in Cheminformatics: A Qualitative Analysis of Rationales",
    "section": "References",
    "text": "References\n\n\n\n\nCao, Dong-Sheng, Qing-Song Xu, Qian-Nan Hu, and Yi-Zeng Liang. 2013. “ChemoPy: Freely Available Python Package for Computational Biology and Chemoinformatics.” Bioinformatics 29 (8): 1092–94.\n\n\nChen, William Lingran. 2006. “Chemoinformatics: Past, Present, and Future.” Journal of Chemical Information and Modeling 46 (6): 2230–55.\n\n\nGuo, Jeff, and Philippe Schwaller. 2024. “It Takes Two to Tango: Directly Optimizing for Constrained Synthesizability in Generative Molecular Design.” arXiv. https://doi.org/10.48550/ARXIV.2410.11527.\n\n\nKing, Gilbert W, Paul C Cross, and George B Thomas. 1946. “The Asymmetric Rotor III. Punched-Card Methods of Constructing Band Spectra.” The Journal of Chemical Physics 14 (1): 35–42.\n\n\nLandrum, Gregory A., Jessica Braun, Paul Katzberger, Marc T. Lehner, and Sereina Riniker. 2024. “Lwreg: A Lightweight System for Chemical Registration and Data Storage.” Journal of Chemical Information and Modeling 64 (16): 6247–52. https://doi.org/10.1021/acs.jcim.4c01133.\n\n\nOrsi, Markus, and Jean-Louis Reymond. 2024. “One Chiral Fingerprint to Find Them All.” Journal of Cheminformatics 16 (1): 53.\n\n\nPeironcely, Julio E, Miguel Rojas-Chertó, Davide Fichera, Theo Reijmers, Leon Coulier, Jean-Loup Faulon, and Thomas Hankemeier. 2012. “OMG: Open Molecule Generator.” Journal of Cheminformatics 4: 1–13.\n\n\nPernaa, Johannes, Aleksi Takala, Veysel Ciftci, José Hernández-Ramos, Lizethly Cáceres-Jensen, and Jorge Rodrı́guez-Becerra. 2023. “Open-Source Software Development in Cheminformatics: A Qualitative Analysis of Rationales.” Applied Sciences 13 (17): 9516.\n\n\nRay, Louis C, and Russell A Kirsch. 1957. “Finding Chemical Records by Digital Computers.” Science 126 (3278): 814–19.\n\n\nWillett, Peter. 2011. “Chemoinformatics: A History.” Wiley Interdisciplinary Reviews: Computational Molecular Science 1 (1): 46–56.",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Open-Source Software Development in Cheminformatics: A Qualitative Analysis of Rationales</span>"
    ]
  },
  {
    "objectID": "papers/universal_chiral_fp_2024.html",
    "href": "papers/universal_chiral_fp_2024.html",
    "title": "MAP4C: One chiral fingerprint to find them all (Orsi and Reymond 2024)",
    "section": "",
    "text": "Why did I choose this paper?",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>MAP4C: One chiral fingerprint to find them all [@orsi2024one]</span>"
    ]
  },
  {
    "objectID": "papers/universal_chiral_fp_2024.html#why-did-i-choose-this-paper",
    "href": "papers/universal_chiral_fp_2024.html#why-did-i-choose-this-paper",
    "title": "MAP4C: One chiral fingerprint to find them all (Orsi and Reymond 2024)",
    "section": "",
    "text": "one of the latest papers for molecular fingerprints\ncombines two important FPs being ECFP and AP FPs\nwanted to learn something about fingerprints, because of potential importance for future machine learning tasks",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>MAP4C: One chiral fingerprint to find them all [@orsi2024one]</span>"
    ]
  },
  {
    "objectID": "papers/universal_chiral_fp_2024.html#general",
    "href": "papers/universal_chiral_fp_2024.html#general",
    "title": "MAP4C: One chiral fingerprint to find them all (Orsi and Reymond 2024)",
    "section": "General",
    "text": "General\n\nAuthors:\n\nMarkus Orsi → University of Bern, Biochemistry and Pharmacy\nJean-Louis Reymond → same as Orsi\nboth are Cheminformaticians (MAYGEN, Surge, etc)\n\nJournal:\n\nJournal of Cheminformatics, 2024\n\nNotes:\n\nMAP4C = Minhashed Atom-Pair Chiral Fingerprint with diameter 4\nbuilds on top of previously published MAP4 (Capecchi, Probst, and Reymond 2020)",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>MAP4C: One chiral fingerprint to find them all [@orsi2024one]</span>"
    ]
  },
  {
    "objectID": "papers/universal_chiral_fp_2024.html#prior-work",
    "href": "papers/universal_chiral_fp_2024.html#prior-work",
    "title": "MAP4C: One chiral fingerprint to find them all (Orsi and Reymond 2024)",
    "section": "Prior work",
    "text": "Prior work\n\nExtended connectivity fingerprint (Rogers and Hahn 2010)\n\ndigital representation of a molecule’s structure\nconversion of molecule’s structural or chemical features into a binary or bit-string format\nhistorically developed for substructure and similarity searching + structure-activity modeling\nadvantages: fast calculation and representation of infinite number of molecular features\n\n\nWorkflow\n\n\n\n\n\n\n\n\n\n\nStep 1: Assigning an integer identifier to each atom\n\nrandom enumeration of atoms\nfor each atom the following properties are then quantified\n\nNumber of non-hydrogen immediate neighbors (3)\ntotal bond order ignoring bonds to hydrogens (4 - 0)\nAtomic number (6)\nAtomic mass (12)\nAtomic charge (0)\nNumber of attached hydrogens (0)\nis atom part of ring? (0 –&gt; no)\n\nall features are then hashed into one integer\n\nidentifier = hash((3, 4, 6, 12, 0, 0, 0))\nprint(identifier)\n# -2155244659601281804\n\nrepeat this process for all non-hydrogen atoms\n\n# Iteration 0\n\n1: -4080868480043360372\n2:  8311098529014133067\n3:  8311098529014133067\n4: -2155244659601281804\n5: -3602994677767288312\n6:  8573586092015465947\nStep 2: Iteratively updating the atom identifiers\n\ninitialization of list of tuples with iteration number and identifier (Atom 4)\n\n[(1, -2155244659601281804)]\n\naddition of two more numbers for each non-hydrogen neighbor\n\nfirst: bond order\nsecond: neighbours identifier\n\nconversion to pure list\n\n[1, -2155244659601281804, 1, -3602994677767288312, 1, 8311098529014133067, 2, 8573586092015465947]\n\nlist is then hashed again\n\nfeature now includes information about direct neighbors\n\n\nidentifier_updated = hash([1, -2155244659601281804, 1, -3602994677767288312, 1, 8311098529014133067, 2, 8573586092015465947])\nprint(identifier_updated)\n# 3790237506519639747\n\n...\n\n# Iteration 1\n\n1: -3879702859024654160\n2:  2648074263463118673\n3:  9209025387859845960\n4:  3790237506519639747\n5: -8399737669368778010\n6:  3271801898087186516\n\nrepetition of the mentioned process but with neighbors of neighbors (Iteration 2)\nafter each iteration the identifiers are added to a feature list\n\n# 18 items, 6 per iteration  (0, 1 and 2)\n\n[-4080868480043360372, 8311098529014133067, 8311098529014133067, -2155244659601281804, -3602994677767288312, 8573586092015465947, -3879702859024654160, 2648074263463118673, 9209025387859845960, 3790237506519639747, -8399737669368778010, 3271801898087186516, 7820245418060671737, -8234949431280515543, -5902629546112570760, -3660103599533977242, -5964710996914813053, 8916398073441202914]\n\n\n\nIterations shown with different example: benzoic acid amide\n\n\n\n\n\n\nStep 3: Deduplication\n\nremoving duplicates from feature list (4 removed, 14 left)\n\n\n\n\n\nStep 4: Conversion of identifiers to bit array\n\ninitialize 1024 bit zero-array\n\nimport numpy as np\nfp = np.zeros(1024)     # traditionally a 1024 bit array is chosen\nprint(fp)\n# array([0., 0., 0., ..., 0., 0., 0.])\n\ndivision of each identifier by bit-array length (1024) and calculation of remainder\n\nExample: 4080868480043360372 % 1024 = 908\n\n\nremainders = [908, 331, 244, 520, 475, 176, 849, 840, 707, 742, 84, 553, 632, 358]\n\neach remainder represents the position/index in the bit zero-array at which a zero is changed to a one\n\nfor x in remainders:\nfp[x] = 1\nprint(fp)\n# [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n\nfinal bit array has been created = molecular fingerprint\n\n\n\n\n\n\n\nAtom-Pair fingerprint (Carhart, Smith, and Venkataraghavan 1985)\n\n\n\nsimilar to ECFP but for each atom all possible atom pairs including their distance are collected as features \n\n\n\nAP vs ECFP\n\nAP is suitable for large molecules but does not encode molecular structure in detail\nECFP is very detailed and suitable for small molecules but misses poor perception of global features",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>MAP4C: One chiral fingerprint to find them all [@orsi2024one]</span>"
    ]
  },
  {
    "objectID": "papers/universal_chiral_fp_2024.html#abstract",
    "href": "papers/universal_chiral_fp_2024.html#abstract",
    "title": "MAP4C: One chiral fingerprint to find them all (Orsi and Reymond 2024)",
    "section": "Abstract",
    "text": "Abstract\n\nchirality often not considered in fingerprints\ndeveloped MAP4C (chiral version of MAP4, publ. in 2020)\nMAP → “MinHashed Atom-Pair fingerprint up to four bonds”\nconcept:\n\n“MinHashes computed from character strings containing the SMILES of all pairs of circular substructures up to a diameter of four bonds and the shortest topological distance between their central atoms.”\nMinHash = compression technique and used for similarity estimation (Probst and Reymond 2018)\n\neasy explanation of minhashing from other field\n\nincludes Cahn-Ingold-Prelog annotation (r,s)\n“?” is used for undefined stereocenters and double bonds\n\nclaim:\n\ncan distinguish between stereoisomers of small drugs to large natural products and peptides",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>MAP4C: One chiral fingerprint to find them all [@orsi2024one]</span>"
    ]
  },
  {
    "objectID": "papers/universal_chiral_fp_2024.html#introduction",
    "href": "papers/universal_chiral_fp_2024.html#introduction",
    "title": "MAP4C: One chiral fingerprint to find them all (Orsi and Reymond 2024)",
    "section": "Introduction",
    "text": "Introduction\n\nWhat is a FP?\n\nvectors encoding molecular structure\n\n\n\n\n\nusage of fingerprints in general:\n\nsimilarity & substructure search, clustering\nligand-based virtual screening & target prediction\n\n\n\n\n\nWhy are there almost no chiral FPs?\n\nmany small drug-like compounds are achiral (Paracetamol, Xylometazolin (Nasenspray), Amylmetakresol (Neo-Angin))\n\n\n\n“Correlation between chirality and heavy atom count (HAC) across ChEMBL, COCONUT, and ZINC datasets. The blue line depicts the percentage of chiral molecules relative to HAC. A steady increase in the percentage of chiral molecules is observed with increasing HAC. The yellow line represents the total count of molecules corresponding to each HAC.”\n\n\n\n\n\n\n\nPrinciple of MAP4C?\n\ncombination of circular substructure encoding with data compression using MinHashing\nencodes all possible pairs of circular substructures up to diameter of 4 bonds\npairs are written as two canonical smiles separated by shortest topological distance, counted in bonds between corresponding pair of central atoms\nincludes CIP-annotation\nseems to be combination of circular FP and Atom Pair FP\n\nAtom Pair = list all possible pairs and their distance\n\n\n\n\n\n\nAdvantage over ECFP4 (Extended Connectivity) and AP (Atom Pair) FPs?\n\nspans wider range of compound classes like small molecules, NPs, peptides and metabolites (claim)\nis chiral",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>MAP4C: One chiral fingerprint to find them all [@orsi2024one]</span>"
    ]
  },
  {
    "objectID": "papers/universal_chiral_fp_2024.html#methods",
    "href": "papers/universal_chiral_fp_2024.html#methods",
    "title": "MAP4C: One chiral fingerprint to find them all (Orsi and Reymond 2024)",
    "section": "Methods",
    "text": "Methods\n\nimplementation in python (RDKit)\n\nextraction of circular substructures at every non-hydrogen atom up to specified radius (r=2) as SMILES\n\nisomeric information (“@”, “@@”) is removed manually\nallene and conformational chirality not considered because not supported by SMILES (\nRadius 0 skipped (no single atom information)\n\nif central atom is chiral, first atom symbol of max-radius SMILES is replaced by CIP-descriptor (e.g. “\\(R\\)”)\nat each radius “shingles” are generated for all possible pairs of extracted substructures\n\nsubstructure 1 | topological distance | substructure 2\n“shingles” → subsequences of a sequence\n\nApplication of MinHash to obtain fixed size vector\n\n\n\n“Chiral shingle generation concept exemplified on a selected atom pair of polymyxin B2. The generated shingle corresponds to the pair of circular substructures (blue) separated by the shortest topological distance (red) of their central atoms. Whenever the central atom of a substructure is chiral, the atom symbol in the substructure SMILES is replaced by the Cahn-Ingold-Prelog (CIP) descriptor (R, S, r, or s), or by a question mark (?) if the stereochemistry is not defined, bracketed by two “$” characters (yellow).”\n\n\n\nBenchmark\n\nMAP4C was compared with ECFP4 and 6, and Atom-Pair FP\nBenchmark by Riniker and Landmark which was made chiral + 60 peptide sets\n\n\n\n\nFigure S3. Distribution of molecular weight (MW) (yellow), number of stereocenters (magenta) and ratio of stereocenters to heavy atom count (blue) in the set uniformly sampled from the extended benchmark. The set contained a total of 10,122 compounds and was used to determine the relative impact of stereochemistry encoding on total similarity.\n\n\ngenerated all possible stereoisomers, isomers and scrambled sequences with RDKit\n\nusage of 2048 bit vectors\n\n\n\n\n\nTMAP:\n\nvisualization of biochemical similarity",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>MAP4C: One chiral fingerprint to find them all [@orsi2024one]</span>"
    ]
  },
  {
    "objectID": "papers/universal_chiral_fp_2024.html#results",
    "href": "papers/universal_chiral_fp_2024.html#results",
    "title": "MAP4C: One chiral fingerprint to find them all (Orsi and Reymond 2024)",
    "section": "Results",
    "text": "Results\n\nEncoding stereochemistry in MAP fingerprints\n\nIn simple words: map4 and map4c of the same molecule with and without stereochemistry are similar, as intended\n\n\n\n\n\nVirtual screening benchmark\n\nAUC, EF1, BEDROC, etc are all metrics for comparing virtual screening methods whose purpose is to rank active compounds towards e.g. an enzyme\ncomparison of performance of MAP, ECFP and AP FPs and their chiral counter parts → MAP4C performed best (fig 2c)\n\n\n\n\n“Mean ranks of fingerprints across all virtual screening datasets for each metric. Small molecule sets (ChEMBL, DUD, MUV) and peptide sets are presented separately to highlight the differences in relative performance”\n\n\nexplanation: combination of high local precision from ECFP and reflection of global features from Atom-Pairs FP\nslight bump compared to non-chiral MAPs due to chirality\n\n\n\n\n\n\nFinding all stereoisomers\n\nTab1 shows ability to assign a different fingerprint value for each stereoisomer on a series of stereochemically complex molecules comprizing carbohydrates, peptides and macrocyclic natural products (molecules shown in fig3)\n\nMAPC performed well on sugars and peptides\nperformance decreased for macrocyclic NPs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuery\nN / Sym.\nTotal\nMAP6C\nMAP4C\nMAP2C\nAPC\nECFP6C\nECFP4C\n\n\n\n\nα-D-glucopyranose (2)\n5 /–\n32\n32\n32\n32\n11\n32\n32\n\n\nLactose (3)\n10 / –\n1,024\n1,024\n1,024\n992\n443\n1,024\n1,024\n\n\nTrehalose (4)\n10 / C2\n528\n528\n528\n516\n336\n528\n512\n\n\nValidamycin A (5)\n14 / –\n16,384\n16,384\n16,384\n16,384\n7,657\n16,384\n16,384\n\n\nInositol (6)\n6 / C6v\n9\n9\n9\n9\n1\n1\n1\n\n\nln65 (7)\n11 / –\n2,048\n2,048\n2,048\n2,048\n196\n1,140\n36\n\n\nln65 (scrambled)\n11 / –\n330\n330\n330\n330\n330\n8\n4\n\n\nln65 (dia × scrambled)\n11 / –\n675,840\n675,840\n675,840\n675,840\n90,217\n38,500\n144\n\n\nR9 (8)\n9 / –\n512\n512\n512\n512\n146\n88\n12\n\n\nPolymyxin B2 (1)\n12 / –\n4,096\n4,096\n4,096\n4,096\n2,500\n4,096\n1,536\n\n\nPMB2 (scrambled)\n9 / –\n1,512\n1,512\n1,512\n1,512\n1,512\n861\n75\n\n\nPMB2 (dia × scrambled)\n9 / –\n774,144\n774,144\n774,144\n774,144\n287,631\n602,003\n9,312\n\n\nPMB2 (R, S or undefined)\n12 / –\n531,441\n531,441\n531,441\n531,441\n277,901\n531,441\n137,781\n\n\nQuinaldopeptin (9)\n8 / C2\n136\n136 g)\n136\n134\n64\n132\n90\n\n\nOnchidin (10)\n12 / C2\n2,080\n2,080\n2,080\n2,064\n469\n1,760\n810\n\n\nGramicidin S (11)\n10 / C2\n528\n528\n504\n334\n25\n448\n243\n\n\nValinomycin (12)\n12 / C3\n1,376\n1,250\n714\n416\n112\n616\n27\n\n\nNonactin (13)\n16 / C4\n16,456\n16,425\n16,176\n10,045\n13,189\n6,474\n675\n\n\nNP213 (14)\n7 / C7\n20\n7\n13\n17\n13\n5\n3\n\n\n\n\n\n\n\n\n\n“Structures of natural products and peptides selected for the stereoisomer distinction task”\n\n\n\n\n\n\nRanking stereoisomers versus isomers\n\nFig4 shows that MAP4C works well in differentiating stereo and structural isomers\n\nthis is indicated by the much higher jaccard distance (similarity measure) for the MAP FPs, which shows that though there are only small changes in chirality and structure, the FP is still sensitive to their difference\n\n\n\n“structural isomers of 1,4-diaminocyclohexane (203) and 4-aminopiperidine (48) and their diastereomers. The skewed distribution of Jaccard distance of 15 with MAP6C is caused by two outliers exhibiting a distance of 0 which cannot be represented on the log scale and is likely due to a bit-clash issue.”",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>MAP4C: One chiral fingerprint to find them all [@orsi2024one]</span>"
    ]
  },
  {
    "objectID": "papers/universal_chiral_fp_2024.html#conclusion",
    "href": "papers/universal_chiral_fp_2024.html#conclusion",
    "title": "MAP4C: One chiral fingerprint to find them all (Orsi and Reymond 2024)",
    "section": "Conclusion",
    "text": "Conclusion\n\nchiral version of MAP performs as good as achiral versions\nperform better in distinguishing stereoisomers than chiral ECFP and AP",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>MAP4C: One chiral fingerprint to find them all [@orsi2024one]</span>"
    ]
  },
  {
    "objectID": "papers/universal_chiral_fp_2024.html#what-i-likeddidnt-like-about-the-paper",
    "href": "papers/universal_chiral_fp_2024.html#what-i-likeddidnt-like-about-the-paper",
    "title": "MAP4C: One chiral fingerprint to find them all (Orsi and Reymond 2024)",
    "section": "What I liked/didn’t like about the paper",
    "text": "What I liked/didn’t like about the paper\n\ndidn’t like:\n\nhard to read\nTMAP not explained (supposed to mean Tree MAP)\n“scrambled sequences” not explained\nrepetition of how FP works in methods and beginning of results\nin supporting info S4 figure “c” is missing\ntypical paper from the “our tool is the best”-field, would have been nice if they included some critical statistics for e.g. bit-collisions\nno reference made to the field of machine learning\n\nlike:\n\nthey made quite an effort to test their new FP",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>MAP4C: One chiral fingerprint to find them all [@orsi2024one]</span>"
    ]
  },
  {
    "objectID": "papers/universal_chiral_fp_2024.html#references",
    "href": "papers/universal_chiral_fp_2024.html#references",
    "title": "MAP4C: One chiral fingerprint to find them all (Orsi and Reymond 2024)",
    "section": "References",
    "text": "References\n\n\n\n\nCapecchi, Alice, Daniel Probst, and Jean-Louis Reymond. 2020. “One Molecular Fingerprint to Rule Them All: Drugs, Biomolecules, and the Metabolome.” Journal of Cheminformatics 12: 1–15.\n\n\nCarhart, Raymond E, Dennis H Smith, and RENGACHARI Venkataraghavan. 1985. “Atom Pairs as Molecular Features in Structure-Activity Studies: Definition and Applications.” Journal of Chemical Information and Computer Sciences 25 (2): 64–73.\n\n\nGuo, Jeff, and Philippe Schwaller. 2024. “It Takes Two to Tango: Directly Optimizing for Constrained Synthesizability in Generative Molecular Design.” arXiv. https://doi.org/10.48550/ARXIV.2410.11527.\n\n\nLandrum, Gregory A., Jessica Braun, Paul Katzberger, Marc T. Lehner, and Sereina Riniker. 2024. “Lwreg: A Lightweight System for Chemical Registration and Data Storage.” Journal of Chemical Information and Modeling 64 (16): 6247–52. https://doi.org/10.1021/acs.jcim.4c01133.\n\n\nOrsi, Markus, and Jean-Louis Reymond. 2024. “One Chiral Fingerprint to Find Them All.” Journal of Cheminformatics 16 (1): 53.\n\n\nProbst, Daniel, and Jean-Louis Reymond. 2018. “A Probabilistic Molecular Fingerprint for Big Data Settings.” Journal of Cheminformatics 10: 1–12.\n\n\nRogers, David, and Mathew Hahn. 2010. “Extended-Connectivity Fingerprints.” Journal of Chemical Information and Modeling 50 (5): 742–54.",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>MAP4C: One chiral fingerprint to find them all [@orsi2024one]</span>"
    ]
  },
  {
    "objectID": "papers/lwreg_2024.html",
    "href": "papers/lwreg_2024.html",
    "title": "lwreg: A Lightweight System for Chemical Registration and Data Storage (Landrum et al. 2024)",
    "section": "",
    "text": "Why did I choose this paper?",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>lwreg: A Lightweight System for Chemical Registration and Data Storage [@landrum_lwreg_2024]</span>"
    ]
  },
  {
    "objectID": "papers/lwreg_2024.html#why-did-i-choose-this-paper",
    "href": "papers/lwreg_2024.html#why-did-i-choose-this-paper",
    "title": "lwreg: A Lightweight System for Chemical Registration and Data Storage (Landrum et al. 2024)",
    "section": "",
    "text": "I have been interested in structure registration/standardisation systems for quite a while now\nIn the open source realm, there are not many systems you can use, primarily the ChEMBL curation pipeline (Bento et al. 2020)\nWe had an early encounter with lwreg at IWOMI 2023 (which is also acknowledged in the paper)",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>lwreg: A Lightweight System for Chemical Registration and Data Storage [@landrum_lwreg_2024]</span>"
    ]
  },
  {
    "objectID": "papers/lwreg_2024.html#general",
    "href": "papers/lwreg_2024.html#general",
    "title": "lwreg: A Lightweight System for Chemical Registration and Data Storage (Landrum et al. 2024)",
    "section": "General",
    "text": "General\n\nAuthors:\n\nAffiliation of all authors: ETH Zurich, Department of Chemistry and Applied Biosciences, Computational Chemistry research group\nGroup website: https://riniker.ethz.ch\nGregory A. Landrum* (Senior scientist, also founder and main maintainer of RDKit open-source cheminformatics toolkit)\nJessica Braun*\nPaul Katzberger\nMarc T. Lehner\nSereina Riniker\n* G.L. and J.B. contributed equally to this work.\n\nJournal:\n\nJournal of Chemical Information and Modeling\n\nNotes:\n\nlwreg: LeightWeight chemical REGistration system\nGitHub repository: https://github.com/rinikerlab/lightweight-registration",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>lwreg: A Lightweight System for Chemical Registration and Data Storage [@landrum_lwreg_2024]</span>"
    ]
  },
  {
    "objectID": "papers/lwreg_2024.html#abstract",
    "href": "papers/lwreg_2024.html#abstract",
    "title": "lwreg: A Lightweight System for Chemical Registration and Data Storage (Landrum et al. 2024)",
    "section": "Abstract",
    "text": "Abstract\n\nlwreg: a lightweight, yet flexible chemical registration system\ncaptures 2D structures (topologies) and 3D conformers of molecules\nopen-source\nPython API (and CLI)\nextendable to also store experimental data and metadata with link to registered structures\ninstallable directly from GitHub repository: https://github.com/rinikerlab/lightweight-registration",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>lwreg: A Lightweight System for Chemical Registration and Data Storage [@landrum_lwreg_2024]</span>"
    ]
  },
  {
    "objectID": "papers/lwreg_2024.html#introduction",
    "href": "papers/lwreg_2024.html#introduction",
    "title": "lwreg: A Lightweight System for Chemical Registration and Data Storage (Landrum et al. 2024)",
    "section": "Introduction",
    "text": "Introduction\n\ndefinition of chemical registration system in this context: “a database (or other software system) that stores chemical structures together with assigned identifiers”\nminimal functionality:\n\ncheck whether a compound is already registered\nif yes, return its identifier\nif no, register the new compound\nreturn the associated structure when given an identifier\n\nless common in academia than in industry\nrare in computational groups/labs\n\ndue to cultural/social factors\ndue to technical and scientific challenges\n\nmost registration systems are GUI applications not optimised for computational workflows (e.g. no programmatic access via an API or no bulk submissions)\nbiggest scientific challenge: context dependency of “are these two compounds the same?” (conformers, stereochemistry, tautomers, charged states, counter-ions, etc.)\n\n\nuse cases for compound registration systems in computational groups:\n\ntracking compounds that were investigated and maybe shared with (experimental) collaborators\ntracking training/testing/validation sets for ML models\ntracking already executed experiments like MD simulations, QM calculations, costly descriptor calculations, etc.\n\nlwreg\n\nopen-source, highly flexible compound registration system\nallows different definitions of chemical identity\nsupports integration of experimental results\nfocus on computational use cases\naccessible via Python API or CLI\nno GUI",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>lwreg: A Lightweight System for Chemical Registration and Data Storage [@landrum_lwreg_2024]</span>"
    ]
  },
  {
    "objectID": "papers/lwreg_2024.html#methods",
    "href": "papers/lwreg_2024.html#methods",
    "title": "lwreg: A Lightweight System for Chemical Registration and Data Storage (Landrum et al. 2024)",
    "section": "Methods",
    "text": "Methods\n\nImplementation and Interface\n\npure Python\nlimited number of external dependencies other than the RDKit\ninstallation via conda with the supplied environment.yml file\nprimary operations:\n\nregister: register a new compound or a bulk of compounds via bulk_register; generate registration ID, called molregno\nquery: determine whether a given structure is already registered and if yes, return its molregno; configurable to query all stereochem variants / tautomers / etc.\nretrieve: give a molregno or multiple and retrieve associated structure; optional: retrieve structure prestandardisation or its hashes\n\n\nFiltering and Standardizing Compounds\n\ncommonly, structure registration systems perform standardisation and filtering at registration\n\nstandardisation: e.g. remove counter-ions and solvents, transform functional groups into standard forms, etc.\nfiltering: e.g. flag/reject structures with overlapping atoms, certain elements or functional groups, unspecified or incorrect stereochem, etc.\n\nlwreg\n\nsome built-in standardisation options, e.g.\n\nRDKit sanitization (https://www.rdkit.org/docs/RDKit_Book.html#molecular-sanitization)\nfragment removal (counter-ions, solvents)\nneutralization\n\nsome built-in filtering options, e.g.\n\ncheck for overlapping atoms\ncheck for polymer definitions and other annotations (SGroups)\n\nflexible and configurable interface for adding custom standardisation/filtering routines, e.g. (taken from standardization_lib.py)\n\n\nclass RDKitSanitize(Standardization):\nname = \"rdkit_sanitize\"\nexplanation = \"runs the standard RDKit sanitization on the molecule\"\n\ndef __call__(self, mol):\n    try:\n        Chem.SanitizeMol(mol)\n    except:\n        return None\n    return mol\n\n\n\nRecognizing Duplicate Compounds\n\neach molecule in the database is assigned a registration hash\nto check whether a structure is already registered, its hash is generated and checked against the database structure hashes using string matching\n\nRegistration Hash\n\nlwreg uses RDKit’s RegistrationHash function to generate its registration hash codes (RDKit UGM 2022 presentation slides)\ngenerated hash has multiple layers to be able to assess molecular similarity on different levels, assessing different structural traits\nlayers:\n\n\n\n\nFORMULA: molecular formula\nCANONICAL_SMILES: canonical CXSMILES with stereochemistry information (including support for “enhanced stereochemistry”, see slides linked above, e.g. definition of enantiomers/racemates)\nTAUTOMER_HASH: tautomer-agnostic structure line notation (HetAtomTautomer hash) with stereochemistry information, developed as part of the MolHash software by NextMove Software (GitHub repo, ACS Fall 2019 presentation)\nNO_STEREO_SMILES: canonical CXSMILES without stereochemistry information\nNO_STEREO_TAUTOMER_HASH: HetAtomTautomer hash without stereochemistry information\nSGROUP_DATA: canonicalised form of some of the molecule’s SGroup data (if any, configurable)\nESCAPE: free-text field (basically allows you to use your own registration hash or add custom information you consider relevant)\n\n\nlayers 1-5 generated using RDKit’s implementation of MolHash\nfinal registration hash generated by computing an SHA1 hash from all of these layers via Python’s hashlib library\n\n\n\n\n\nWhy Not InChI?\n\nalso provides a multi-layer hash that could be used for identity determination\nbut(!) InChI standardises input structures (e.g. to standard tautomers)\nthis way, more control over standardisation and hashing details\n\nQuerying Using Hash Layers\n\ndefault query behaviour of lwreg: standardise query molecule, generate its default registration hash, use it to query the table of registration hashes, return molregno of matching record or none\ncan be specified which layer to use, to e.g. query for all stereoisomers, tautomers, etc.\n\nRetrieving Structures\n\nStructures are retrieved by giving molregnos\nStructures are returned as v3000 MOL blocks\npossible to retrieve standardised structure or “as registered” form of the structure\npossible to retrieve all mol hashes\n\nStoring Conformers\n\nspecial “registerConformers” mode can be set in the system\nconformers of an existing record/structure are stored under its molregno and are each assigned a conf_id related to that molregno (so only fully defined by tuple (molregno, conf_id))\nconformers are also hashed using a simple scheme that arranges the atom coordinates in a defined order, concatenates them, and hashes the result (details in paper)\nthis scheme is independent of the atom ordering but neither translationally nor rotationally invariant (must be standardised at registration if necessary)\n\nStoring Experimental Data\n\nSQL-based database managed by lwreg can be extended with additional tables to store experimental data with relations to the registered molecules\nSee Jupyter Notebook demos in the GitHub repository\ngeneral recommendation would be to store experimental data/results and metadata in separate tables, with UUIDs to identify experiments\n\nAdding Chemical Search: Integration With the RDKit Cartridges\n\nNo similarity or substructure search functions in lwreg\nWould be possible to add externally via the RDKit PostgreSQL cartridge or the chemicalite cartridge for sqlite",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>lwreg: A Lightweight System for Chemical Registration and Data Storage [@landrum_lwreg_2024]</span>"
    ]
  },
  {
    "objectID": "papers/lwreg_2024.html#results-and-discussion",
    "href": "papers/lwreg_2024.html#results-and-discussion",
    "title": "lwreg: A Lightweight System for Chemical Registration and Data Storage (Landrum et al. 2024)",
    "section": "Results and Discussion",
    "text": "Results and Discussion\n\nperformed several benchmarks (performance snapshots)\nexecuted lwreg commands on a two-year old workstation (3.8 GHz Intel Xeon W-1270P CPU, 64 GB RAM, Ubuntu 22.04)\ndatabase was a PostgresSQL v14 installation running on an eight-year-old workstation (3.5 GHz Intel Xeon E3−1240 CPU, 32 GB RAM, Ubuntu 22.04.)\nRegistering All Compounds From ChEMBL33\n\nRDKit sanitization for standardisation\n2.3 mio molecules in final database\ntotal runtime of registration: 17.5 h (26 ms per molecule avg.)\n7.4 GB of disk space required (1 GB molecule hashes, 3 GB original mol blocks, 3.4 GB for mol blocks table)\nmedian query time of the database was 5.6 ms (maximum 274 ms)\n\nStoring DASH Tree Conformers\n\ntook the data from (Lehner et al. 2023), 365,419 molecules and 1,027,559 conformers, and built a database with lwreg\nconformer table occupied 3.5 GB of disk space\nmolecule registration data only 1.3 GB\nregistering and retrieving conformers took about the same time as for the ChEMBL structures (more details in paper)\n\nStoring Molecular Data Sets for Bioactivity Prediction and Associated Machine-Learning Results\n\ndetails in the Jupyter Notebook tutorials",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>lwreg: A Lightweight System for Chemical Registration and Data Storage [@landrum_lwreg_2024]</span>"
    ]
  },
  {
    "objectID": "papers/lwreg_2024.html#conclusions",
    "href": "papers/lwreg_2024.html#conclusions",
    "title": "lwreg: A Lightweight System for Chemical Registration and Data Storage (Landrum et al. 2024)",
    "section": "Conclusions",
    "text": "Conclusions\n\nsummary of the paper, highlighting the layered hash, the good performance, and the extendability of the underlying database\nlwreg does not have a GUI but can be a solid (backend) for one in the future\ninvite the community to the issues and discussions sections of the GitHub repository",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>lwreg: A Lightweight System for Chemical Registration and Data Storage [@landrum_lwreg_2024]</span>"
    ]
  },
  {
    "objectID": "papers/lwreg_2024.html#personal-comments-questions-to-discuss",
    "href": "papers/lwreg_2024.html#personal-comments-questions-to-discuss",
    "title": "lwreg: A Lightweight System for Chemical Registration and Data Storage (Landrum et al. 2024)",
    "section": "Personal comments / questions to discuss",
    "text": "Personal comments / questions to discuss\n\nis there a technical reason for why there is no pypi package or sth similar? Installation currently only possible from the GitHub repo\nis there a certain order to the registration hash layers? The FORMULA layer is very unspecified while the CANONICAL_SMILES layer is very defined",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>lwreg: A Lightweight System for Chemical Registration and Data Storage [@landrum_lwreg_2024]</span>"
    ]
  },
  {
    "objectID": "papers/lwreg_2024.html#references",
    "href": "papers/lwreg_2024.html#references",
    "title": "lwreg: A Lightweight System for Chemical Registration and Data Storage (Landrum et al. 2024)",
    "section": "References",
    "text": "References\n\n\n\n\nBento, A. Patrícia, Anne Hersey, Eloy Félix, Greg Landrum, Anna Gaulton, Francis Atkinson, Louisa J. Bellis, Marleen De Veij, and Andrew R. Leach. 2020. “An Open Source Chemical Structure Curation Pipeline Using RDKit.” Journal of Cheminformatics 12 (1): 51. https://doi.org/10.1186/s13321-020-00456-1.\n\n\nGuo, Jeff, and Philippe Schwaller. 2024. “It Takes Two to Tango: Directly Optimizing for Constrained Synthesizability in Generative Molecular Design.” arXiv. https://doi.org/10.48550/ARXIV.2410.11527.\n\n\nLandrum, Gregory A., Jessica Braun, Paul Katzberger, Marc T. Lehner, and Sereina Riniker. 2024. “Lwreg: A Lightweight System for Chemical Registration and Data Storage.” Journal of Chemical Information and Modeling 64 (16): 6247–52. https://doi.org/10.1021/acs.jcim.4c01133.\n\n\nLehner, Marc T., Paul Katzberger, Niels Maeder, Carl C. G. Schiebroek, Jakob Teetz, Gregory A. Landrum, and Sereina Riniker. 2023. “DASH: Dynamic Attention-Based Substructure Hierarchy for Partial Charge Assignment.” Journal of Chemical Information and Modeling 63 (19): 6014–28. https://doi.org/10.1021/acs.jcim.3c00800.\n\n\nOrsi, Markus, and Jean-Louis Reymond. 2024. “One Chiral Fingerprint to Find Them All.” Journal of Cheminformatics 16 (1): 53.",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>lwreg: A Lightweight System for Chemical Registration and Data Storage [@landrum_lwreg_2024]</span>"
    ]
  },
  {
    "objectID": "papers/tango.html",
    "href": "papers/tango.html",
    "title": "It Takes Two to Tango: Directly Optimizing for Constrained Synthesizability in Generative Molecular Design (Guo and Schwaller 2024)",
    "section": "",
    "text": "Why did I choose this paper?",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>It Takes Two to Tango: Directly Optimizing for Constrained Synthesizability in Generative Molecular Design [@https://doi.org/10.48550/arxiv.2410.11527]</span>"
    ]
  },
  {
    "objectID": "papers/tango.html#why-did-i-choose-this-paper",
    "href": "papers/tango.html#why-did-i-choose-this-paper",
    "title": "It Takes Two to Tango: Directly Optimizing for Constrained Synthesizability in Generative Molecular Design (Guo and Schwaller 2024)",
    "section": "",
    "text": "From a chemist point of view prediction of synthesis paths is really relevant.\nFrom a ML perspective they are still a lot of challenges.",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>It Takes Two to Tango: Directly Optimizing for Constrained Synthesizability in Generative Molecular Design [@https://doi.org/10.48550/arxiv.2410.11527]</span>"
    ]
  },
  {
    "objectID": "papers/tango.html#context",
    "href": "papers/tango.html#context",
    "title": "It Takes Two to Tango: Directly Optimizing for Constrained Synthesizability in Generative Molecular Design (Guo and Schwaller 2024)",
    "section": "Context",
    "text": "Context\nPredicting synthesis path of complex organic molecules whit multiple intermediates is challenging. This task gets even more complex if the products are constrained to e.g. commercially available products.",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>It Takes Two to Tango: Directly Optimizing for Constrained Synthesizability in Generative Molecular Design [@https://doi.org/10.48550/arxiv.2410.11527]</span>"
    ]
  },
  {
    "objectID": "papers/tango.html#prior-work",
    "href": "papers/tango.html#prior-work",
    "title": "It Takes Two to Tango: Directly Optimizing for Constrained Synthesizability in Generative Molecular Design (Guo and Schwaller 2024)",
    "section": "Prior work",
    "text": "Prior work\nSynthesizability constrained molecular generation: Enforcing valid chemical transformations\n\n“SynNet Gao et al. (https://doi.org/10.48550/arXiv.2410.03494?):\n\nUses a Markov decision process to generate molecules and synthesis pathways simultaneously through a bottom-up tree model\nFocuses primarily on synthesis tree generation without a broad multi-parameter optimization (MPO) approach\n\nSynFormer Gao et al. (Gao, Luo, and Coley 2024):\n\ngenerative framework that uses a scalable transformer architecture and a diffusion model to navigate synthesizable chemical space\ngenerates synthetic pathways by linking commercially available building blocks through known reactions\nfocuses on generating pathways within a defined synthesizable space",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>It Takes Two to Tango: Directly Optimizing for Constrained Synthesizability in Generative Molecular Design [@https://doi.org/10.48550/arxiv.2410.11527]</span>"
    ]
  },
  {
    "objectID": "papers/tango.html#main-contributions",
    "href": "papers/tango.html#main-contributions",
    "title": "It Takes Two to Tango: Directly Optimizing for Constrained Synthesizability in Generative Molecular Design (Guo and Schwaller 2024)",
    "section": "Main contributions",
    "text": "Main contributions\n\ngeneral-purpose molecular generative model that generates synthesize molecules\nTANimoto Group Overlap (TANGO) reward function for the generation of synthesizable molecules with the presence of enforced building blocks\ngenerated molecules can satisfy multi-parameter-optimization (MPO) objects relevant to drug discovery",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>It Takes Two to Tango: Directly Optimizing for Constrained Synthesizability in Generative Molecular Design [@https://doi.org/10.48550/arxiv.2410.11527]</span>"
    ]
  },
  {
    "objectID": "papers/tango.html#structure-of-the-overall-pipeline",
    "href": "papers/tango.html#structure-of-the-overall-pipeline",
    "title": "It Takes Two to Tango: Directly Optimizing for Constrained Synthesizability in Generative Molecular Design (Guo and Schwaller 2024)",
    "section": "Structure of the overall pipeline",
    "text": "Structure of the overall pipeline\n\n\n\nOverview\n\n\n\nSaturn model as molecular generative model:\n\nbased on Mamba a lightweight, autoregressive model for the sequential prediction of SMILES\npretrained on PubChem\n\nFine-tuning with a reinforcement-learning-setup:\n\nTANGO reward function to measure for similarity of generated molecules and enforced building blocks\nSyntheseus as retro-synthesis tool to check if molecules are synthesizable\nMPO for docking-score (biological effectiveness) and QED score (Quantitative Estimate of Drug-likeness)",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>It Takes Two to Tango: Directly Optimizing for Constrained Synthesizability in Generative Molecular Design [@https://doi.org/10.48550/arxiv.2410.11527]</span>"
    ]
  },
  {
    "objectID": "papers/tango.html#problem-formulation",
    "href": "papers/tango.html#problem-formulation",
    "title": "It Takes Two to Tango: Directly Optimizing for Constrained Synthesizability in Generative Molecular Design (Guo and Schwaller 2024)",
    "section": "Problem formulation",
    "text": "Problem formulation\n\nCase 1: Starting material constrained\nA synthesis graph G(M, R) is starting-material constrained if there exists at least one leaf node \\[ m \\in G(M, R) \\] that meets both conditions: \\[ m = b \\in B_{enf} \\] \\[ \\text{depth}(m) = \\max \\text{depth} \\]\n\\[\n\\exists m \\in G(M, R) \\; \\text{such that} \\; \\text{depth}(m) = \\max \\; \\text{depth and} \\; m = b \\in B_{enf}\n\\]\n\nrequires at least one terminal node (leaf) in the synthesis route to correspond to a specified starting material\nuseful for ensuring the synthesis begins with commercially available or cost-effective starting reagents\n\n\n\n\nCase 2: Intermediate Constrained Synthesis\nMathematical Definition:\nA synthesis graph G(M, R) is intermediate constrained if there exists at least one intermediate node \\[ m \\in G(M, R) \\] that belongs to the enforced building blocks \\[ B_{enf} \\].\n\\[\n\\exists m \\in G(M, R) \\; \\text{such that} \\; m \\in B_{enf}\n\\]\n\nallows for one or more intermediate steps in the synthesis to include specified building blocks\nenables designing synthetic pathways that feature important building blocks at intermediate stages\n\n\n\n\nCase 3: Divergent Synthesis\nA synthesis graph G(M, R) is divergent if there exists at least one intermediate node \\[ m \\in G(M, R) \\] that meets the following conditions: \\[ m = b \\in B_{enf} \\] All \\[ b \\in B_{enf} \\] are non-commercial.\n\\[\n\\exists m \\in G(M, R) \\; \\text{such that} \\; \\forall b \\in B_{enf}, \\; b \\; \\text{is non-commercial and} \\; m = b \\in B_{enf}\n\\]\n\nrequires that an intermediate node in the synthesis graph matches a non-commercial building block, allowing it to diverge into multiple branches\nsuitable for late-stage functionalization, often required to generate diverse molecular structures in drug discovery",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>It Takes Two to Tango: Directly Optimizing for Constrained Synthesizability in Generative Molecular Design [@https://doi.org/10.48550/arxiv.2410.11527]</span>"
    ]
  },
  {
    "objectID": "papers/tango.html#tango-reward-function",
    "href": "papers/tango.html#tango-reward-function",
    "title": "It Takes Two to Tango: Directly Optimizing for Constrained Synthesizability in Generative Molecular Design (Guo and Schwaller 2024)",
    "section": "TANGO Reward Function",
    "text": "TANGO Reward Function\n\nTanimoto Similarity:\n\nMetric used to quantify the similarity between two molecular structures.\nCalculated based on the overlap of molecular fingerprints, with values ranging from 0 (no similarity) to 1 (perfect similarity).\n\nFuzzy Matching Substructure (FMS):\n\nMeasures the maximum overlap between substructures of a generated molecule.\nTakes into account atom types, hybridization, and bonding patterns.\nDesigned to capture functional groups and conserved structures.\n\n\nThe TANGO reward for a molecule m in a synthesis graph G(M, R) is calculated as the weighted sum of the Tanimoto similarity (TanSim) and Fuzzy Matching Substructure (FMS) similarity to the enforced building blocks \\[ B_{enf} \\].\n\\[\n\\text{TANGO}(m, B_{enf}) = \\left( \\max(\\text{TanSim}(m, B_{enf})) \\times 0.5 \\right) + \\left( \\max(\\text{FMS}(m, B_{enf})) \\times 0.5 \\right) \\in [0, 1]\n\\]\n\nCalculates the reward based on the maximum similarity between each non-root node in the synthesis graph and the enforced building blocks.\nThe reward is normalized within the range [0, 1] and helps the model learn to generate molecules closer to the desired structural and functional goals.\n\n\n\n\nTANGO reward function",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>It Takes Two to Tango: Directly Optimizing for Constrained Synthesizability in Generative Molecular Design [@https://doi.org/10.48550/arxiv.2410.11527]</span>"
    ]
  },
  {
    "objectID": "papers/tango.html#experimental-setting",
    "href": "papers/tango.html#experimental-setting",
    "title": "It Takes Two to Tango: Directly Optimizing for Constrained Synthesizability in Generative Molecular Design (Guo and Schwaller 2024)",
    "section": "Experimental setting",
    "text": "Experimental setting\n\nMolecular Generative Model: used model: Saturn (autoregressive SMILES-based model leveraging the Mamba architecture)\nRetrosynthesis Model: Syntheseus (framework supporting various retrosynthesis algorithms) with MEGAN as the single-step retrosynthesis model, Retro* as search algorithm\nSet of Commercial Building Blocks: includes fragments and reactive compounds from the ZINC database\nDrug Discovery Case Study: focuses on generating molecules with optimal docking scores against ClpP (cancer-related protease target), high QED (drug-likeness) values\nExperimental Details: oracle budget (maximum number of evaluations of a molecule) of 10,000 steps and evaluation across multiple seeds\nMetrics:\n\nNon-solved: count of generated molecules that the retrosynthesis model deems unsynthesizable\nSolved (Enforced): count of generated molecules that the retrosynthesis model deems synthesizable and include at least one enforced building block\nN (Replicates): number of successful replicates (out of 10 seeds) in which at least one Solved (Enforced) molecule is generated\nUnique Enforced Blocks: average and standard deviation of unique enforced building blocks in the synthesis routes for Solved (Enforced) molecules\nReaction Steps: average and standard deviation of the number of reaction steps in synthesis routes for Solved (Enforced) molecules\nDocking Score and QED",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>It Takes Two to Tango: Directly Optimizing for Constrained Synthesizability in Generative Molecular Design [@https://doi.org/10.48550/arxiv.2410.11527]</span>"
    ]
  },
  {
    "objectID": "papers/tango.html#results",
    "href": "papers/tango.html#results",
    "title": "It Takes Two to Tango: Directly Optimizing for Constrained Synthesizability in Generative Molecular Design (Guo and Schwaller 2024)",
    "section": "Results",
    "text": "Results\n\nSynthesizibility constrained molecules generation\n\ndifferent configurations: with 10 and 100 enforced building blocks, diffenrent constrains (SM, divergent and unconstrained)\nMetrics:\n\nSynthesisablity: Non-solved and Solved (enforced)\nmean value for differnet docking score intervals (lower number are better)\nnumber of molecules in the docking score interval M\nmean QED values for every docking score interval (higher values are better)\nmean number of reactions steps\nmean number of enforced building blocks used in synthesis routs of synthesizible molecules\n\n\n\n\n\nResults\n\n\n\ncomparison to other models because of new metrics hard and not shown by the authors\nTANGO reward function archives highest scores compared to only Tanimoto similarity or Fuzzy matching substructures\nequal weighting (0.5) of Tanimoto similarity and FMS in the TANGO reward function shows the best performance\nmore consistent and stable outcomes across different seeds compared to other reward configurations\n\n\n\nLearning a desirable distribution\nComparison of pre-trained model to final checkpoint of fine-tuned model (mean over 10 seeds over 1000 randomly sampled molecules)\n\nDistribution Shift with TANGO: enables a shift in the model’s distribution towards generating molecules that meet MPO objectives\n\nsignificant increase in Solved (Enforced) molecules, indicating the model’s successful learning of constrained synthesizability\ndistribution of docking scores and QED values showed a shift towards favorable ranges\nmodel learned to focus on a narrower, more optimized chemical space\n\nExploitation Advantage: model benefits from exploiting certain favorable enforced building blocks which promotes local exploration, allowing the generation of structurally related but optimized molecules\n\n\n\n\nComparison of pre-trained and fine-tuned model",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>It Takes Two to Tango: Directly Optimizing for Constrained Synthesizability in Generative Molecular Design [@https://doi.org/10.48550/arxiv.2410.11527]</span>"
    ]
  },
  {
    "objectID": "papers/tango.html#conclusion",
    "href": "papers/tango.html#conclusion",
    "title": "It Takes Two to Tango: Directly Optimizing for Constrained Synthesizability in Generative Molecular Design (Guo and Schwaller 2024)",
    "section": "Conclusion",
    "text": "Conclusion\n\nEffectiveness of TANGO: The TANGO reward function enables the Saturn model to learn constrained synthesizability effectively, balancing both synthesizability and structural similarity requirements.\nMulti-Parameter Optimization: TANGO facilitates multi-parameter optimization (MPO), allowing the generation of molecules that meet complex objectives, including optimal docking scores, high QED values, and the inclusion of enforced building blocks.\nImproved Synthesizability: Compared to previous methods, TANGO demonstrates a higher success rate in generating synthesizable molecules, particularly with constrained conditions.\nPotential in Drug Discovery: The approach provides a promising framework for generating drug-like molecules with specific requirements, potentially accelerating the early stages of drug discovery and development.",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>It Takes Two to Tango: Directly Optimizing for Constrained Synthesizability in Generative Molecular Design [@https://doi.org/10.48550/arxiv.2410.11527]</span>"
    ]
  },
  {
    "objectID": "papers/tango.html#personal-opinion",
    "href": "papers/tango.html#personal-opinion",
    "title": "It Takes Two to Tango: Directly Optimizing for Constrained Synthesizability in Generative Molecular Design (Guo and Schwaller 2024)",
    "section": "Personal opinion",
    "text": "Personal opinion\n\nshown method and functions are easy to understand\nmissing comparison with other models to prove improvement of scores with new method",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>It Takes Two to Tango: Directly Optimizing for Constrained Synthesizability in Generative Molecular Design [@https://doi.org/10.48550/arxiv.2410.11527]</span>"
    ]
  },
  {
    "objectID": "papers/tango.html#references",
    "href": "papers/tango.html#references",
    "title": "It Takes Two to Tango: Directly Optimizing for Constrained Synthesizability in Generative Molecular Design (Guo and Schwaller 2024)",
    "section": "References",
    "text": "References\n\n\n\n\nGao, Wenhao, Shitong Luo, and Connor W. Coley. 2024. “Generative Artificial Intelligence for Navigating Synthesizable Chemical Space.” arXiv. https://doi.org/10.48550/ARXIV.2410.03494.\n\n\nGuo, Jeff, and Philippe Schwaller. 2024. “It Takes Two to Tango: Directly Optimizing for Constrained Synthesizability in Generative Molecular Design.” arXiv. https://doi.org/10.48550/ARXIV.2410.11527.\n\n\nLandrum, Gregory A., Jessica Braun, Paul Katzberger, Marc T. Lehner, and Sereina Riniker. 2024. “Lwreg: A Lightweight System for Chemical Registration and Data Storage.” Journal of Chemical Information and Modeling 64 (16): 6247–52. https://doi.org/10.1021/acs.jcim.4c01133.\n\n\nOrsi, Markus, and Jean-Louis Reymond. 2024. “One Chiral Fingerprint to Find Them All.” Journal of Cheminformatics 16 (1): 53.",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>It Takes Two to Tango: Directly Optimizing for Constrained Synthesizability in Generative Molecular Design [@https://doi.org/10.48550/arxiv.2410.11527]</span>"
    ]
  },
  {
    "objectID": "papers/core_bench.html",
    "href": "papers/core_bench.html",
    "title": "CORE-Bench: Fostering the Credibility of Published Research Through a Computational Reproducibility Agent Benchmark",
    "section": "",
    "text": "Why discussing this paper?\nI chose Siegel et al.’s paper (Siegel et al. 2024) for our journal club because:",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>CORE-Bench: Fostering the Credibility of Published Research Through a Computational Reproducibility Agent Benchmark</span>"
    ]
  },
  {
    "objectID": "papers/core_bench.html#why-discussing-this-paper",
    "href": "papers/core_bench.html#why-discussing-this-paper",
    "title": "CORE-Bench: Fostering the Credibility of Published Research Through a Computational Reproducibility Agent Benchmark",
    "section": "",
    "text": "I really like what these folks are proposing about agents’ evaluation.\nI think that the evaluation of agents is the current bottleneck in the development of new agents.\nThey write some really interesting statements such as “Before agents can automate scientific research, they must be able to reproduce existing results.”.\nOne of the conclusions is really interesting for one of the projects that I am working on.",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>CORE-Bench: Fostering the Credibility of Published Research Through a Computational Reproducibility Agent Benchmark</span>"
    ]
  },
  {
    "objectID": "papers/core_bench.html#prior-work",
    "href": "papers/core_bench.html#prior-work",
    "title": "CORE-Bench: Fostering the Credibility of Published Research Through a Computational Reproducibility Agent Benchmark",
    "section": "Prior work",
    "text": "Prior work\nWith the appearance of the LLMs, agents have gained a lot of importance, especially during the last year. However, the evaluation of these agents is still a challenge, and a bunch of benchmarks and different evaluations have been proposed to evaluate them.\n\nSWE-Bench\nIn this work by Jimenez et al. (Jimenez et al. 2024), they introduced another agent benchmark. The tasks that they evaluate are based on GitHub Pull Requests extracted from some popular repositories. They evaluate the agents in solving the issues that are opened in these PRs. Nowadays, it is one of the most used benchmarks for evaluating new LLMs.\n\n\nAI Agents That Matter\nThis is previous work from the same authors of the CORE-Bench. In this work, Kappor et al. (Kapoor et al. 2024) propose different case studies, each with a different evaluation method for some of the more common NLP benchmarks. The particularity is that these evaluations are thought to evaluate more than only the agents’ final answers.\n\n\nLAB-Bench\nThis benchmark proposed by Laurent et al. (Laurent et al. 2024) is focused on evaluating agents in the scientific domain, specifically in the medical/biochemistry domain. It is multimodal, including tasks such as analyzing lab images, scientific tables, lab protocols, or retrieving data from biological databases.\n\n\n\nFigure taken from Laurent et al. paper (Laurent et al. 2024) illustrating the results of the different leading LLMs in LAB-Bench.",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>CORE-Bench: Fostering the Credibility of Published Research Through a Computational Reproducibility Agent Benchmark</span>"
    ]
  },
  {
    "objectID": "papers/core_bench.html#problem-setting",
    "href": "papers/core_bench.html#problem-setting",
    "title": "CORE-Bench: Fostering the Credibility of Published Research Through a Computational Reproducibility Agent Benchmark",
    "section": "Problem setting",
    "text": "Problem setting\n\nEvaluating LLM-powered agents is a real challenge.\nIt is possible to evaluate only the final result, but interesting conclusions can be lost in the reasoning process.\nThis reasoning evaluations is really important within the develpment of new agents since it can help to understand the agents’ limitations.\nThe current evaluation of the agents is a bottleneck in the development of new agents.",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>CORE-Bench: Fostering the Credibility of Published Research Through a Computational Reproducibility Agent Benchmark</span>"
    ]
  },
  {
    "objectID": "papers/core_bench.html#approach",
    "href": "papers/core_bench.html#approach",
    "title": "CORE-Bench: Fostering the Credibility of Published Research Through a Computational Reproducibility Agent Benchmark",
    "section": "Approach",
    "text": "Approach\nThey proposed a new benchmark that allows the agents to be evaluated more deeply. They propose to evaluate the agents in three different tasks: CORE-Bench-Easy, CORE-Bench-Medium, and CORE-Bench-Hard. The first one is a simple task, the second one is a medium task, and the last one is a hard task. The evaluation is done so that the agents have to provide the answer and the reasoning behind it.\n\n\n\nTable 22.1: Description of the different tasks according to the difficulty and the actual task for each\n\n\n\n\n\n\n\n\n\n\nTask level\nInformation provided to the agent\nAgent task\n\n\n\n\nCORE-Bench-Easy\nResults are provided\nExtract the results\n\n\nCORE-Bench-Medium\nThe dockerfile is provided\nRun the docker and extract the results\n\n\nCORE-Bench-Hard\nOnly a README is provided\nCreate and run the docker and extract the results\n\n\n\n\n\n\nThey reproduced some published works to generate the ground truth of the different tasks. To give some numerical room, they sampled by repeating each manual run three times, which was the ground truth for the 95 % prediction interval.\n\n\n\nFigure taken from by Siegel et al. paper (Siegel et al. 2024). In the Figure (a) it is detailed an example for a medium-level task. The figure (b) illustrates how the evaluation is performed.\n\n\nSimilarly to other benchmarks such as MLAgentBench (Huang et al. 2024) or MLE-BENCH (Chan et al. 2024), they provide the agent with access to different files and the agent need to read and run those files to reach the final answer.\nInterestingly, they evaluate two different agents: AutoGPT and CORE-Agent. The first one is a general agent widely used, while the second is an adapted, improved version of the first one.",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>CORE-Bench: Fostering the Credibility of Published Research Through a Computational Reproducibility Agent Benchmark</span>"
    ]
  },
  {
    "objectID": "papers/core_bench.html#results",
    "href": "papers/core_bench.html#results",
    "title": "CORE-Bench: Fostering the Credibility of Published Research Through a Computational Reproducibility Agent Benchmark",
    "section": "Results",
    "text": "Results\nThe general results of the evaluation are shown in the following table:\n\n\n\nTable 22.2: Accuracy in the most difficult partitions of the benchmark\n\n\n\n\n\nAgent\nLLM\nCORE-Bench-Medium\nCORE-Bench-Hard\n\n\n\n\nCORE-Agent\nGPT-4o\n0.57\n0.21\n\n\n\nGPT-4o-mini\n0.32\n0.16\n\n\n\n\n\n\n\n\nAutoGPT\nGPT-4o\n0.37\n0.06\n\n\n\nGPT-4o-mini\n0.02\n0.02\n\n\n\n\n\n\nThe number of models that they tested is limited. However, some important conclusions can be taken from there.\nAs expected and the most obvious is that the performance decreases as the difficulty of the task increases. Also, the bigger model shows better results than the smaller ones, and the adapted agent shows better results than the general one, specially for the smallest or weakest model.\nThey propose a case study in which they study the relation of the agents’ performance with the economical cost of the models. However, this has not much sense since the models powering the agents are not the same, and they have very different costs per tokens expended.\nThe most interesting result these days is that the models perform much better in the text than in the images. They claim that this is the case because the vision tasks are more complex, and the agents are not yet able to perform them. However, in my opinion, more ablations would be needed to clarify this point.\n\nAdditional results\n\nThe results of the Python tasks are better than those of the R tasks.\nAgents struggle to retrieve results from many files.\nBetter safety measures and guardrails are needed to prevent agents from running malicious code when commercially deploying these agents.",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>CORE-Bench: Fostering the Credibility of Published Research Through a Computational Reproducibility Agent Benchmark</span>"
    ]
  },
  {
    "objectID": "papers/core_bench.html#takeaways",
    "href": "papers/core_bench.html#takeaways",
    "title": "CORE-Bench: Fostering the Credibility of Published Research Through a Computational Reproducibility Agent Benchmark",
    "section": "Takeaways",
    "text": "Takeaways\n\nA simple adaptation of the agents can really improve the evaluation results.\nModels perform much better in the text than in the images.\nDespite they present this benchmark as a way to evaluate the agents, and especially if the agents can reproduce the results of the published works, I do not think that it is yet possible to use an agent to check the code reproduction of a paper.",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>CORE-Bench: Fostering the Credibility of Published Research Through a Computational Reproducibility Agent Benchmark</span>"
    ]
  },
  {
    "objectID": "papers/core_bench.html#references",
    "href": "papers/core_bench.html#references",
    "title": "CORE-Bench: Fostering the Credibility of Published Research Through a Computational Reproducibility Agent Benchmark",
    "section": "References",
    "text": "References\n\n\n\n\nChan, Jun Shern, Neil Chowdhury, Oliver Jaffe, James Aung, Dane Sherburn, Evan Mays, Giulio Starace, et al. 2024. “MLE-Bench: Evaluating Machine Learning Agents on Machine Learning Engineering.” https://arxiv.org/abs/2410.07095.\n\n\nGuo, Jeff, and Philippe Schwaller. 2024. “It Takes Two to Tango: Directly Optimizing for Constrained Synthesizability in Generative Molecular Design.” arXiv. https://doi.org/10.48550/ARXIV.2410.11527.\n\n\nHuang, Qian, Jian Vora, Percy Liang, and Jure Leskovec. 2024. “MLAgentBench: Evaluating Language Agents on Machine Learning Experimentation.” https://arxiv.org/abs/2310.03302.\n\n\nJimenez, Carlos E., John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. 2024. “SWE-Bench: Can Language Models Resolve Real-World GitHub Issues?” https://arxiv.org/abs/2310.06770.\n\n\nKapoor, Sayash, Benedikt Stroebl, Zachary S. Siegel, Nitya Nadgir, and Arvind Narayanan. 2024. “AI Agents That Matter.” https://arxiv.org/abs/2407.01502.\n\n\nLandrum, Gregory A., Jessica Braun, Paul Katzberger, Marc T. Lehner, and Sereina Riniker. 2024. “Lwreg: A Lightweight System for Chemical Registration and Data Storage.” Journal of Chemical Information and Modeling 64 (16): 6247–52. https://doi.org/10.1021/acs.jcim.4c01133.\n\n\nLaurent, Jon M., Joseph D. Janizek, Michael Ruzo, Michaela M. Hinks, Michael J. Hammerling, Siddharth Narayanan, Manvitha Ponnapati, Andrew D. White, and Samuel G. Rodriques. 2024. “LAB-Bench: Measuring Capabilities of Language Models for Biology Research.” https://arxiv.org/abs/2407.10362.\n\n\nOrsi, Markus, and Jean-Louis Reymond. 2024. “One Chiral Fingerprint to Find Them All.” Journal of Cheminformatics 16 (1): 53.\n\n\nSiegel, Zachary S., Sayash Kapoor, Nitya Nagdir, Benedikt Stroebl, and Arvind Narayanan. 2024. “CORE-Bench: Fostering the Credibility of Published Research Through a Computational Reproducibility Agent Benchmark.” https://arxiv.org/abs/2409.11363.",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>CORE-Bench: Fostering the Credibility of Published Research Through a Computational Reproducibility Agent Benchmark</span>"
    ]
  },
  {
    "objectID": "papers/bpm_L.html",
    "href": "papers/bpm_L.html",
    "title": "A Bond-Based Machine Learning Model for Molecular Polarizabilities and A Priori Raman Spectra",
    "section": "",
    "text": "Why I chose this paper",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>A Bond-Based Machine Learning Model for Molecular Polarizabilities and A Priori Raman Spectra</span>"
    ]
  },
  {
    "objectID": "papers/bpm_L.html#why-i-chose-this-paper",
    "href": "papers/bpm_L.html#why-i-chose-this-paper",
    "title": "A Bond-Based Machine Learning Model for Molecular Polarizabilities and A Priori Raman Spectra",
    "section": "",
    "text": "Reconstructing IR/Raman spectra is interesting to me\nApplications of ML to electronic structure theory (outside of potential) is cool",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>A Bond-Based Machine Learning Model for Molecular Polarizabilities and A Priori Raman Spectra</span>"
    ]
  },
  {
    "objectID": "papers/bpm_L.html#introduction",
    "href": "papers/bpm_L.html#introduction",
    "title": "A Bond-Based Machine Learning Model for Molecular Polarizabilities and A Priori Raman Spectra",
    "section": "Introduction",
    "text": "Introduction\n\nMachine learning force fields (ML FF) have accelerated the field of molecular simulations – specifically for systems where there is not an established force field\nML FFs are significantly more cost-efficient\nLearning molecular properties like dipole moment (IR) and polarizability (Raman) can help in interpreting spectra signals and benchmarking ML accuracy against experiment\nNeural network algorithms v.s. kernel algorithms\n\nNN: better performance + lower cost for larger systems\nkernel: requires less data + high cost for large systems",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>A Bond-Based Machine Learning Model for Molecular Polarizabilities and A Priori Raman Spectra</span>"
    ]
  },
  {
    "objectID": "papers/bpm_L.html#existing-work-on-ml-models-for-electric-polarizability",
    "href": "papers/bpm_L.html#existing-work-on-ml-models-for-electric-polarizability",
    "title": "A Bond-Based Machine Learning Model for Molecular Polarizabilities and A Priori Raman Spectra",
    "section": "Existing Work on ML Models for Electric Polarizability",
    "text": "Existing Work on ML Models for Electric Polarizability\n\nEquivariant neural networks, response formalism, Applequists’s dipole interaction model\nTwo kernel-based methods\n\nalign structures in training data and treat tensor components as scalars\n\nrequires lots of data\n\nsymmetry-adapted Gaussian regression\n\ngeneralization of scalar kernel ridge regression (KRR)\n\n\n\n\nGoal of this work\n\nKRR on bond polarizability model (BPM)\n\nmolecular polarizability is a sum over bond contributions",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>A Bond-Based Machine Learning Model for Molecular Polarizabilities and A Priori Raman Spectra</span>"
    ]
  },
  {
    "objectID": "papers/bpm_L.html#theory",
    "href": "papers/bpm_L.html#theory",
    "title": "A Bond-Based Machine Learning Model for Molecular Polarizabilities and A Priori Raman Spectra",
    "section": "Theory",
    "text": "Theory\n\nBond Polarizability Model\n\ntotal molecular polarizability, \\(\\alpha\\) is the sum of bond polarizabilities\n\n\\[\n\\alpha = \\sum_b{\\alpha^b}\n\\]\n\nelements of individual bond polarizability tensors\n\n\\[\n\\alpha_{ij}^{b} = \\frac{1}{3}(2\\alpha_p^b + \\alpha_l^b)\\delta_{ij} + (\\alpha_l^b + \\alpha_p^b)(\\hat{R}_i^b\\hat{R}_j^b - \\frac{1}{3}\\delta_{ij})\n\\]\n\nassumes bonds are cylindrically symmetric and typically assumes total polarizability only depends on bond length",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>A Bond-Based Machine Learning Model for Molecular Polarizabilities and A Priori Raman Spectra</span>"
    ]
  },
  {
    "objectID": "papers/bpm_L.html#ml-model",
    "href": "papers/bpm_L.html#ml-model",
    "title": "A Bond-Based Machine Learning Model for Molecular Polarizabilities and A Priori Raman Spectra",
    "section": "ML Model",
    "text": "ML Model\n\nSeparate polarizability tensor into isotropic and anisotropic components so the ML task is to infer these\n\n\\[\n\\alpha = \\alpha_{\\text{iso}}\\bf{1}+\\beta\n\\]\n\nRewrite elements of tensor in terms of components\n\n\\[\n\\alpha_{ij}^{b} = \\alpha_{\\text{iso}}^b\\delta_{ij} + \\beta^b(\\hat{R}_i^b\\hat{R}_j^b - \\frac{1}{3}\\delta_{ij})\n\\]\n\nKRR used to evaluate isotropic component, summed over bonds instead of atoms\n\n\\[\n\\alpha_{\\text{iso}} = \\sum_b{\\alpha_{\\text{iso}}^b} = \\sum_n{\\sum{K(\\textbf{q}^b},\\textbf{q}^{b'})w_n}\n\\]\n\nUsing a Gaussian kernel\n\n\\[\nK(\\textbf{q}^b,\\textbf{q}^{b'}) = \\text{exp}(-\\gamma||\\textbf{q}^b-\\textbf{q}^{b'}||^2)\n\\]\n\nThe same can be done for anisotropic component\n\n\\[\n\\beta_{ij} = \\sum_b{\\beta^bQ_{ij}^b} = \\sum_n{\\sum_{b,b'}{K(\\textbf{q}^b},\\textbf{q}^{b'})Q_{ij}^bv_n}\n\\]\n\nloss function\n\n\\[\n\\mathcal{L} = \\frac{1}{2}\\sum_{i,j}{||\\beta_{ij} - \\textbf{K}_{ij}\\textbf{v}||^2}\n\\]",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>A Bond-Based Machine Learning Model for Molecular Polarizabilities and A Priori Raman Spectra</span>"
    ]
  },
  {
    "objectID": "papers/bpm_L.html#raman-spectra",
    "href": "papers/bpm_L.html#raman-spectra",
    "title": "A Bond-Based Machine Learning Model for Molecular Polarizabilities and A Priori Raman Spectra",
    "section": "Raman Spectra",
    "text": "Raman Spectra\n\nCalculating the anharmonic IR and Raman spectra\n\n\\[\nI_{\\text{iso}}(\\omega) \\propto v(\\omega) \\int{dt \\ e^{i\\omega t}\\langle\\dot{\\alpha}_{\\text{iso}}(\\tau)\\dot{\\alpha}_{\\text{iso}}(t-\\tau)}\\rangle_\\tau\n\\] \\[\nI_{\\text{aniso}}(\\omega) \\propto v(\\omega) \\int{dt \\ e^{i\\omega t}\\langle Tr[\\dot{\\beta}_{\\text{iso}}(\\tau)\\dot{\\beta}_{\\text{iso}}(t-\\tau)}]\\rangle_\\tau\n\\]",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>A Bond-Based Machine Learning Model for Molecular Polarizabilities and A Priori Raman Spectra</span>"
    ]
  },
  {
    "objectID": "papers/bpm_L.html#biphenyl",
    "href": "papers/bpm_L.html#biphenyl",
    "title": "A Bond-Based Machine Learning Model for Molecular Polarizabilities and A Priori Raman Spectra",
    "section": "Biphenyl",
    "text": "Biphenyl\n\n\n\nbiphenyl",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>A Bond-Based Machine Learning Model for Molecular Polarizabilities and A Priori Raman Spectra</span>"
    ]
  },
  {
    "objectID": "papers/bpm_L.html#raman-spectra-evaluation",
    "href": "papers/bpm_L.html#raman-spectra-evaluation",
    "title": "A Bond-Based Machine Learning Model for Molecular Polarizabilities and A Priori Raman Spectra",
    "section": "Raman spectra evaluation",
    "text": "Raman spectra evaluation\n\n\n\nspectra",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>A Bond-Based Machine Learning Model for Molecular Polarizabilities and A Priori Raman Spectra</span>"
    ]
  },
  {
    "objectID": "papers/bpm_L.html#malonaldehyde",
    "href": "papers/bpm_L.html#malonaldehyde",
    "title": "A Bond-Based Machine Learning Model for Molecular Polarizabilities and A Priori Raman Spectra",
    "section": "Malonaldehyde",
    "text": "Malonaldehyde\n\nketo and enol forms",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>A Bond-Based Machine Learning Model for Molecular Polarizabilities and A Priori Raman Spectra</span>"
    ]
  },
  {
    "objectID": "papers/bpm_L.html#future-directions",
    "href": "papers/bpm_L.html#future-directions",
    "title": "A Bond-Based Machine Learning Model for Molecular Polarizabilities and A Priori Raman Spectra",
    "section": "Future Directions",
    "text": "Future Directions\n\nDeep neural network implementation of BPM\nConsider all bonds within a cutoff region\n\n\n\n\n\nGuo, Jeff, and Philippe Schwaller. 2024. “It Takes Two to Tango: Directly Optimizing for Constrained Synthesizability in Generative Molecular Design.” arXiv. https://doi.org/10.48550/ARXIV.2410.11527.\n\n\nLandrum, Gregory A., Jessica Braun, Paul Katzberger, Marc T. Lehner, and Sereina Riniker. 2024. “Lwreg: A Lightweight System for Chemical Registration and Data Storage.” Journal of Chemical Information and Modeling 64 (16): 6247–52. https://doi.org/10.1021/acs.jcim.4c01133.\n\n\nOrsi, Markus, and Jean-Louis Reymond. 2024. “One Chiral Fingerprint to Find Them All.” Journal of Cheminformatics 16 (1): 53.",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>A Bond-Based Machine Learning Model for Molecular Polarizabilities and A Priori Raman Spectra</span>"
    ]
  },
  {
    "objectID": "papers/coconut.html",
    "href": "papers/coconut.html",
    "title": "Training Large Language Models to Reason in a Continuous Latent Space",
    "section": "",
    "text": "Making LLMs better at reasoning\nA lot of research currently goes into making LLMs better at reasoning. Much of this is linked to the fact that current systems “think” with the “same intensity” for every token they produce. Many think that systems would be better if models could “think harder” for harder tasks.",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Training Large Language Models to Reason in a Continuous Latent Space</span>"
    ]
  },
  {
    "objectID": "papers/coconut.html#making-llms-better-at-reasoning",
    "href": "papers/coconut.html#making-llms-better-at-reasoning",
    "title": "Training Large Language Models to Reason in a Continuous Latent Space",
    "section": "",
    "text": "Ilya Sutskever at NeurIPS 2024. Reasoning is very prominent on many research agendas. Full video on YouTube\n\n\n\nChain of Thought (CoT)\nChain of thought prompting is a surprisingly simple method that has shown to improve the performance of LLMs on various benchmarks. Effectively, Wei (Wei et al. 2022) showed in their paper (almost 10k citations) that thought demonstrations improve performance.\nKojima (around 3.5k citations) (Kojima et al. 2022) showed that simply adding “Let’s think step by step” into the prompt can yield comparable performance boosts.\n\n\n\n\n\n\nNote\n\n\n\nVarious CoT variants have been proposed. I think Lilian Weng’s take that many of those things should be blog posts and not papers is true.\nSome useful variants are shown in (Fu et al. 2023). The paper also shows some of the prompt sensitivity (e.g., changing Q: to Question: or using \\n instead of . to separate steps.)\n\n\nSome follow up work as been discussing that one explanation for the improved performance via CoT reasoning is that the effective compute increases.\n\n\n\n\n\n\nNote\n\n\n\nCoT can also be thought of as one flavor of test-time compute. This is currently one of the most promising and active streams of research to increase the performance of LLMs. Models like o1 (Qin et al. 2024) heavily lean on utilizing test-time compute (i.e. “thinking” more inference time - ideally, making the amount of thinking proportional to the difficulty of the question).\n\n\n\n\nInternalized CoT\nSince generating extra tokens for reasoning in inference time is expensive, researchers attempted to internalize the reasoning pathway.\n\n\n\n\n\n\nTip\n\n\n\nTo converge training, the authors found that it is important to reset optimizer state. Optimizers such as AdamwW keep running averages - and those cause problems when the loss function suddenly changes.\n\n\n\n\nCoT traces are not faithful and might be limiting\n\n\n\nA tweet by one of the godfathers. Reasoning might not need to be verbalized.\n\n\nIt is well known, that CoT traces are not faithful., Turpin showed this by adding biases into the prompt. Those biases led to drops in model performance but were not mentioned by the model. This experiments directly allows to conclude that the verbalized explanations are not faithful as a reason for change in predictions is not verbalized.\nSome anthropomorphize this by linking this to neuroscience results that show that “Language is primarily a tool for communication rather than thought” (Fedorenko, Piantadosi, and Gibson 2024)\n\n\n\nRoles of the language network. Taken from (Fedorenko, Piantadosi, and Gibson 2024). Subfigure b shows that the language network is not strongly activated for non-linguistic tasks.\n\n\nIn addition, it is notable that CoT restricts models to one discrete reasoning path. However, it might be effective to explore multiple paths. A relevant work that does this is Tree of Thoughts (ToT) (Yao et al. 2023). ToT works by creating a branching structure of multiple potential solutions:\n\nSimilar to CoT it breaks down problems into sequential thought steps\nBut it generates multiple alternative thoughts at each step\nThis can be used to create a tree-like structure of reasoning paths\nThese trees can be explored using either:\n\nBreadth-first search (BFS)\nDepth-first search (DFS)\n\nEach node (state) can be evaluated using a classifier or majority vote",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Training Large Language Models to Reason in a Continuous Latent Space</span>"
    ]
  },
  {
    "objectID": "papers/coconut.html#methods",
    "href": "papers/coconut.html#methods",
    "title": "Training Large Language Models to Reason in a Continuous Latent Space",
    "section": "Methods",
    "text": "Methods\nThe key idea presented in the paper is to not verbalize “thoughts” as language tokens but instead use the hidden representations that the model produces as richer “thought vectors” that could, in principle, also encode multiple reasoning pathways at the same time.\n\n\n\nMethod proposed by Coconut compared to CoT.\n\n\nThat is, the approach continues to “think” in its internal representation but only verbalized the final answer.\n\nTraining approach\nTo train this model, the authors use a protocol that is inspired by the one utilized for internalized CoT: Over multiple steps, they replace verbalized thinking steps with latent ones.\n\n\n\nTraining protocol used by the authors.",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Training Large Language Models to Reason in a Continuous Latent Space</span>"
    ]
  },
  {
    "objectID": "papers/coconut.html#results",
    "href": "papers/coconut.html#results",
    "title": "Training Large Language Models to Reason in a Continuous Latent Space",
    "section": "Results",
    "text": "Results\nIn their benchmarks, the authors observed promising results for their approach. They showed that their approached outperforms other “internalized thought” techniques.\n\n\n\n\n\n\n\n\n\n\n\n\nMethod\nGSM8k\n\nProntoQA\n\nProsQA\n\n\n\n\n\n\nAcc. (%)\n# Tokens\nAcc. (%)\n# Tokens\nAcc. (%)\n# Tokens\n\n\nCoT\n\\(42.9 \\pm 0.2\\)\n25.0\n\\(98.8 \\pm 0.8\\)\n92.5\n\\(77.5 \\pm 1.9\\)\n49.4\n\n\nNo-CoT\n\\(16.5 \\pm 0.5\\)\n2.2\n\\(93.8 \\pm 0.7\\)\n3.0\n\\(76.7 \\pm 1.0\\)\n8.2\n\n\niCoT\n\\(30.0^*\\)\n2.2\n\\(99.8 \\pm 0.3\\)\n3.0\n\\(98.2 \\pm 0.3\\)\n8.2\n\n\nPause Token\n\\(16.4 \\pm 1.8\\)\n2.2\n\\(77.7 \\pm 21.0\\)\n3.0\n\\(75.9 \\pm 0.7\\)\n8.2\n\n\nCoconut (Ours)\n\\(34.1 \\pm 1.5\\)\n8.2\n\\(99.8 \\pm 0.2\\)\n9.0\n\\(97.0 \\pm 0.3\\)\n14.2\n\n\n- w/o curriculum\n\\(14.4 \\pm 0.8\\)\n8.2\n\\(52.4 \\pm 0.4\\)\n9.0\n\\(76.1 \\pm 0.2\\)\n14.2\n\n\n- w/o thought\n\\(21.6 \\pm 0.5\\)\n2.3\n\\(99.9 \\pm 0.1\\)\n3.0\n\\(95.5 \\pm 1.1\\)\n8.2\n\n\n- pause as thought\n\\(24.1 \\pm 0.7\\)\n2.2\n\\(100.0 \\pm 0.1\\)\n3.0\n\\(96.6 \\pm 0.8\\)\n8.2\n\n\n\nThey also found that increasing the number of latent thoughts per thinking step increases performance.\nBy decoding the hidden thoughts they could assign probabilities to different options that COCONUT “explored”. This can be used to construct search trees.\n\n\n\nSearch tree proposed for ProsQA.\n\n\nThe trees are a bit more “anecdotal” (as there are no systematic statistics) but an interesting perspective on the results",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Training Large Language Models to Reason in a Continuous Latent Space</span>"
    ]
  },
  {
    "objectID": "papers/coconut.html#conclusions",
    "href": "papers/coconut.html#conclusions",
    "title": "Training Large Language Models to Reason in a Continuous Latent Space",
    "section": "Conclusions",
    "text": "Conclusions\n\nThe current protocol is computationally expensive in training (requires multiple forward passes). If one would like to do this on scale, development of suitable infrastructure is needed.\nIt is nice to explore some new paradigms (with smaller models)\nSome of this also links to agents (instead of letting them talk via text we could also used hidden representation)\n\n\n\n\n\nFedorenko, Evelina, Steven T. Piantadosi, and Edward A. F. Gibson. 2024. “Language Is Primarily a Tool for Communication Rather Than Thought.” Nature 630 (8017): 575–86. https://doi.org/10.1038/s41586-024-07522-w.\n\n\nFu, Yao, Hao Peng, Ashish Sabharwal, Peter Clark, and Tushar Khot. 2023. “Complexity-Based Prompting for Multi-Step Reasoning.” https://arxiv.org/abs/2210.00720.\n\n\nGuo, Jeff, and Philippe Schwaller. 2024. “It Takes Two to Tango: Directly Optimizing for Constrained Synthesizability in Generative Molecular Design.” arXiv. https://doi.org/10.48550/ARXIV.2410.11527.\n\n\nKojima, Takeshi, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. “Large Language Models Are Zero-Shot Reasoners.” Advances in Neural Information Processing Systems 35: 22199–213.\n\n\nLandrum, Gregory A., Jessica Braun, Paul Katzberger, Marc T. Lehner, and Sereina Riniker. 2024. “Lwreg: A Lightweight System for Chemical Registration and Data Storage.” Journal of Chemical Information and Modeling 64 (16): 6247–52. https://doi.org/10.1021/acs.jcim.4c01133.\n\n\nOrsi, Markus, and Jean-Louis Reymond. 2024. “One Chiral Fingerprint to Find Them All.” Journal of Cheminformatics 16 (1): 53.\n\n\nQin, Yiwei, Xuefeng Li, Haoyang Zou, Yixiu Liu, Shijie Xia, Zhen Huang, Yixin Ye, et al. 2024. “O1 Replication Journey: A Strategic Progress Report – Part 1.” https://arxiv.org/abs/2410.18982.\n\n\nWei, Jason, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. “Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.” Advances in Neural Information Processing Systems 35: 24824–37.\n\n\nYao, Shunyu, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik Narasimhan. 2023. “Tree of Thoughts: Deliberate Problem Solving with Large Language Models.” https://arxiv.org/abs/2305.10601.",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Training Large Language Models to Reason in a Continuous Latent Space</span>"
    ]
  },
  {
    "objectID": "papers/emnpd.html",
    "href": "papers/emnpd.html",
    "title": "🧬 EMNPD Database",
    "section": "",
    "text": "A comprehensive endophytic microorganism natural products database for prompt the discovery of new bioactive substances",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>🧬 EMNPD Database</span>"
    ]
  },
  {
    "objectID": "papers/emnpd.html#why-did-i-choose-this-paper",
    "href": "papers/emnpd.html#why-did-i-choose-this-paper",
    "title": "🧬 EMNPD Database",
    "section": "Why did I choose this paper?",
    "text": "Why did I choose this paper?\n\nIt is a natural product database that aligns with our interests and is now integrated into COCONUT 2.0.\nThe data in this database was mined from literature and using LitSuggest reviews and low-scoring articles were discarded (threshold &lt;0.6), which also involved fine-tuning the model for optimal performance.\nThis is one of the newest databases that provides extensive information about molecules, including their source organisms, a key feature highly relevant to COCONUT.",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>🧬 EMNPD Database</span>"
    ]
  },
  {
    "objectID": "papers/emnpd.html#emnpd-endophytic-microorganism-natural-products-database",
    "href": "papers/emnpd.html#emnpd-endophytic-microorganism-natural-products-database",
    "title": "🧬 EMNPD Database",
    "section": "EMNPD (Endophytic Microorganism Natural Products Database)",
    "text": "EMNPD (Endophytic Microorganism Natural Products Database)\n\nEndophytic microorganisms, residing within plant tissues, play a vital role in biotic and abiotic stress responses by producing diverse natural products (NPs). Approximately 49.5% of FDA-approved drugs are derived from NPs or their derivatives, making endophytes a promising source for novel bioactive compounds. Despite this, existing databases lack a comprehensive focus on endophytes. The newly developed EMNPD database addresses this gap, providing open access to curated data on endophytic microorganism natural products and their bioactivities.\n\n\n📊 Key Statistics\n\n\n\n\n\n\n\n\n\n🧬 Natural Products\n🦠 Endophytes\n🎯 Biological Targets\n⚡ Bioactivities\n\n\n\n\n6,632\n1,017\n1,286\n91\n\n\n\n\nData: Includes physicochemical properties, ADMET information, and fermentation conditions\nAccessibility: Open-access, registration-free database at EMNPD website",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>🧬 EMNPD Database</span>"
    ]
  },
  {
    "objectID": "papers/emnpd.html#comparative-summary-of-microbial-np-databases",
    "href": "papers/emnpd.html#comparative-summary-of-microbial-np-databases",
    "title": "🧬 EMNPD Database",
    "section": "📈 Comparative Summary of Microbial NP Databases 📉",
    "text": "📈 Comparative Summary of Microbial NP Databases 📉\n\n\n\n\n\n\n\n\n\n\n\n\nDatabase\nFocus Area\nNatural Products\nSpecies\nBioactivity Data\nContent Data\nQuantitative Activity\n\n\n\n\n🌟 EMNPD\nEndophyte NPs\n6,632\n1,017\n✓\n✓\n✓\n\n\nMyxoDB\nMyxobacterial\n674\n✓\n✗\n✗\n✗\n\n\nmVOC\nMicrobial volatiles\n2,061\n1,034\n✗\n✗\n✗\n\n\nNPcVar\nPlant & microbial\n2,201\n694\n✗\n✓\n✗\n\n\nStreptomeDB\nStreptomycetes\n6,524\n3,302\n✓\n✗\n✗\n\n\nCMNPD\nMarine\n31,561\n3,354\n✗\n✗\n✓\n\n\nNPAtlas\nMicrobial\n33,372\n✓\n✗\n✗\n✗\n\n\nNPASS\nPlant & microbial\n96,481\n32,287\n✗\n✓\n✓",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>🧬 EMNPD Database</span>"
    ]
  },
  {
    "objectID": "papers/emnpd.html#construction-and-content",
    "href": "papers/emnpd.html#construction-and-content",
    "title": "🧬 EMNPD Database",
    "section": "Construction and Content",
    "text": "Construction and Content\n\nData Extraction and Curation\n\nSources: Data gathered from PubMed, keyword searches, and computational tools.\n\nFiltration Process:\n\nInitial collection: 2600 articles.\n\nRemoval of reviews and low-scoring articles using LitSuggest (threshold &lt;0.6).\n\nFinal curation: 1000 articles selected.\n\n\n\n\n\nData Collection and Processing\n\nCompound Characterization:\n\nStructures sourced using PubChemPy and manually drawn using tools like ChemDraw and KingDraw .\n\nClassification was performed using ClassyFire and ADMET predictions with ADMETlab 2.0.\n\nTaxonomic Information:\n\nEndophyte taxonomy derived from NCBI Taxonomy.\n\nHost plant and geographic data integrated for added context.\n\nBiological Activity Data:\n\nCategorized into antibacterial, cytotoxic, anti-inflammatory, etc.\n\nDetailed bioactivity records linked to targets, including proteins, cell lines, and organisms.",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>🧬 EMNPD Database</span>"
    ]
  },
  {
    "objectID": "papers/emnpd.html#database-content-and-statistics",
    "href": "papers/emnpd.html#database-content-and-statistics",
    "title": "🧬 EMNPD Database",
    "section": "Database Content and Statistics",
    "text": "Database Content and Statistics\n\nChemical Diversity:\n\n21 chemical superclasses represented.\n\nMajority of compounds adhere to Lipinski’s “Rule of Five.”\n\nTaxonomic Diversity:\n\n\npie title Distribution\n    \"Fungi\" : 87.5\n    \"Bacteria\" : 12.5\n\nBioactivity Data:\n\n\n\n\n\n\n\n\n\n\n\n📊 Activity records\n🎯 Bioactivity Types\n🧬 Proteins\n🔬 Cell Lines\n🌍 Species\n\n\n\n\n9,457\n86\n94\n282\n910",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>🧬 EMNPD Database</span>"
    ]
  },
  {
    "objectID": "papers/emnpd.html#utility-and-discussion",
    "href": "papers/emnpd.html#utility-and-discussion",
    "title": "🧬 EMNPD Database",
    "section": "Utility and Discussion",
    "text": "Utility and Discussion\n\nWeb Interface\n\nSearch Capabilities:\n\nAdvanced search with Boolean operators for NPs, targets, and bioactivities.\n\nStructure search enabled through Ketcher molecular editor.\n\n\n\n\nBrowsing Features:\n\nVisual tools like bar, tree, and sunburst charts for exploring data.\n\n\n\n\nDownloads\n\nAll data is available for free download, including Docker support for local deployment.\n\nGit: https://github.com/boilism/EMNPD\nData: https://figshare.com/articles/dataset/EMNPD_Download_Data/24078474",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>🧬 EMNPD Database</span>"
    ]
  },
  {
    "objectID": "papers/emnpd.html#conclusion",
    "href": "papers/emnpd.html#conclusion",
    "title": "🧬 EMNPD Database",
    "section": "Conclusion",
    "text": "Conclusion\nEndophytic microorganisms are a rich source of novel secondary metabolites, known for their diverse structures and significant biological activities, particularly in antimicrobial and anticancer research. Despite the frequent discovery of new and active natural products (NPs) from these microorganisms, the integration of this data into large-scale databases is often slow, highlighting the importance of efficient information sharing for research and development.\nTo address this EMNPD was developed. This database provides extensive data and interactive visualisation tools to aid in exploring the chemical diversity of these NPs. It is fully searchable and downloadable, designed to support various research perspectives. As interest in endophytic microorganisms continues to grow, EMNPD aims to become an essential resource for advancing drug discovery.\n\n\n\n\nGuo, Jeff, and Philippe Schwaller. 2024. “It Takes Two to Tango: Directly Optimizing for Constrained Synthesizability in Generative Molecular Design.” arXiv. https://doi.org/10.48550/ARXIV.2410.11527.\n\n\nLandrum, Gregory A., Jessica Braun, Paul Katzberger, Marc T. Lehner, and Sereina Riniker. 2024. “Lwreg: A Lightweight System for Chemical Registration and Data Storage.” Journal of Chemical Information and Modeling 64 (16): 6247–52. https://doi.org/10.1021/acs.jcim.4c01133.\n\n\nOrsi, Markus, and Jean-Louis Reymond. 2024. “One Chiral Fingerprint to Find Them All.” Journal of Cheminformatics 16 (1): 53.",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>🧬 EMNPD Database</span>"
    ]
  },
  {
    "objectID": "papers/light-rag.html",
    "href": "papers/light-rag.html",
    "title": "🚀 LightRAG",
    "section": "",
    "text": "Simple and Fast Retrieval-Augmented Generation",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>🚀 LightRAG</span>"
    ]
  },
  {
    "objectID": "papers/light-rag.html#simple-and-fast-retrieval-augmented-generation",
    "href": "papers/light-rag.html#simple-and-fast-retrieval-augmented-generation",
    "title": "🚀 LightRAG",
    "section": "",
    "text": "GitHub",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>🚀 LightRAG</span>"
    ]
  },
  {
    "objectID": "papers/light-rag.html#why-did-i-choose-this-paper",
    "href": "papers/light-rag.html#why-did-i-choose-this-paper",
    "title": "🚀 LightRAG",
    "section": "Why did I choose this paper?",
    "text": "Why did I choose this paper?\n\nOne of our core areas of focus is natural products. Our objective is to extract, curate, and derive insights from natural product (NP) data published across journals, literature, and web articles spanning decades, while also leveraging knowledge from newly published works.\nWe aim to extract and process this information in a cost-effective manner, with the ultimate aim of achieving higher level of factual accuracy and abstract understanding of NP chemistry.\nWhile retrieval-augmented generation (RAG) represents a recent and promising architecture for this purpose, it has inherent limitations. Upon further exploration, LightRAG has emerged as a more effective solution, addressing many of our requirements more comprehensively compared to fine-tuning traditional LLMs.",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>🚀 LightRAG</span>"
    ]
  },
  {
    "objectID": "papers/light-rag.html#introduction",
    "href": "papers/light-rag.html#introduction",
    "title": "🚀 LightRAG",
    "section": "Introduction",
    "text": "Introduction\nRetrieval-Augmented Generation (RAG) systems enhance large language models (LLMs) byintegrating external knowledge sources, enabling more accurate and contextuallyrelevant responses tailored to user needs. However, existing RAG systems havesignificant limitations, including reliance on flat data representations andinadequate contextual awareness, which can lead to fragmented answers that fail tocapture complex inter-dependencies. To address these challenges, we propose LightRAG,which incorporates graph structures into text indexing and retrieval processes. Thisinnovative framework employs a dual-level retrieval system that enhances comprehensiveinformation retrieval from both low-level and high-level knowledge discovery. Additionally,the integration of graph structures with vector representations facilitates efficientretrieval of related entities and their relationships, significantly improving responsetimes while maintaining contextual relevance. This capability is further enhanced by anincremental update algorithm that ensures the timely integration of new data, allowing thesystem to remain effective and responsive in rapidly changing data environments. Extensiveexperimental validation demonstrates considerable improvements in retrieval accuracy andefficiency compared to existing approaches.",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>🚀 LightRAG</span>"
    ]
  },
  {
    "objectID": "papers/light-rag.html#architecture",
    "href": "papers/light-rag.html#architecture",
    "title": "🚀 LightRAG",
    "section": "Architecture",
    "text": "Architecture\n\n\n\nTeaser\n\n\n\nGraph-Enhanced Entity and Relationship Extraction\nLightRAG enhances the retrieval system by segmenting documents into smaller, more manageable pieces. This strategy allows for quick identification and access to relevant information without analyzing entire documents. The process includes the following functions:\n\nExtracting Entities and Relationships (R):\nPrompts an LLM to identify entities (nodes) and their relationships (edges) within the text data. For example, entities like “Cardiologists” and “Heart Disease” and relationships such as “Cardiologists diagnose Heart Disease” can be extracted. Raw text is segmented into multiple chunks for efficiency.\nLLM Profiling for Key-Value Pair Generation (P):\nUses a profiling function to generate key-value pairs for each entity and relation. Keys are words or short phrases for retrieval, while values summarize relevant snippets for text generation.\nDeduplication to Optimize Graph Operations (D):\nMerges identical entities and relations from different text segments, reducing graph size and improving processing efficiency.\n\nAdvantages:\n\nComprehensive Information Understanding: Graph structures enable multi-hop subgraph extraction for complex queries.\nEnhanced Retrieval Performance: Key-value structures optimize retrieval accuracy and efficiency.\n\nFast Adaptation to Incremental Knowledge Base\nLightRAG adapts efficiently to evolving data by incrementally updating the knowledge base without reprocessing the entire database.\n\nSeamless Integration of New Data: Consistently integrates new data while preserving existing graph integrity.\nReducing Computational Overhead: Eliminates the need to rebuild the entire graph, conserving resources and maintaining system accuracy.\n\nDual-level Retrieval Paradigm\nSupports detailed and abstract queries for specific and broader information retrieval.\n\nSpecific Queries: Target precise details (e.g., “Who wrote ‘Pride and Prejudice’?”).\nAbstract Queries: Cover broader topics (e.g., “How does AI influence modern education?”).\n\nCombines low-level and high-level retrieval strategies for effective query handling.\nRetrieval-Augmented Answer Generation\nUses an LLM to generate answers by unifying the query with multi-source data, ensuring relevance and alignment with user needs.",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>🚀 LightRAG</span>"
    ]
  },
  {
    "objectID": "papers/light-rag.html#algorithm-flowchart",
    "href": "papers/light-rag.html#algorithm-flowchart",
    "title": "🚀 LightRAG",
    "section": "Algorithm Flowchart",
    "text": "Algorithm Flowchart\n Figure 1: LightRAG Indexing Flowchart - Img Caption : Source  Figure 2: LightRAG Retrieval and Querying Flowchart - Img Caption : Source",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>🚀 LightRAG</span>"
    ]
  },
  {
    "objectID": "papers/light-rag.html#experiments",
    "href": "papers/light-rag.html#experiments",
    "title": "🚀 LightRAG",
    "section": "Experiments",
    "text": "Experiments\nDefining ground truth for complex RAG queries is challenging. To address this, LightRAG adopts an LLM-based multi-dimensional comparison method, using GPT-4o-mini for evaluation across four dimensions:\n\nComprehensiveness: How thoroughly the answer addresses all aspects of the question.\nDiversity: The richness of perspectives and insights.\nEmpowerment: How effectively the answer enables understanding and informed judgments.\nOverall: The cumulative performance across all dimensions.\n\nComparison with Existing RAG Methods\nLightRAG outperforms baselines across evaluation dimensions and datasets. Results are shown in Table 1:\n\n\n\nBaseline Comparison\n\n\nAblation Study\nAnalyzed the impact of low-level and high-level retrieval paradigms:\n\nLow-level-only Retrieval: Focuses on specific entities but struggles with complex queries.\nHigh-level-only Retrieval: Gathers broad content but lacks detailed insights.\nHybrid Mode: Combines both approaches, ensuring comprehensive and detailed data retrieval.\n\n\n\n\nAblation Study\n\n\nModel Cost and Adaptability Analysis\nCompared LightRAG with GraphRAG in terms of tokens, API calls, and adaptability to dynamic data changes. Results are shown in Table:",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>🚀 LightRAG</span>"
    ]
  },
  {
    "objectID": "papers/light-rag.html#conclusion",
    "href": "papers/light-rag.html#conclusion",
    "title": "🚀 LightRAG",
    "section": "Conclusion",
    "text": "Conclusion\nThis work presents a significant advancement in Retrieval-Augmented Generation (RAG) by integrating a graph-based indexing approach that enhances both the efficiency and depth of information retrieval. LightRAG employs a comprehensive knowledge graph to enable rapid and relevant document retrieval, facilitating a more profound understanding of complex queries. Its dual-level retrieval paradigm supports the extraction of both specific details and abstract insights, addressing diverse user requirements. Additionally, LightRAG’s seamless incremental update mechanism ensures the system remains adaptive and effective, maintaining its relevance in the face of evolving data. By excelling in efficiency and effectiveness, LightRAG substantially improves the speed and quality of information retrieval and generation while reducing the costs associated with LLM inference.\nOnly time will reveal the true potential of the LightRAG architecture as an effective tool for extracting natural product (NP) data / metadata in a structured format, and ultimately submitting it to COCONUT to support broader research initiatives and further curation efforts in the field.\n\n\n\n\nGuo, Jeff, and Philippe Schwaller. 2024. “It Takes Two to Tango: Directly Optimizing for Constrained Synthesizability in Generative Molecular Design.” arXiv. https://doi.org/10.48550/ARXIV.2410.11527.\n\n\nLandrum, Gregory A., Jessica Braun, Paul Katzberger, Marc T. Lehner, and Sereina Riniker. 2024. “Lwreg: A Lightweight System for Chemical Registration and Data Storage.” Journal of Chemical Information and Modeling 64 (16): 6247–52. https://doi.org/10.1021/acs.jcim.4c01133.\n\n\nOrsi, Markus, and Jean-Louis Reymond. 2024. “One Chiral Fingerprint to Find Them All.” Journal of Cheminformatics 16 (1): 53.",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>🚀 LightRAG</span>"
    ]
  },
  {
    "objectID": "papers/nmr-shift-prediction-from-small-data-quantities.html",
    "href": "papers/nmr-shift-prediction-from-small-data-quantities.html",
    "title": "NMR shift prediction from small data quantities",
    "section": "",
    "text": "Why discuss this paper?\nI chose the NMR shift prediction from small data quantities (Rull, Fischer, and Kuhn 2023) for current topics in the cheminformatics seminar because: - Many of the team memebers and I work on nmrXiv repository development and we would like to introduce NMR shift prediction in the future, which makes the article very relevant. - It is crucial to us to be familiar with available NMR shift prediction models and to understand the conditions under which they perform best. - To understand the effect of NMR experiment parameters (nuclei, solvents) on the prediction results.",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>NMR shift prediction from small data quantities</span>"
    ]
  },
  {
    "objectID": "papers/nmr-shift-prediction-from-small-data-quantities.html#context",
    "href": "papers/nmr-shift-prediction-from-small-data-quantities.html#context",
    "title": "NMR shift prediction from small data quantities",
    "section": "Context",
    "text": "Context\nMachine Learning (ML) models perform best when trained with the maximum amount of data available. For heteronuclei NMR experiments, such an amount of data is not available. The paper demonstrates a novel ML model (called the 2023 model) that achieves good results for low amounts of data. They predict 13C and 19F NMR chemical shifts of small molecules in specific solvents.  Fig1: Graphic abstract of the paper",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>NMR shift prediction from small data quantities</span>"
    ]
  },
  {
    "objectID": "papers/nmr-shift-prediction-from-small-data-quantities.html#introduction",
    "href": "papers/nmr-shift-prediction-from-small-data-quantities.html#introduction",
    "title": "NMR shift prediction from small data quantities",
    "section": "Introduction",
    "text": "Introduction\nNMR chemical shift prediction can be done in two ways: - Ab-Initio: Calculate the chemical shifts based on the molecular structure and theoretical principles. They require no experimental data but are computationally intensive. - Using Existing Data: Like using ML models to predict shifts for new compounds relying on the experimental data of others. They are computationally efficient (Jonas, Kuhn, and Schlörer 2022).",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>NMR shift prediction from small data quantities</span>"
    ]
  },
  {
    "objectID": "papers/nmr-shift-prediction-from-small-data-quantities.html#problem-setting",
    "href": "papers/nmr-shift-prediction-from-small-data-quantities.html#problem-setting",
    "title": "NMR shift prediction from small data quantities",
    "section": "Problem setting",
    "text": "Problem setting\n\nThe amount of experimental data in NMR for heteronuclei, specific classes of compounds, and particular solvents is limited.\nThe number of data points used is a significant factor in the quality of the predictions.\nSome machine learning methods only show suitable predictive power with more than 5000 training examples.",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>NMR shift prediction from small data quantities</span>"
    ]
  },
  {
    "objectID": "papers/nmr-shift-prediction-from-small-data-quantities.html#approach",
    "href": "papers/nmr-shift-prediction-from-small-data-quantities.html#approach",
    "title": "NMR shift prediction from small data quantities",
    "section": "Approach",
    "text": "Approach\n\nThe research was restricted to small organic molecules in solution, as other NMR fields require different and more specific models.\nIt focuses on the prediction of NMR chemical shifts using ML (graph neural networks).\nThe molecules were represented as a graph, where the atoms are nodes, and the bonds are edges. Each has its own features.\nAtoms / Nodes features: Atomic number, Atomic radius, Number of neutrons, Electronegativity, Electron affinity.\nBonds / Edges features: Bond length, Bond type.\nThis specific feature selection was chosen because of preliminary experiments, which measured the impact of each feature on the final prediction.\nThe best-performing features were combined until the prediction quality was no longer improving.",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>NMR shift prediction from small data quantities</span>"
    ]
  },
  {
    "objectID": "papers/nmr-shift-prediction-from-small-data-quantities.html#information-flow",
    "href": "papers/nmr-shift-prediction-from-small-data-quantities.html#information-flow",
    "title": "NMR shift prediction from small data quantities",
    "section": "Information Flow",
    "text": "Information Flow\n Fig2: Schematic flow of information in the message‑passing graph network. The workflow can be split into the following steps: encoding, message‑passing, and prediction of shifts with an MLP.\n\nThe chosen features are encoded by multi-layer perceptron functions.\nThe encoded features are then processed through multiple rounds of message-passing in graph network (Duvenaud et al. 2015) (Gilmer et al. 2017).\nThis results in new node features that get passed to another MLP that predicts the final chemical shifts.\nThe most important hyperparameters were the number of message-passing steps, the learning rate, and the weight decay.",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>NMR shift prediction from small data quantities</span>"
    ]
  },
  {
    "objectID": "papers/nmr-shift-prediction-from-small-data-quantities.html#performance-comparison-for-13c-nmr-shift-prediction",
    "href": "papers/nmr-shift-prediction-from-small-data-quantities.html#performance-comparison-for-13c-nmr-shift-prediction",
    "title": "NMR shift prediction from small data quantities",
    "section": "Performance comparison for 13C NMR shift prediction",
    "text": "Performance comparison for 13C NMR shift prediction\n\nTwo other prediction methods were used for comparison:\n\nHierarchically Ordered Spherical Environment (HOSE) codes:\n\nIt describes atoms and their environments as strings.\nCompounds with similar strings and known chemical shifts are looked up and used for prediction (Bremser 1978).\n\n2019 model\n\n\nIt uses a convolutional graphical neural network (Jonas and Kuhn 2019).\n\nData:\n\nAll data was taken from NMRShiftDB2, an open NMR database (Oellien, Fechner, and Engel 2012), which contains lists of chemical shift values.\nThe datasets consist of random selections of structures.\n\nEvaluation:\n\n75:25 training-to-test split.\nNo separate validation set due to the small size of the datasets.\nThe predictive performance of the different models was analyzed when trained on an increasing number of molecules.\n\nError metrics:\n\nThe mean absolute error (MAE): Less Sensitive to Large Errors.\nThe root mean squared error (RMSE): It gives more weight to larger errors.\nThe mean absolute scaled error (MASE): Insensitive to the scale of the data, such as dealing with different nuclei or solvents.\nThe standard deviation σ of the error: Variability of the errors in a set of predictions.\n\n\n\nResults\n MAE of a 13C NMR shift prediction, using increasing numbers of spectra\n RMSE of a 13C NMR shift prediction, using increasing numbers of spectra\n MASE of a 13C NMR shift prediction, using increasing numbers of spectra\n σ of a 13C NMR shift prediction, using increasing numbers of spectra\n\nThe 2023 model outperforms the 2019 model when trained on up to 2500 data points (2023 is better for small data).\nThe HOSE codes is even better than the 2023 model for small data. However, in some cases, a prediction based on HOSE codes is not possible if no examples with high enough similarity exist in the training set.",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>NMR shift prediction from small data quantities</span>"
    ]
  },
  {
    "objectID": "papers/nmr-shift-prediction-from-small-data-quantities.html#other-heteronuclei",
    "href": "papers/nmr-shift-prediction-from-small-data-quantities.html#other-heteronuclei",
    "title": "NMR shift prediction from small data quantities",
    "section": "Other heteronuclei",
    "text": "Other heteronuclei\n\nOthe than 13C, 19F spectra was used to test 2023 model.\nIn NMRShiftDB2, there were 957 structures with measured 19F spectra.\n\n\n\n\nMAE of a 19F NMR shift prediction, using increasing number of samples, on a logarithmic scale\n\n\nMAE of a 19F NMR shift prediction, using increasing number of samples, on a logarithmic scale\n\nResults\n\nHOSE codes is again the best with small data with the 2019 model being the worst with them.\nThe 2023 model almost reached HOSE codes performance with the increasing number of spectra.\nHose codes always has the disadvantage that it might not give a prediction at all.",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>NMR shift prediction from small data quantities</span>"
    ]
  },
  {
    "objectID": "papers/nmr-shift-prediction-from-small-data-quantities.html#solvents",
    "href": "papers/nmr-shift-prediction-from-small-data-quantities.html#solvents",
    "title": "NMR shift prediction from small data quantities",
    "section": "Solvents",
    "text": "Solvents\nThe used solvent is a major factor influencing the chemical shift values of a particular compound due to: - its influence on the chemical environment of the molecule - the possibility of forming hydrogen bonds - changes in the charge state of the investigated molecule.\nAccurate predictions require using solvent information, but this makes the available data even fewer. Using 13C spectra from NMRShiftDB2 they train separate models for each solvent and compare the results to the values achieved by using all 13C spectra. The models are not optimized for a solvent-specific prediction.\n\nResults\nThe overall tendency is similar to what we have seen before: The predictive quality of the 2019 model starts off with high errors and significantly improves beyond 1000 spectra. The 2023 model outperforms the 2019 model on smaller datasets. HOSE codes are generally doing well.\n\n\n\nCDCl3\n\n\nMAE of a 13C NMR shift prediction, using increasing numbers of samples\nThe solvent-specific training produces much better results compared to the overall model.",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>NMR shift prediction from small data quantities</span>"
    ]
  },
  {
    "objectID": "papers/nmr-shift-prediction-from-small-data-quantities.html#model-optimization-options",
    "href": "papers/nmr-shift-prediction-from-small-data-quantities.html#model-optimization-options",
    "title": "NMR shift prediction from small data quantities",
    "section": "Model optimization options:",
    "text": "Model optimization options:\n2023 model can possibly be further optimized by: - Ensuring structural diversity in training/testing datasets, which wasn’t done in model 2023. The random distribution of data was assumed. - optimizing a model hyperparameters specifically for a nucleus or a solvent.",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>NMR shift prediction from small data quantities</span>"
    ]
  },
  {
    "objectID": "papers/nmr-shift-prediction-from-small-data-quantities.html#takeaways",
    "href": "papers/nmr-shift-prediction-from-small-data-quantities.html#takeaways",
    "title": "NMR shift prediction from small data quantities",
    "section": "Takeaways",
    "text": "Takeaways\n\nPredicting NMR shifts is a challenging task for small data, which is the case with heteronuclei and even further with different solvents.\nThis paper demonstrates a new ML model that performs better than another model by the same group presented in 2019 for small data.\nEven though Hose codes can be even better than 2023 for small data, it can’t always provide a prediction.",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>NMR shift prediction from small data quantities</span>"
    ]
  },
  {
    "objectID": "papers/nmr-shift-prediction-from-small-data-quantities.html#references",
    "href": "papers/nmr-shift-prediction-from-small-data-quantities.html#references",
    "title": "NMR shift prediction from small data quantities",
    "section": "References",
    "text": "References\n\n\n\n\nBremser, W. 1978. “Hose—a Novel Substructure Code.” Analytica Chimica Acta 103 (4): 355–65.\n\n\nDuvenaud, David K, Dougal Maclaurin, Jorge Iparraguirre, Rafael Bombarell, Timothy Hirzel, Alán Aspuru-Guzik, and Ryan P Adams. 2015. “Convolutional Networks on Graphs for Learning Molecular Fingerprints.” Advances in Neural Information Processing Systems 28.\n\n\nGilmer, Justin, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. 2017. “Neural Message Passing for Quantum Chemistry,” 1263–72.\n\n\nGuo, Jeff, and Philippe Schwaller. 2024. “It Takes Two to Tango: Directly Optimizing for Constrained Synthesizability in Generative Molecular Design.” arXiv. https://doi.org/10.48550/ARXIV.2410.11527.\n\n\nJonas, Eric, and Stefan Kuhn. 2019. “Rapid Prediction of NMR Spectral Properties with Quantified Uncertainty.” Journal of Cheminformatics 11: 1–7.\n\n\nJonas, Eric, Stefan Kuhn, and Nils Schlörer. 2022. “Prediction of Chemical Shift in NMR: A Review.” Magnetic Resonance in Chemistry 60 (11): 1021–31.\n\n\nLandrum, Gregory A., Jessica Braun, Paul Katzberger, Marc T. Lehner, and Sereina Riniker. 2024. “Lwreg: A Lightweight System for Chemical Registration and Data Storage.” Journal of Chemical Information and Modeling 64 (16): 6247–52. https://doi.org/10.1021/acs.jcim.4c01133.\n\n\nOellien, Frank, Uli Fechner, and Thomas Engel. 2012. “7th German Conference on Chemoinformatics: 25 CIC-Workshop Goslar, Germany. 6-8 November 2011. Abstracts.” Journal of Cheminformatics 4 (Suppl 1): A1–P62.\n\n\nOrsi, Markus, and Jean-Louis Reymond. 2024. “One Chiral Fingerprint to Find Them All.” Journal of Cheminformatics 16 (1): 53.\n\n\nRull, Herman, Markus Fischer, and Stefan Kuhn. 2023. “NMR Shift Prediction from Small Data Quantities.” Journal of Cheminformatics 15 (1): 114.",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>NMR shift prediction from small data quantities</span>"
    ]
  },
  {
    "objectID": "papers/llm_confidence_2025.html",
    "href": "papers/llm_confidence_2025.html",
    "title": "What large language models know and what people think they know",
    "section": "",
    "text": "Why did I choose this paper?",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>What large language models know and what people think they know</span>"
    ]
  },
  {
    "objectID": "papers/llm_confidence_2025.html#why-did-i-choose-this-paper",
    "href": "papers/llm_confidence_2025.html#why-did-i-choose-this-paper",
    "title": "What large language models know and what people think they know",
    "section": "",
    "text": "interesting topic for last current topics session\npsychological aspect of confidence: LLM vs human\nlink: (Steyvers et al. 2025)",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>What large language models know and what people think they know</span>"
    ]
  },
  {
    "objectID": "papers/llm_confidence_2025.html#motivation",
    "href": "papers/llm_confidence_2025.html#motivation",
    "title": "What large language models know and what people think they know",
    "section": "Motivation",
    "text": "Motivation\n\nlimited prior work (as stated by authors)\nquestion of how well LLMs communicate uncertainty is unexplored\ninvestigation of gap between what LLM knows and what human thinks it knows\nessential due to increasing reliance on LLMs for decision making\nrecent studies show that LLMs may possess internal knowledge reflection, since they can distinguish…\n\nanswerable and unanswerable questions\ntruth and lies",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>What large language models know and what people think they know</span>"
    ]
  },
  {
    "objectID": "papers/llm_confidence_2025.html#terminology",
    "href": "papers/llm_confidence_2025.html#terminology",
    "title": "What large language models know and what people think they know",
    "section": "Terminology",
    "text": "Terminology\n\ninternal LLM confidence: internal probability of a selected answer is compared to other possible answers, BUT is not displayed to user\nhuman confidence: human assessment of likelihood of answer being correct (numerical probability)\n\nhumans rely only on LLM language to assess confidence\nno info about LLM’s internal confidence\n\ncalibration gap: difference between LLMs internal confidence and human perception of this confidence\ndiscrimination gap: ability of human and LLM to distinguish (discriminate) between right or wrong answers",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>What large language models know and what people think they know</span>"
    ]
  },
  {
    "objectID": "papers/llm_confidence_2025.html#main-research-questions",
    "href": "papers/llm_confidence_2025.html#main-research-questions",
    "title": "What large language models know and what people think they know",
    "section": "Main research questions",
    "text": "Main research questions\n\ninvestigation of relationship between models confidence and its accuracy\n\nis it well calibrated?\n\nAmount of calibration and discrimination gap?\n\nMeans: Is there a notable gap in assessment of output accuracy by human vs LLM?\n\nCan the gaps be reduced?\n\nMeans: can human confidence be improved by reflecting LLMs internal confidence in output?",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>What large language models know and what people think they know</span>"
    ]
  },
  {
    "objectID": "papers/llm_confidence_2025.html#methods",
    "href": "papers/llm_confidence_2025.html#methods",
    "title": "What large language models know and what people think they know",
    "section": "Methods",
    "text": "Methods\n\nLarge language models\n\nGPT 3.5, PaLM2, GPT 4.o\nGPT 3.5 and PaLM2 were applied subset of Massive Multitask Language Understanding (MMLU) dataset\n\nmultiple choice questions from science, technology, engineering, maths (STEM), humanities, social sciences…\n\nGPT 4.o was applied to TriviaQA dataset\n\nshort-answer questions\n\nmodels confidence is computed by computing the token likelihoods for each question (numerical propability)\n\n\n\n\n\nBehavioural experiments\n\n\n\n\n“Overview of the evaluation methodology for assessing the calibration gap between model confidence and human confidence in the model.”\n\n\nsteps for multiple choice:\n\nprompt LLM with question to get internal confidence for each answer\nselect most likely answer\nprompt LLM again to generate explanation for the chosen answer\nshowing the LLM-question and -answer to the user (non expert) → estimation of probability if correct\nuser provides his/her own answer for later experiment\n\nsteps for short-answer question:\n\nsame as for multiple choice with one exception\ninstead of step 2 from before LLM is prompted again to evaluate if answer is true or false\n\nAll experiments:\n\n\n\n\n“Overview of experiments”\n\n\nExp. 1: assesses human perception of LLMs default answer (gives information about calibration and discrimination gap)\nExp. 2: Manipulation of prompts with uncertainty language (low, medium, high confidence) and length (short, mid, long)\n\n\n\n\n“Prompt styles across Experiments 1, 2, and 3”\n\n\n\n\n\n\n\nMetrics\n\nhow well do human and model confidence levels correlate with correctness of answers…\nusage of ECE (expected calibration error)\n\n\n\n\n“Equation for calibration error”\n\n\naveraging absolute differences between accuracy and confidence across M probability bins\nif whole term equals zero than confidence predicts accuracy perfectly\nN - total sample count\nB(m) - m’th confidence bin\nacc(B(m)) - accuracy for samples in m’th bin\nconf(B(m)) - average confidence in m’th bin\n\n\n\n“Visualisation of ECE”\n\n\n\nand AUC (area under curve)\n\n\n\n\n“Equation for area under curve”\n\n\nassessment of diagnostic ability of confidence scores in distinguishing between correct and incorrect answers\nif whole term equals 1 then confidence perfectly describes if an answer is correct or wrong (perfect discrimination)\nif term equals 0.5 confidence is not better than chance discrimination\nN(pos) and N(neg) - counts for negative and positive answers\nC(i) and C(j) - confidence scores for i’th correct and j’th incorrect answer\nI - equals “1” if confidence of correct answer is larger than that of incorrect answer\n\n\n\n“Visualization for area under curve”",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>What large language models know and what people think they know</span>"
    ]
  },
  {
    "objectID": "papers/llm_confidence_2025.html#results",
    "href": "papers/llm_confidence_2025.html#results",
    "title": "What large language models know and what people think they know",
    "section": "Results",
    "text": "Results\n\nCalibration and discrimination gap\n\n\n\n\n“Calibration error and discrimination for model confidence and human confidence across the behavioural experiments and LLMs.”\n\n\ncomparison of model and human confidence\ndashed lines show calibration and discrimination gap\ncalibration error (left side)\n\nbig gap between model and human calibration\nmeans accuracy is over- or underestimated by humans compared to model\nstandard explanations of LLM do not enable user to judge likelihood of correctness\n\ndiscrimination error\n\nagain big gap between how well humans and models confidence can discriminate between correct or wrong answer\nhumans only slightly better than random guessing for baseline (default) answers\n\n\n\n\n\n\nReducing calibration and discrimination gap\n\nQuestion: Can gaps be narrowed if low, medium and high confidence answer is displayed when LLM has low, medium or high confidence?\nshort answer: yes, see figure above!\nlong answer: application of selection rules (see Eq. 1)\n\n\n\n\n“Equation 1”\n\n\nfor this task only questions were chosen where the uncertainty language matched the inner model confidence\n\nas seen in figure (red bar) calibration and discrimination gap is narrowed substantially after application of selection rule\ngaps are narrowed with modified explanations sensitive to models internal confidence!\nresult: it makes sense to link uncertainty language to internal model confidence\n\n\n\n\n\nDeeper insight into calibration error: overconfidence\n\n\n\n\n“Calibration diagrams for model confidence and human confidence across experiments 1 and 2.”\n\n\ngraphs plot model or human confidence (x-axis) against accuracy (y-axis)\nhumans and models mostly overconfident (graph under line)\nonly GPT-4o is underconfident (graph above line)\nhumans mostly believe that LLMs are more accurate than they actually are!\n\n\n\n\n\nExplanation style and length affect human confidence\n\nQuestion: How is human confidence affected by uncertainty expressed by LLM?\n\n\n\n“Mean human confidence across LLM explanation styles varying in uncertainty language and length”\n\n\ngraphs show human confidence for every explanation style…\n\nlow, mid, high confidence (uncertainty language)\nshort, mid, long answers\n\ndashed line is average human confidence from baseline explanations (Exp. 1)\nuncertainty language and length has influence on human confidence\n\nlow confidence language results in lower human confidence (less trust)\nlonger sentences result in higher human confidence (more trust)\n\nanswer\n\n\n\n\n\nParticipants lack specialized knowledge\n\nQuestion: Do participants have any independent (expert) knowledge?\nfor exp. 2 participants also provided their own answer\ntests resulted that participants were not smarter than LLM (no experts present in tested groups)\napplication of selection rule did not result in accuracy increase caused by the lack of human knowledge\n\nmeaning: you might know if an LLM is certain or uncertain about its answer, but you still have to find a better answer",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>What large language models know and what people think they know</span>"
    ]
  },
  {
    "objectID": "papers/llm_confidence_2025.html#potential-causes-of-overconfidence",
    "href": "papers/llm_confidence_2025.html#potential-causes-of-overconfidence",
    "title": "What large language models know and what people think they know",
    "section": "Potential causes of overconfidence",
    "text": "Potential causes of overconfidence\n\nReinforcement Learning from Human Feedback\n\nLLMs are trained on human preferences, leading to bias in explanations\nUsers favor detailed explanations, causing models to generate overly persuasive responses\n\nAutoregressive Nature of LLMs\n\nModels generate explanations that reinforce their initial answer, leading to assertive reasoning\nSimilar to choice-supportive biases in human psychology (describes the tendency to justify past (less meaningful) purchases with rational arguments in retrospect)",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>What large language models know and what people think they know</span>"
    ]
  },
  {
    "objectID": "papers/llm_confidence_2025.html#conclusion",
    "href": "papers/llm_confidence_2025.html#conclusion",
    "title": "What large language models know and what people think they know",
    "section": "Conclusion",
    "text": "Conclusion\n\nusers overestimate LLM accuracy, especially with default explanations\nhuman overconfidence poses risks in critical decision making\nlonger explanations increased user confidence, meaning users relied on explanation length rather than content to judge accuracy\nmodified answer explanations improved human accuracy perception, which highlights the need for transparent LLM communication",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>What large language models know and what people think they know</span>"
    ]
  },
  {
    "objectID": "papers/llm_confidence_2025.html#references",
    "href": "papers/llm_confidence_2025.html#references",
    "title": "What large language models know and what people think they know",
    "section": "References",
    "text": "References\n\n\n\n\nGuo, Jeff, and Philippe Schwaller. 2024. “It Takes Two to Tango: Directly Optimizing for Constrained Synthesizability in Generative Molecular Design.” arXiv. https://doi.org/10.48550/ARXIV.2410.11527.\n\n\nLandrum, Gregory A., Jessica Braun, Paul Katzberger, Marc T. Lehner, and Sereina Riniker. 2024. “Lwreg: A Lightweight System for Chemical Registration and Data Storage.” Journal of Chemical Information and Modeling 64 (16): 6247–52. https://doi.org/10.1021/acs.jcim.4c01133.\n\n\nOrsi, Markus, and Jean-Louis Reymond. 2024. “One Chiral Fingerprint to Find Them All.” Journal of Cheminformatics 16 (1): 53.\n\n\nSteyvers, Mark, Heliodoro Tejeda, Aakriti Kumar, Catarina Belem, Sheer Karny, Xinyue Hu, Lukas W Mayer, and Padhraic Smyth. 2025. “What Large Language Models Know and What People Think They Know.” Nature Machine Intelligence, 1–11.",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>What large language models know and what people think they know</span>"
    ]
  },
  {
    "objectID": "papers/paper_bench_2025.html",
    "href": "papers/paper_bench_2025.html",
    "title": "PaperBench: Evaluating AI’s Ability to Replicate AI Research",
    "section": "",
    "text": "Why discussing this paper?\nI chose PaperBench from OpenAI (Starace et al. 2025) because:",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>PaperBench: Evaluating AI’s Ability to Replicate AI Research</span>"
    ]
  },
  {
    "objectID": "papers/paper_bench_2025.html#why-discussing-this-paper",
    "href": "papers/paper_bench_2025.html#why-discussing-this-paper",
    "title": "PaperBench: Evaluating AI’s Ability to Replicate AI Research",
    "section": "",
    "text": "It is a recent paper that proposes a new benchmark for evaluating LLM-powered agents that work in developing AI research.\nSpecifically, the task of reproducing existing work should be the first milestone in the development of LLM-powered agents that can autonomously conduct research.\n\nThey use LLM-as-Judge for evaluating some of the tasks.",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>PaperBench: Evaluating AI’s Ability to Replicate AI Research</span>"
    ]
  },
  {
    "objectID": "papers/paper_bench_2025.html#context",
    "href": "papers/paper_bench_2025.html#context",
    "title": "PaperBench: Evaluating AI’s Ability to Replicate AI Research",
    "section": "Context",
    "text": "Context\nAgents are becoming increasingly capable of performing complex tasks, including some sparks of autonomous independent research (Lu et al. 2024). However, perhaps one of the first steps in AI and coding research is the ability to reproduce existing work. This is a crucial step in validating and building upon previous research findings. While there are many benchmarks for evaluating LLMs in various domains, there are few that focus specifically on the task of reproducing AI research, and more specifically, only having access to the paper.",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>PaperBench: Evaluating AI’s Ability to Replicate AI Research</span>"
    ]
  },
  {
    "objectID": "papers/paper_bench_2025.html#prior-work",
    "href": "papers/paper_bench_2025.html#prior-work",
    "title": "PaperBench: Evaluating AI’s Ability to Replicate AI Research",
    "section": "Prior Work",
    "text": "Prior Work\n\nSciCode: A Research Coding Benchmark Curated by Scientists\nSciCode (Tian et al. 2024) is a benchmark designed to evaluate the ability of LLMs to reproduce scientific code. It consists of a set of tasks based on reproducing AI scientific scripts. Interestingly, the scripts are not synthetic, but rather real-world scripts that have been used in actual research papers. The benchmark is curated by scientists and includes a diverse set of tasks from various domains, including biology, chemistry, and physics.\nThe benchmark is very well designed since each of the scripts is divided into subtasks, which allows for a more detailed evaluation of the agents. The agents are provided only with the task and the different docstrings explaining the subtasks. The benchmark is designed to be challenging, with a focus on evaluating the ability of LLMs to understand and reproduce complex scientific code. The authors also introduce a novel evaluation framework that uses LLMs-as-Judge to assess the quality of the agent’s output.\n\n\n\n\n\n\nFigure 29.1: Figure taken from SciCode (Tian et al. 2024) illustrating how the main task and subtasks are described to the agents.\n\n\n\nInterestingly, by the time of the paper, the authors had already evaluated several LLMs, including OpenAI’s GPT-4o and Anthropic’s Claude 3.5 Sonnet. The results show that the models struggle in general to reproduce the scripts, with a success rate lower than 5% for all models for the main tasks, and around 25% the best results for reproducing subtasks.\n\n\nMLAgentBench: Evaluating Language Agents on Machine Learning Experimentation\nMLAgentBench (Huang et al. 2023) is a benchmark designed to evaluate the ability of LLM-powered agents to reproduce machine learning experiments. The benchmark consists of a set of tasks that expands around Kaggle competitions and other ML-related tasks. Each of the tasks includes their own evaluator, which can be checking if the Kaggle submission is correct or if the proposed code improves the predictive performance of the model. \nFor each of the tasks they provide the agents with some initial files and a task description and the agent has to build from there. One of the problems of this paper is that they only evaluated one agent (only ReAct) and only thirteen tasks which limits the evaluation. \nThe results vary a lot between the different tasks despite being averaged over 8 trials (pass@k). While for some the score is perfect, for others the score is 0.\n\n\nMLE-Bench: Evaluating Machine Learning Agents on Machine Learning Engineering\nChen et al. (Chan et al. 2024) in previous work from OpenAI focused in the Kaggle competition as the origin of tasks, and curate 75 machine-learning related tasks from there. To improve the evaluation, they also run the human baseline that helps to evaluate the performance of the agents. Similarly as the proposed in MLAgentBench for the Kaggle tasks, the agents need to provide a submission according to the Kaggle rules. Thus, each of the tasks provides the agents and humans with the description of the task and the competition dataset. Interestingly, the authors also use a source code plagiarism detection tool to check that the code that either the models or the humans submitted is not plagiarized from the original Kaggle competition.\n\n\n\nTable 29.1: Accuracy in submitting and winning a medal (top submission)\n\n\n\n\n\n\n\n\n\n\n\n\nModel\nMade Submission (%)\nValid Submission (%)\nGold (%)\nAny Medal (%)\n\n\n\n\nAIDE\n\n\n\n\n\n\no1-preview\n98.4 ± 0.4\n82.8 ± 1.1\n9.4 ± 0.8\n16.9 ± 1.1\n\n\ngpt-4o-2024-08-06\n70.7 ± 0.9\n54.9 ± 1.0\n5.0 ± 0.4\n8.7 ± 0.5\n\n\nllama-3.1-405b-instruct\n46.3 ± 2.9\n27.3 ± 2.6\n1.7 ± 0.7\n3.0 ± 1.0\n\n\nclaude-3.5-sonnet-20240620\n68.9 ± 3.1\n51.1 ± 3.3\n4.4 ± 1.4\n7.6 ± 1.8\n\n\nMLAB\n\n\n\n\n\n\ngpt-4o-2024-08-06\n65.6 ± 2.5\n44.3 ± 2.6\n0.8 ± 0.5\n0.8 ± 0.5\n\n\nOpenHands\n\n\n\n\n\n\ngpt-4o-2024-08-06\n59.1 ± 3.3\n52.0 ± 3.3\n2.7 ± 1.1\n4.4 ± 1.4\n\n\n\n\n\n\nThe results show that the models can prepare a correct submission in a good amount of times. However, of all of these times, only a small fraction—around 15% for the best agent—win a medal in the competition. While this results show promising results, still they struggle to produce high-quality submissions.\n\n\nCore-Bench\nCore-Bench is a benchmark designed to evaluate the ability of LLM-powered agents to reproduce scientific results. The benchmark consists of a set of tasks that require the agent to read and understand a research paper and its codebase, extract relevant information, and implement the proposed methods.\n\n\n\n\n\n\nFigure 29.2: Figure taken from Core Bench (Siegel et al. 2024) illustrating how each task is organized.\n\n\n\nFor a more in depth evaluation, they divided the benchmark by levels of difficulty in easy, medium, and hard. The differences between the different levels are the amount of information provided to the agent. For the easy level, the agent is provided with the results of executing the code. For the medium level, the agent is provided with a dockerfile that can be used to reproduce the results. Finally, for the hard level, only a README file is provided and the agent has to create and run the dockerfile by itself.\nCore Bench design allows the agents to have access to the codebases. Thus the agents can read the code and understand how it works resulting in a benchmark that is more focused in code understanding that in code reproduction. To focus on the second Starace et al. proposed PaperBench (Starace et al. 2025).\nRefer to Siegel et al. (2024) for further discussion on Core Bench.",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>PaperBench: Evaluating AI’s Ability to Replicate AI Research</span>"
    ]
  },
  {
    "objectID": "papers/paper_bench_2025.html#paperbench-problem-setting",
    "href": "papers/paper_bench_2025.html#paperbench-problem-setting",
    "title": "PaperBench: Evaluating AI’s Ability to Replicate AI Research",
    "section": "PaperBench problem setting",
    "text": "PaperBench problem setting\n\nReproducing existing work can be a challenging and interesting task for LLM-based agents.\nGrading the replication attemps can be really time-consuming.\nAt the same time, if you use an LLM to grade, how do you evaluate the LLM?",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>PaperBench: Evaluating AI’s Ability to Replicate AI Research</span>"
    ]
  },
  {
    "objectID": "papers/paper_bench_2025.html#approach",
    "href": "papers/paper_bench_2025.html#approach",
    "title": "PaperBench: Evaluating AI’s Ability to Replicate AI Research",
    "section": "Approach",
    "text": "Approach\nThe authors compile a dataset of 20 articles published as Spotlight and Oral papers from ICML 2024. For each of the papers they compile a set of rubrics that are used to evaluate the replication attempts. Each of the rubrics is structured as a hierarchical tree, with the top-level nodes representing the main categories of evaluation and the lower-level or leaf-nodes nodes representing the specific criteria within each category. This decomposition allows to do a very robust scoring which allows a more insightful evaluation of the agents. \n\nOverall workflow\nFor each of the tasks, the agents are given with the original paper to replicate and some clarifications compiled by the authors of the paper to better understand what was done in the paper. To avoid reward hacking, the agents are not allowed to see the rubric used to evaluate the replication attempts. The agents are asked to produce a reproduce.sh script that when run reproduces the results of the paper. To check if the results are correctly reproduced, the produced script is run in a fresh environment in a virtual machine with access to one GPU. The agent is not constrained in time and it has $1000 of OpenAI credits and a HuggingFace API key.\n\n\n\nTable 29.2: Evaluation requirements for each file type in PaperBench\n\n\n\n\n\n\nCode Dev.\nExecution\nRes. Match\n\n\n\n\nREADMEs & Docs\n✓\n✓\n✓\n\n\nSource code\n✓\n✓\n✗\n\n\nreproduce.sh\n✓\n✓\n✓\n\n\nreproduce.log\n✗\n✓\n✓\n\n\nRepro outputs\n✗\n✗\n✓\n\n\n\n\n\n\nThe grading for each of the tasks is done by evaluating each of the leaf nodes of the rubric. Depending on the leaf node, there are different evaluation requirements:\n\nResult Match: in these leaf nodes is checked if the specific results are reproduced.\nExecution: if the script is executable and runs without errors.\nCode Development: checks if the code seems to can reproduce the results.\n\n\n\n\n\n\n\nFigure 29.3: Figure from PaperBench (Starace et al. 2025) explaining how the rubric evaluation is done.\n\n\n\nThe evaluation following the rubric is done by checking each of the leaf nodes. The score in the leaf nodes is always binary, either 0 for not matching or 1 for matching. The score in the leaf nodes is then aggregated to the parent node, and so on until the root node resulting in very descriptive score for each of the tasks.\n\n\nRules\nThe agent is allowed to do almost anything, including:\n\nBrowsing the web.\nNo computing constraints.\n\nHowever, there are two limitations. The first is all API keys need to be provided to the agents. Second, the agents, despite they can browse the web, have a black list of websites that they are not allowed to use. This is done to avoid the agents to check the original codebases of the papers or related replications.\n\n\nDev PaperBench\nSince the entire benchmark is expensive to run, the authors also compile a smaller version of the benchmark called PaperBench Code-Dev which consideraly reduces the costs. However, the results in this benchmark are not directly comparable to the full PaperBench benchmark.\n\n\nLLM Judge\nSince they arrived at such large rubrics, the human evaluation became non-scalable during the first stages of the study. Therefore, they proposed to evaluate using LLMs as judges. However, for this evaluation to be robust, the accuracy needs to be assesed (Shankar et al. 2024). To do so, they compile a set of partial modified replications of the articles, and manually graded them using the rubrics. The authors then use these scores to build JudgeEval which is a benchmark for evaluating the performance of LLMs as judges.\n\n\n\nTable 29.3: Results of the JudgeEval benchmark.\n\n\n\n\n\n\nACC.\nPREC.\nREC.\nF1\nCOST\n\n\n\n\nRANDOM\n0.48\n0.49\n0.49\n0.49\n0\n\n\nSIMPLEJUDGE\n\n\n\n\n\n\n\nGPT-4o-mini\n0.63\n0.64\n0.60\n0.59\n8\n\n\nGPT-4o\n0.74\n0.74\n0.72\n0.73\n120\n\n\no1-mini\n0.81\n0.85\n0.76\n0.78\n72\n\n\no1\n0.84\n0.84\n0.84\n0.84\n830\n\n\no3-mini\n0.83\n0.83\n0.83\n0.83\n66\n\n\n\n\n\n\nBy evaluating different leading LLMs, the authors show that the performance of the LLMs as judges is good and they can be trusted as evaluators. The best model in terms of accuracy versus cost is the o3-mini model. However, despite this being the best model, the authors report a cost of $66 per task. While the cost reduces to only $10 in the PaperBench Code-Dev benchmark, this is still a very expensive evaluation.",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>PaperBench: Evaluating AI’s Ability to Replicate AI Research</span>"
    ]
  },
  {
    "objectID": "papers/paper_bench_2025.html#results",
    "href": "papers/paper_bench_2025.html#results",
    "title": "PaperBench: Evaluating AI’s Ability to Replicate AI Research",
    "section": "Results",
    "text": "Results\nThey evaluate the performance of some of the leading models using a simple agent scaffolding that uses the available tools until the agent decides to end the run. The tools available to the agent are: a bash shell command execution tool, a Python code execution tool, a web browser tool, and a paginated file reader tool for reading long documents.\n\n\n\nTable 29.4: Average Replication Scores (%) of the evaluation of PaperBench benchmark.\n\n\n\n\n\nModel\nPaperBench\n\n\n\n\no3-mini-high\n2.6 ± 0.2\n\n\nGPT-4o\n4.1 ± 0.1\n\n\nGemini-2.0-Flash\n3.2 ± 0.2\n\n\nDeepSeek-R1\n6.0 ± 0.3\n\n\no1-high\n13.2 ± 0.3\n\n\nClaude-3.5-Sonnet\n21.0 ± 0.8\n\n\nIterativeAgent\n\n\n\no3-mini-high\n8.5 ± 0.8\n\n\nClaude-3.5-Sonnet\n16.1 ± 0.1\n\n\no1-high\n24.4 ± 0.7\n\n\nIterativeAgent with an extended 36 hour limit\n\n\n\no1-high\n24.4 ± 0.7\n\n\n\n\n\n\nThe results show that the best model is (ironically) Claude 3.5 Sonnet, with an average replication score of 21.0% in the basic agent scaffolding. The authors claim that by manual inspection of the log files, the most common failure case was due to early termination by the agents, while some other model such as o3-mini struggle with tool usage. These results show the weakness of the current LLMs in carrying long-horizon tasks.\nSince in the basic agent implementation the models tend to finish prematurely, the authors propose the IterativeAgent which consists of forcing the agent to run the full available time by removing their ability to finishing earlier and uses prompts to encourage the agents to work in a step-based manner. By using this agent, while the scores decrease for Claude 3.5 Sonnet, o1-high model improved more than 10% that gets increased another 2% by extending the time limit to 36 hours.\n\nHuman Baseline Performance\nThe authors also evaluate the performance of humans in the benchmark. For that they recruited 8 PhDs in machine learning. To evaluate the participands, they provided them with the same resources as the agents, including the original paper and the clarifications, and access to the same computing resources. The humans were allowed to work during four weeks in the tasks using tracked hours to compare them with the agents.\n\n\n\n\n\n\nFigure 29.4: Figure from PaperBench (Starace et al. 2025) comparing humans and agents.\n\n\n\nThe results show that while the agents clearly outperform the humans during the first hours, the humans are able to catch up and even outperform the agents after 24 hours. This is mainly due to the fact that the agents are not able to improve significantly their performance after the first hour of work.",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>PaperBench: Evaluating AI’s Ability to Replicate AI Research</span>"
    ]
  },
  {
    "objectID": "papers/paper_bench_2025.html#takeaways",
    "href": "papers/paper_bench_2025.html#takeaways",
    "title": "PaperBench: Evaluating AI’s Ability to Replicate AI Research",
    "section": "Takeaways",
    "text": "Takeaways\n\nCurrent models still fail to follow long-horizon tasks.\nThe use of LLMs as judges is a promising approach to evaluate the performance of LLM-powered agents.\nReproducing existing work is a challenging task for LLM-powered agents (and can result in very high cost attemps).",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>PaperBench: Evaluating AI’s Ability to Replicate AI Research</span>"
    ]
  },
  {
    "objectID": "papers/paper_bench_2025.html#references",
    "href": "papers/paper_bench_2025.html#references",
    "title": "PaperBench: Evaluating AI’s Ability to Replicate AI Research",
    "section": "References",
    "text": "References\n\n\n\n\nChan, Jun Shern, Neil Chowdhury, Oliver Jaffe, James Aung, Dane Sherburn, Evan Mays, Giulio Starace, et al. 2024. “MLE-Bench: Evaluating Machine Learning Agents on Machine Learning Engineering.” arXiv Preprint arXiv: 2410.07095.\n\n\nGuo, Jeff, and Philippe Schwaller. 2024. “It Takes Two to Tango: Directly Optimizing for Constrained Synthesizability in Generative Molecular Design.” arXiv. https://doi.org/10.48550/ARXIV.2410.11527.\n\n\nHuang, Qian, Jian Vora, Percy Liang, and J. Leskovec. 2023. “MLAgentBench: Evaluating Language Agents on Machine Learning Experimentation.” International Conference on Machine Learning.\n\n\nLandrum, Gregory A., Jessica Braun, Paul Katzberger, Marc T. Lehner, and Sereina Riniker. 2024. “Lwreg: A Lightweight System for Chemical Registration and Data Storage.” Journal of Chemical Information and Modeling 64 (16): 6247–52. https://doi.org/10.1021/acs.jcim.4c01133.\n\n\nLu, Chris, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, and David Ha. 2024. “The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery.” arXiv Preprint arXiv: 2408.06292.\n\n\nOrsi, Markus, and Jean-Louis Reymond. 2024. “One Chiral Fingerprint to Find Them All.” Journal of Cheminformatics 16 (1): 53.\n\n\nShankar, Shreya, J. D. Zamfirescu-Pereira, Bjorn Hartmann, Aditya G. Parameswaran, and Ian Arawjo. 2024. “Who Validates the Validators? Aligning LLM-Assisted Evaluation of LLM Outputs with Human Preferences.” ACM Symposium on User Interface Software and Technology. https://doi.org/10.48550/arXiv.2404.12272.\n\n\nSiegel, Zachary S., Sayash Kapoor, Nitya Nagdir, Benedikt Stroebl, and Arvind Narayanan. 2024. “CORE-Bench: Fostering the Credibility of Published Research Through a Computational Reproducibility Agent Benchmark.” https://arxiv.org/abs/2409.11363.\n\n\nStarace, Giulio, Oliver Jaffe, Dane Sherburn, James Aung, Jun Shern Chan, Leon Maksin, Rachel Dias, et al. 2025. “PaperBench: Evaluating AI’s Ability to Replicate AI Research.” arXiv Preprint arXiv: 2504.01848.\n\n\nTian, Minyang, Luyu Gao, Shizhuo Dylan Zhang, Xinan Chen, Cunwei Fan, Xuefei Guo, Roland Haas, et al. 2024. “SciCode: A Research Coding Benchmark Curated by Scientists.” Neural Information Processing Systems. https://doi.org/10.48550/arXiv.2407.13168.",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>PaperBench: Evaluating AI’s Ability to Replicate AI Research</span>"
    ]
  }
]
[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Lamalab Tool and Paper Notes",
    "section": "",
    "text": "In our group seminars, we have a tradition of dedicating a few minutes to showcase tools/software/tricks/methods that we find useful. This repository is a collection of these tool minutes.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Tool and paper minutes</span>"
    ]
  },
  {
    "objectID": "tools/hydra.html",
    "href": "tools/hydra.html",
    "title": "Hydra",
    "section": "",
    "text": "Getting started\nHydra is an open-source Python framework that simplifies the development of research and other complex applications. The key feature is the ability to dynamically create a hierarchical configuration by composition and override it through config files and the command line. The name Hydra comes from its ability to run multiple similar jobs - much like a Hydra with multiple heads.",
    "crumbs": [
      "Tools",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Hydra</span>"
    ]
  },
  {
    "objectID": "tools/hydra.html#getting-started",
    "href": "tools/hydra.html#getting-started",
    "title": "Hydra",
    "section": "",
    "text": "Key features:\n\nHierarchical configuration composable from multiple sources\nConfiguration can be specified or overridden from the command line\nDynamic command line tab completion\nRun your application locally or launch it to run remotely\nRun multiple jobs with different arguments with a single command\n\n\n\nInstallation\npip install hydra-core --upgrade\n\n\nBasic example\nConfig, e.g., in conf/config.yaml:\ndb:\ndriver: mysql\nuser: omry\npass: secret",
    "crumbs": [
      "Tools",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Hydra</span>"
    ]
  },
  {
    "objectID": "tools/ip_rotator.html",
    "href": "tools/ip_rotator.html",
    "title": "IP Rotator",
    "section": "",
    "text": "GitHub repository\niq-requests-rotator",
    "crumbs": [
      "Tools",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>IP Rotator</span>"
    ]
  },
  {
    "objectID": "tools/ip_rotator.html#github-repository",
    "href": "tools/ip_rotator.html#github-repository",
    "title": "IP Rotator",
    "section": "",
    "text": "Example usage:\nimport requests\nfrom requests_ip_rotator import ApiGatewaywith ApiGateway(\"https://site.com\") as g:\n    session = requests.Session()\n    session.mount(\"https://site.com\", g)    response = session.get(\"https://site.com/index.php\")\n    print(response.status_code)",
    "crumbs": [
      "Tools",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>IP Rotator</span>"
    ]
  },
  {
    "objectID": "tools/polars.html",
    "href": "tools/polars.html",
    "title": "Polars",
    "section": "",
    "text": "An alternative to pandas\nThe advantages of polars can be directly seen in the image above. It is clear from the graph that Polars perform faster than Pandas for most operations. This is particularly true for the GroupBy operation, where Polars is nearly 20 times faster than Pandas. The Filter operation is also significantly faster in Polars, while Create operations are somewhat faster in Pandas. Overall, Polars seems to be a more performant library for data manipulation, particularly for large datasets.",
    "crumbs": [
      "Tools",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Polars</span>"
    ]
  },
  {
    "objectID": "tools/polars.html#syntax-example",
    "href": "tools/polars.html#syntax-example",
    "title": "Polars",
    "section": "Syntax example",
    "text": "Syntax example\nimport polars as pl\n\nq = (\n    pl.scan_csv(\"docs/data/iris.csv\")\n    .filter(pl.col(\"sepal_length\") &gt; 5)\n    .group_by(\"species\")\n    .agg(pl.all().sum())\n)\n\ndf = q.collect()",
    "crumbs": [
      "Tools",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Polars</span>"
    ]
  },
  {
    "objectID": "tools/thunder_client.html",
    "href": "tools/thunder_client.html",
    "title": "Thunder Client",
    "section": "",
    "text": "Installation\nInstall the Thunder client extension from the marketplace.",
    "crumbs": [
      "Tools",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Thunder Client</span>"
    ]
  },
  {
    "objectID": "tools/tmux.html",
    "href": "tools/tmux.html",
    "title": "tmux",
    "section": "",
    "text": "Installation\nor on Mac",
    "crumbs": [
      "Tools",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>tmux</span>"
    ]
  },
  {
    "objectID": "tools/tmux.html#installation",
    "href": "tools/tmux.html#installation",
    "title": "tmux",
    "section": "",
    "text": "sudo apt install tmux\n\nbrew install tmux",
    "crumbs": [
      "Tools",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>tmux</span>"
    ]
  },
  {
    "objectID": "tools/tmux.html#usage",
    "href": "tools/tmux.html#usage",
    "title": "tmux",
    "section": "Usage",
    "text": "Usage\nLet‚Äôs assume you are via ssh on a remote server and you want to run a long running process. You can use tmux to run the process in a session and then detach from it. You can then log out and log back in later to check on the process. Your process will still be running, even if your ssh session is closed.\n\nOn the remote server\ntmux new -s myprocess\nThen run your process. When you are done, detach from the session by pressing Ctrl+b and then d.\n\n\nOn the remote server later\ntmux ls\nThis will list all the sessions. You can then reattach to the session you want by typing:\ntmux attach -t myprocess\n\n\nPanes\nYou can split your terminal into panes. This is useful if you want to run multiple processes in the same terminal. You can split the terminal vertically by pressing Ctrl+b and then \" or horizontally by pressing Ctrl+b and then %.\nTo move panes around, you can use Ctrl+b and then o to cycle through the panes.",
    "crumbs": [
      "Tools",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>tmux</span>"
    ]
  },
  {
    "objectID": "tools/trimean.html",
    "href": "tools/trimean.html",
    "title": "Robust statistics and Trimean",
    "section": "",
    "text": "from scipy.stats import skewnorm\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nLet‚Äôs generate some data that might be something we find in the real world.\n\nskew_magnitude = -6\narr = skewnorm.rvs(skew_magnitude, loc=0, scale=1000, size=100000)\n\n(The skew is a third-order moment.)\n\nplt.hist(arr, bins=100, density=True)\nplt.show()\n\n\n\n\n\n\n\n\nLet‚Äôs get a very common measure of central tendency:\n\nnp.mean(arr)\n\n-789.5809069979605\n\n\nThe mean overstates the central tendency because of the skew.\nThe mean is defined as\n\\[\\bar{x} = \\frac{1}{n} \\sum_{i=1}^n x_i\\]\nand treats all numbers equally. No matter how big or small.\nOne can ‚Äúfix‚Äù this by looking at ‚Äúrobust‚Äù statistics that are often rank based. Rank based means that we sort the data and then base our statistics on the rank of the data. In this way, they are no longer sensitive to outliers.\n\ndef interquartile_range(arr):\n    q1 = np.percentile(arr, 25)\n    q3 = np.percentile(arr, 75)\n    return q3 - q1\n\nprint(\"Median\", np.percentile(arr, 50))\nprint(\"Interquartile Range\", interquartile_range(arr))\nprint(\"Mean\", arr.mean())\nprint(\"Standard Deviation\", arr.std())\n\nMedian -679.7024551978025\nInterquartile Range 834.2816858677052\nMean -789.5809069979605\nStandard Deviation 614.9363837309692\n\n\nA very nice measure of centrality is the so-called trimean.\n\n‚ÄúAn advantage of the trimean as a measure of the center (of a distribution) is that it combines the median‚Äôs emphasis on center values with the midhinge‚Äôs attention to the extremes.‚Äù\n‚Äî‚ÄâHerbert F. Weisberg, Central Tendency and Variability\n\nIt is defined as\n\\[\n\\text{trimean} = \\frac{Q_1 + 2Q_2 + Q_3}{4}\n\\]\nwhere \\(Q_1\\) is the first quartile, \\(Q_2\\) is the median, and \\(Q_3\\) is the third quartile.\n\ndef trimean(arr):\n    q1 = np.percentile(arr, 25)\n    q3 = np.percentile(arr, 75)\n    median = np.percentile(arr, 50)\n    return (q1 + 2*median + q3)/4\n\nprint(\"Trimean\", trimean(arr))\n\nTrimean -708.4430042323374",
    "crumbs": [
      "Tools",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Robust statistics and Trimean</span>"
    ]
  },
  {
    "objectID": "tools/pandarallel.html",
    "href": "tools/pandarallel.html",
    "title": "Easy fast .apply for pandas",
    "section": "",
    "text": "apply in pandas is slow. This is the case because it does not take advantage of vectorization. That means, in general, if you have something for which there is a built-in pandas (or numpy) function, you should use that instead of apply, because those functions will be optimized and typically vectorized.\nThe pandarallel package allows you to parallelize apply on a pandas DataFrame or Series object. It does this by using multiprocessing. However, since it uses multiple processes, it will use more memory than a simple apply.\nIf your data just barley fits in memory, you should not use pandarallel. However, if it does fit in memory, and you have a lot of cores, then pandarallel can speed up your code significantly with just changing one line of code.\n\nfrom pandarallel import pandarallel\n\npandarallel.initialize(progress_bar=True)\n\n# df.apply(func)\ndf.parallel_apply(func)",
    "crumbs": [
      "Tools",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Easy fast `.apply` for pandas</span>"
    ]
  },
  {
    "objectID": "tools/bfg.html",
    "href": "tools/bfg.html",
    "title": "BFG Repo-Cleaner",
    "section": "",
    "text": "If you did not take with your .gitignore or just used git add . you might have by accident committed large files. This might lead to an error like\nremote: error: See https://gh.io/lfs for more information.\nremote: error: File reports/gemini-pro/.langchain.db is 123.01 MB; this exceeds GitHub's file size limit of 100.00 MB\nremote: error: GH001: Large files detected. You may want to try Git Large File Storage - https://git-lfs.github.com.\nTo github.com:lamalab-org/chem-bench.git\n ! [remote rejected]     kjappelbaum/issue258 -&gt; kjappelbaum/issue258 (pre-receive hook declined)\nerror: failed to push some refs to 'github.com:lamalab-org/chem-bench.git'\nTo fix this, you need to remove the large files. A convenient tool for doing this is BFG.\nOnce you download the file you can run it using something like\njava -jar ~/Downloads/bfg-1.14.0.jar --strip-blobs-bigger-than 100M --no-blob-protection\nto remove large files.\nNote that this here uses --no-blob-protection as BFG defaults to not touching the last commit.\nAfter the BFG run, it will prompt you to run something like\ngit reflog expire --expire=now --all && git gc --prune=now --aggressive",
    "crumbs": [
      "Tools",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>BFG Repo-Cleaner</span>"
    ]
  },
  {
    "objectID": "tools/showyourwork.html",
    "href": "tools/showyourwork.html",
    "title": "showyourwork",
    "section": "",
    "text": "showyourwork : https://github.com/showyourwork is a framework for building reproducible papers. The package works on a combination of Tex and Python code, where you can on the fly modify your plots.\nThe pre-requisites are: 1. define a conda environment with the packages are that necessary for plotting 2. use the \\script{}, \\variable{} and other commands to link your figures/tables to a Python script. 3. compile the paper",
    "crumbs": [
      "Tools",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>showyourwork</span>"
    ]
  },
  {
    "objectID": "papers/llm4mat.html",
    "href": "papers/llm4mat.html",
    "title": "Leveraging language representation for materials exploration and discovery",
    "section": "",
    "text": "Why discussing this paper?\nI chose Jiaxing et al.‚Äôs paper for our journal club because",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Leveraging language representation for materials exploration and discovery</span>"
    ]
  },
  {
    "objectID": "papers/llm4mat.html#why-discussing-this-paper",
    "href": "papers/llm4mat.html#why-discussing-this-paper",
    "title": "Leveraging language representation for materials exploration and discovery",
    "section": "",
    "text": "LLMs successfully applied in other domain, interesting to see what can be done in material science\nOne among the few paper where LLMs are applied in materials science for actual material discovery.\nNice embedding figures",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Leveraging language representation for materials exploration and discovery</span>"
    ]
  },
  {
    "objectID": "papers/llm4mat.html#context",
    "href": "papers/llm4mat.html#context",
    "title": "Leveraging language representation for materials exploration and discovery",
    "section": "Context",
    "text": "Context\n\nMaterial space is not completely explored. And there is possibility of finding better materials in many applications.\nML recommender systems for exploring material spaces are already there but not many using ‚ÄúLLMs‚Äù\nLLM framework for recommending prototype crystal structures and later validate through first-principles calculations and experiments\nWhy LLMs? - Universal task agnostic representations",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Leveraging language representation for materials exploration and discovery</span>"
    ]
  },
  {
    "objectID": "papers/llm4mat.html#some-previous-llm-models",
    "href": "papers/llm4mat.html#some-previous-llm-models",
    "title": "Leveraging language representation for materials exploration and discovery",
    "section": "Some Previous LLM Models",
    "text": "Some Previous LLM Models\n\nMatSciBERT\nMatsciBERT was pretrained on whole sections of more than 1 million materials science articles with masked language modelling.\n\n\nMatBERT\nMatBERT was trained by sampling 50 million paragraphs from 2 million articles masked language modelling.\n\n\nWord2Vec\nMat2Vec was trained similarly as Word2vec training through skip-gram with negative sampling. Each word is embedded into a 200-dimensional vector.",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Leveraging language representation for materials exploration and discovery</span>"
    ]
  },
  {
    "objectID": "papers/llm4mat.html#problem-setting",
    "href": "papers/llm4mat.html#problem-setting",
    "title": "Leveraging language representation for materials exploration and discovery",
    "section": "Problem setting",
    "text": "Problem setting\n\nHand-crafted features and specialized structural models have limitations in providing universal and task-agnostic representations within the vast material space.\nAdditional contexts are also very useful. for eg: (doping, temperature, synthesis conditions)\nIn materials exploration and discovery context:\n\n\neffective representations of both chemical and structural complexity, (ii) successful recall of relevant candidates to property of interest\naccurate candidate ranking based on multiple desired functional properties.",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Leveraging language representation for materials exploration and discovery</span>"
    ]
  },
  {
    "objectID": "papers/llm4mat.html#approach",
    "href": "papers/llm4mat.html#approach",
    "title": "Leveraging language representation for materials exploration and discovery",
    "section": "Approach",
    "text": "Approach\nThe authors propose a two-step funnel based approach\n\n\nRECALL - Given a material finding similar material from a set of materials\n\n\nRANKING - Based on functional properties rank the recalled materials\n\n\n\n\n\nFunnel based recommender framework\n\n\n\nRecalling similar materials\nThe authors use Robocrystallographer representation to describe the material. Encode the material description using pretrained MatBERT (compared other encoders as well), and use this as a feature vector\n\nUse a Query material (a well studied known material with property of interest).\nEncode all material in database and Query material (Robocrystallographer + MatBERT)\nLook at cosine similarity of feature vectors (material in database with Query material)\n\n\n\n\nRecall material based on cosine similarity\n\n\n\n\nRanking potential materials\nBased on multiple properties the recalled materials are ranked.\nUsually for any application, and in this paper, for thermoelectric material as well many properties are important hence a ranker based on performance on different functional aspects.\n\nAuthor train a Multitask Mixture of Expert Model (MMoEM), using multitask learning to rank the materials.\n\n\n\n\nRank material based on cosine similarity",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Leveraging language representation for materials exploration and discovery</span>"
    ]
  },
  {
    "objectID": "papers/llm4mat.html#results",
    "href": "papers/llm4mat.html#results",
    "title": "Leveraging language representation for materials exploration and discovery",
    "section": "Results",
    "text": "Results\nThe authors perform ablations to understand the importance of the different components of their model. While there are some differences, the differences are not drastic.\n\nAblation on Representation suitable for RECALL step\nTwo set of models 1. Uses only composition of materials Baseline: Mat2Vec\nA(Composition)‚Äì&gt; B(MatBERT)\n\nUses Both composition and structure Baseline: CrystalNN Fingerprint\n\nA(Material)‚Äì&gt; B(RoboCrystallographer)‚Äì&gt; C(MatBERT)\n\nEmbeddings from composition only and Composition + Structure\nStructure level representations exhibit more distinct separation (well-defined domains) by material groups\n\n\n\nUMAP of embeddings from different representations\n\n\nFor further evaluation, authors evaluated material embedding performance on downstream property prediction tasks.\nThe task models were multi-layer perceptrons (MLPs) with meanabsolute-error (MAE) training loss.\nThe tasks consisted of band gap, energy per atom, bulk modulus, shear modulus, Debye temperature, and coefficient of thermal expansion from AFLOW dataset.\n\n\n\nProperty prediction using embeddings from different representations\n\n\n\n\n\nFinding similar materials\nStarting with known materials with favorable properties for TEs such as PbTe, we analyzed the top recalled candidates and found significantly different predicted figure-of-merit zT distributions from selected baseline representations.\n\n\n\nDistributions of predicted zT of the top-100 recalled candidates for PbTe as the query material predicted by MatBERT\n\n\n\n\nRanking potential materials\nLearning from multiple related tasks provides superior performance over single-task learning by modeling task-specific objectives and cross-task relationships.\nIn addition to the embeddings derived from language models, the authors added further information based on context (one hot encoded temperature)\n\n\n\nMulti-task learning framework for material property prediction.\n\n\n\n\n\nPerformance for 6 material property prediction tasks between single-task models and MMoE using composition or structure embeddings.",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Leveraging language representation for materials exploration and discovery</span>"
    ]
  },
  {
    "objectID": "papers/llm4mat.html#takeaways",
    "href": "papers/llm4mat.html#takeaways",
    "title": "Leveraging language representation for materials exploration and discovery",
    "section": "Takeaways",
    "text": "Takeaways\n\nMight not need a Language model for this task\nGood to see that some of the materials where later tested in lab\nComposition vs Composition + structure not convincing.",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Leveraging language representation for materials exploration and discovery</span>"
    ]
  },
  {
    "objectID": "papers/chem_yield_prediction_2024.html",
    "href": "papers/chem_yield_prediction_2024.html",
    "title": "Uncertainty-Aware Yield Prediction with Multimodal Molecular Features",
    "section": "",
    "text": "Why discussing this paper?\nI chose Chen et al.‚Äôs paper (Chen et al. 2024) for our journal club because",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Uncertainty-Aware Yield Prediction with Multimodal Molecular Features</span>"
    ]
  },
  {
    "objectID": "papers/chem_yield_prediction_2024.html#why-discussing-this-paper",
    "href": "papers/chem_yield_prediction_2024.html#why-discussing-this-paper",
    "title": "Uncertainty-Aware Yield Prediction with Multimodal Molecular Features",
    "section": "",
    "text": "An important and interesting problem in chemistry\nUses many of the techniques we care about in our group",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Uncertainty-Aware Yield Prediction with Multimodal Molecular Features</span>"
    ]
  },
  {
    "objectID": "papers/chem_yield_prediction_2024.html#context",
    "href": "papers/chem_yield_prediction_2024.html#context",
    "title": "Uncertainty-Aware Yield Prediction with Multimodal Molecular Features",
    "section": "Context",
    "text": "Context\nPredicting the yield of chemical reactions is a crucial task in organic chemistry. It can help to optimize the synthesis of new molecules, reduce the number of experiments needed, and save time and resources. However, predicting the yield of a reaction is challenging due to the complexity of chemical reactions and the large number of factors that can influence the outcome.",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Uncertainty-Aware Yield Prediction with Multimodal Molecular Features</span>"
    ]
  },
  {
    "objectID": "papers/chem_yield_prediction_2024.html#prior-work",
    "href": "papers/chem_yield_prediction_2024.html#prior-work",
    "title": "Uncertainty-Aware Yield Prediction with Multimodal Molecular Features",
    "section": "Prior work",
    "text": "Prior work\n\nAhneman et al.¬†(2018)\nAhneman et al. (Ahneman et al. 2018) reported in Science a random forest model that predicts the yield of chemical reactions in a high-throughput dataset (palladium-catalyzed Buchwald-Hartwig cross-coupling reactions). For this, the authors created a set of features using computational techniques.\nA very interesting aspect of this work is the subsequent exchange with Chuang and Keiser (Chuang and Keiser 2018) who point out that the chemical features used in the work by Ahneman et al.¬†perform not distinguishably better than non-meaningful features.\n\n\n\nFigure taken from Chuang and Keiser‚Äôs paper(Chuang and Keiser 2018) illustrating models trained with various featurization approaches.\n\n\n\n\nSchwaller et al.¬†(2020, 2021)\nSchwaller et al. (Schwaller et al. 2020, 2021) utilized BERT models with a regression head to predict yields based on reaction SMILES.\nThey observed multiple interesting effects:\n\nThe performance on high-throughput datasets is good, on USPTO datasets the models are not predictive (\\(R^2\\) on a random split of 0.117 for the gram scale)\nThe yield distribution depends on the scale, which might be due to reaction at larger scale being better optimized\n\n\n\n\nFigure taken from Schwaller et al. (Schwaller et al. 2021) illustrating the distribution of yields on different scales.\n\n\n\n\nKwon et al.¬†(2022)\nKwon et al. (Kwon et al. 2022), in contrast, used graph neural networks to predict yields. They pass reactants and products through a graph neural network and concatenate the embeddings to predict the yield. They train on a similar loss as the work at hand (but use also use dropout Monte-Carlo (Gal and Ghahramani 2016) to estimate the epistemic uncertainty).",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Uncertainty-Aware Yield Prediction with Multimodal Molecular Features</span>"
    ]
  },
  {
    "objectID": "papers/chem_yield_prediction_2024.html#problem-setting",
    "href": "papers/chem_yield_prediction_2024.html#problem-setting",
    "title": "Uncertainty-Aware Yield Prediction with Multimodal Molecular Features",
    "section": "Problem setting",
    "text": "Problem setting\n\nprior works perform well on high-throughput datasets but not on real-world datasets\nthis is partially due to a lot of noise in datasets\nof course, reaction conditions are important, too\n\nAdditionally, the authors propose that the previous representations might not be ‚Äúrich‚Äù enough to capture the complexity of chemical reactions.",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Uncertainty-Aware Yield Prediction with Multimodal Molecular Features</span>"
    ]
  },
  {
    "objectID": "papers/chem_yield_prediction_2024.html#approach",
    "href": "papers/chem_yield_prediction_2024.html#approach",
    "title": "Uncertainty-Aware Yield Prediction with Multimodal Molecular Features",
    "section": "Approach",
    "text": "Approach\nThe authors propose to fuse multiple features. In addition, they also use a special loss function and a mixture of experts (MoE) model used to transform human-designed features.\n\n\n\nOverview of the model architecture. Figure taken from Chem et al. (Chen et al. 2024)\n\n\n\nGraph encoder and SMILES encoder\nThe authors pretrain the graph and SMILES encoders using a contrastive loss. The graph encoder is a GNN, the SMILES encoder is a transformer.\n\nGraph convolutional neural network\nTheir graph encoder is basically a message graph convolutional neural network. The authors use the DGL library to implement this.\nThe forward pass looks like this:\nfor _ in range(self.num_step_message_passing):\n    node_feats = self.activation(self.gnn_layer(g, node_feats, edge_feats)).unsqueeze(0)\n    node_feats, hidden_feats = self.gru(node_feats, hidden_feats)\n    node_feats = node_feats.squeeze(0)\nWhere the GNN layer performs a simple operation such as\n\\[\n\\mathbf{x}_i^{\\prime}=\\boldsymbol{\\Theta}^{\\top} \\sum_{j \\in \\mathcal{N}(i) \\cup\\{i\\}} \\frac{e_{j, i}}{\\sqrt{\\hat{d}_j \\hat{d}_i}} \\mathbf{x}_j\n\\]\nwhere \\(\\hat{d}_i\\) is the degree of node \\(i\\) and \\(\\boldsymbol{\\Theta}\\) is a learnable weight matrix. \\(\\mathcal{N}(i)\\) is the set of neighbors of node \\(i\\). \\(\\mathbf{x}_i\\) is the node embedding of node \\(i\\), \\(e_{j, i}\\) is the edge feature between node \\(i\\) and \\(j\\).\nThe node embeddings are then aggregated using Set2Set pooling (Vinyals, Bengio, and Kudlur 2016).\n\n\nSMILES encoder\nFor encoding SMILES, the use a transformer model. In their code, they seem to pass through only one transformer layer.\nThe forward pass looks like this:\nx = self.token_embedding(text)\nx = x + self.positional_embedding\nx = x.permute(1, 0, 2)  # NLD -&gt; LND\nx = self.transformer(x)\nx = x.permute(1, 0, 2)  # LND -&gt; NLD\nx = self.ln_final(x)\nx = self.pooler(x[:,0,:])\nThey take the first token of the sequence and pass it through a linear layer to get the final representation.\n\n\nContrastive training\nThe authors use a contrastive loss to train the encoders.\n\\[\n\\mathcal{L}_c=-\\frac{1}{2} \\log \\frac{e^{\\left\\langle f_G^j, f_S^j\\right\\rangle / \\tau}}{\\sum_{k=1}^N e^{\\left\\langle f_G^j, f_S^k\\right\\rangle / \\tau}}-\\frac{1}{2} \\log \\frac{e^{\\left\\langle f_G^j, f_S^j\\right\\rangle / \\tau}}{\\sum_{k=1}^N e^{\\left\\langle f_G^k, f_S^j\\right\\rangle / \\tau}},\n\\]\nIn contrastive training, we try to maximize the similarity between positive pairs and minimize the similarity between negative pairs. In the equation above, \\(f_G^j\\) and \\(f_S^j\\) are the representations of the graph and SMILES of the same reaction, respectively. \\(\\tau\\) is a temperature parameter.\nSuch contrastive training allows to pretrain the encoders on a large dataset without labels.\n\n\n\n\n\n\nNote\n\n\n\nContrastive learning is one of the most popular methods in self-supervised learning. A good overview can be found in Lilian Weng‚Äôs amazing blog.\n\n\n\n\n\nHuman-features encoder\nThe authors also encode additional features with feedforward networks in a mixture of experts (MoE) model. The key idea behind MoE is that we replace ‚Äúconventional layers‚Äù with ‚ÄúMoE layers‚Äù which are copies of the same layer. A gating network decides, based on the input, which layer to use. This is powerful if we sparsely select the experts-then only a subset of all weights are used in a given forward pass.\n\\[\n\\operatorname{MoE}\\left(x_H\\right)=\\sum_{i=1}^t \\mathcal{G}\\left(x_H\\right)_i \\cdot E_i\\left(x_H\\right)\n\\]\nThis is a mixture of experts model. The authors use a gating network \\(\\mathcal{G}\\) to decide which expert to use. The experts \\(E_i\\) are simple feedforward networks. The gating network might be a simple softmax layer:\n\\[\nG_\\sigma(x)=\\operatorname{Softmax}\\left(x \\cdot W_g\\right)\n\\]\nin practice, one can improve that by adding sparsity (e.g.¬†selecting top-k).\n\n\n\n\n\n\nNote\n\n\n\nMoE (Shazeer et al. 2017) has become popular recently as a way to scale LLMs. You might have across model names like Mixtral-8x7B (Jiang et al. 2024), which indicates that the model is a mixture of 8 experts, each of which is a 7B parameter model. The total number of parameters is 47B parameters, but the inference cost is similar to the one of a 14B parameter model. (Note however, that memory consumption is still high as all experts need to be loaded into memory.)\nThis blog by Cameron Wolfe gives a good overview. You might also find Yannic Kilcher‚Äôs video about Mixtral of Experts useful.\n\n\n\n\nFusion\nThe fusion of the different features is done by concatenating them\nThe complete forward pass looks like this:\nr_graph_feats = torch.sum(torch.stack([self.clme.mpnn(mol) for mol in rmols]), 0)\np_graph_feats = self.clme.mpnn(pmols)\nfeats, a_loss = self.mlp(input_feats)\nseq_feats = self.clme.transformer(smiles)\nconcat_feats = torch.cat([r_graph_feats, p_graph_feats, feats, seq_feats], 1)\nout = self.predict(concat_feats)\nwhere the mpnn method is the graph encoder, the transformer method is the SMILES encoder, and the mlp method is the human-features encoder.\n\n\nUncertainty (quantification)\nThe authors define the prediction as\n\\[\n\\hat{y}=\\mu(\\boldsymbol{x})+\\epsilon * \\sigma(\\boldsymbol{x})\n\\]\nwhere \\(\\mu(\\boldsymbol{x})\\) is the prediction, \\(\\sigma(\\boldsymbol{x})\\) is the uncertainty, and \\(\\epsilon\\) is a random variable sampled from a normal distribution.\nThe model is trained with a loss function that includes the uncertainty:\n\\[\n\\mathcal{L}_u=\\frac{1}{N} \\sum_{i=1}^N\\left[\\frac{1}{\\sigma\\left(\\boldsymbol{x}_i\\right)^2}\\left\\|y_i-\\mu\\left(\\boldsymbol{x}_i\\right)\\right\\|^2+\\log \\sigma\\left(\\boldsymbol{x}_i\\right)^2\\right]\n\\]\nThe \\(\\sigma\\) term is capturing observation noise (aleatoric uncertainty).\n\n\n\n\n\n\nNote\n\n\n\nThis loss comes from the idea of variational inference.\n\\[\n\\mathcal{L}(\\boldsymbol{\\lambda})=-\\mathbb{E}_{q(\\boldsymbol{\\theta} ; \\boldsymbol{\\lambda})}[\\log p(\\mathbf{y} \\mid \\mathbf{x}, \\boldsymbol{\\theta})]+\\mathrm{KL}(q(\\boldsymbol{\\theta} ; \\boldsymbol{\\lambda}) \\| p(\\boldsymbol{\\theta}))\n\\]\nIn this equation, the first term is the negative log-likelihood, and the second term is the KL divergence between the approximate posterior \\(q(\\boldsymbol{\\theta} ; \\boldsymbol{\\lambda})\\) and the prior \\(p(\\boldsymbol{\\theta})\\). The KL divergence is a measure of how much the approximate posterior diverges from the prior. The idea is to minimize the negative log-likelihood while keeping the approximate posterior close to the prior. This is a way to quantify the uncertainty in the model.\nThe idea comes from Bayesian inference, where we want to estimate the posterior distribution over the parameters of the model. In practice, this is intractable, so we use variational inference to approximate the posterior with a simpler distribution. The posterior (which quantifies uncertainty) is typically computationally expensive to compute, so we use variational inference to approximate it with a simpler distribution, this is called variational inference. Since during training, we do some sampling, we need to perform a reparametrization trick (Kingma, Salimans, and Welling 2015) to make the gradients flow through the sampling operation.",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Uncertainty-Aware Yield Prediction with Multimodal Molecular Features</span>"
    ]
  },
  {
    "objectID": "papers/chem_yield_prediction_2024.html#results",
    "href": "papers/chem_yield_prediction_2024.html#results",
    "title": "Uncertainty-Aware Yield Prediction with Multimodal Molecular Features",
    "section": "Results",
    "text": "Results\nAs in most ML papers, we have tables with bold numbers, e.g.¬†for a dataset with amide coupling reactions:\n\n\n\n\n\n\n\n\n\nModel\nMAE \\(\\downarrow\\)\nRMSE \\(\\downarrow\\)\n\\(R^2 \\uparrow\\)\n\n\n\n\nMordred\n\\(15.99 \\pm 0.14\\)\n\\(21.08 \\pm 0.16\\)\n\\(0.168 \\pm 0.010\\)\n\n\nYieldBert\n\\(16.52 \\pm 0.20\\)\n\\(21.12 \\pm 0.13\\)\n\\(0.172 \\pm 0.016\\)\n\n\nYieldGNN\n\\(\\underline{15.27 \\pm 0.18}\\)\n\\(\\underline{19.82} \\pm 0.08\\)\n\\(\\underline{0.216} \\pm 0.013\\)\n\n\nMPNN\n\\(16.31 \\pm 0.22\\)\n\\(20.86 \\pm 0.27\\)\n\\(0.188 \\pm 0.021\\)\n\n\nOurs\n\\(\\mathbf{1 4 . 7 6} \\pm \\mathbf{0 . 1 5}\\)\n\\(\\mathbf{1 9 . 3 3} \\pm \\mathbf{0 . 1 0}\\)\n\\(\\mathbf{0 . 2 6 2} \\pm \\mathbf{0 . 0 0 9}\\)\n\n\n\nHere, their model outperforms the baselines. But it is also interesting to see how well the Mordred baseline performs compared to much more complex models.\nThe pattern of their model being bold in tables is persistent across datasets.\n\nAblations\nThe authors perform ablations to understand the importance of the different components of their model. While there are some differences, the differences are not drastic (partially overlapping errorbars).\n\n\n\n\n\n\n\n\n\nModel\nMAE \\(\\downarrow\\)\nRMSE \\(\\downarrow\\)\n\\(R^2 \\uparrow\\)\n\n\n\n\nOurs\n\\(14.76 \\pm 0.15\\)\n\\(19.33 \\pm 0.10\\)\n\\(0.262 \\pm 0.009\\)\n\n\nw/o UQ\n\\(15.08 \\pm 0.13\\)\n\\(19.63 \\pm 0.09\\)\n\\(0.249 \\pm 0.009\\)\n\n\nw/o \\(\\mathcal{L}_r\\)\n\\(14.80 \\pm 0.16\\)\n\\(19.51 \\pm 0.10\\)\n\\(0.261 \\pm 0.010\\)\n\n\nw/o MoE\n\\(15.12 \\pm 0.18\\)\n\\(20.03 \\pm 0.13\\)\n\\(0.230 \\pm 0.012\\)\n\n\nw/o Seq.\n\\(14.97 \\pm 0.16\\)\n\\(19.55 \\pm 0.11\\)\n\\(0.261 \\pm 0.010\\)\n\n\nw/o Graph\n\\(15.06 \\pm 0.15\\)\n\\(19.59 \\pm 0.10\\)\n\\(0.260 \\pm 0.009\\)\n\n\nw/o H.\n\\(15.83 \\pm 0.20\\)\n\\(20.46 \\pm 0.18\\)\n\\(0.212 \\pm 0.016\\)",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Uncertainty-Aware Yield Prediction with Multimodal Molecular Features</span>"
    ]
  },
  {
    "objectID": "papers/chem_yield_prediction_2024.html#take-aways",
    "href": "papers/chem_yield_prediction_2024.html#take-aways",
    "title": "Uncertainty-Aware Yield Prediction with Multimodal Molecular Features",
    "section": "Take aways",
    "text": "Take aways\n\nA lot of machinery, but not a drastic improvement\nIt is the data, stupid! üòâ (It is not really clear how this is even supposed to work with information about the conditions)\nInterestingly, they didn‚Äôt test USPTO or other datasets\nTheir approach with frozen encoders is interesting, it would have been interesting to see learning curves to better understand the data efficiency of the approach",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Uncertainty-Aware Yield Prediction with Multimodal Molecular Features</span>"
    ]
  },
  {
    "objectID": "papers/chem_yield_prediction_2024.html#references",
    "href": "papers/chem_yield_prediction_2024.html#references",
    "title": "Uncertainty-Aware Yield Prediction with Multimodal Molecular Features",
    "section": "References",
    "text": "References\n\n\nAhneman, Derek T., Jes√∫s G. Estrada, Shishi Lin, Spencer D. Dreher, and\nAbigail G. Doyle. 2018. ‚ÄúPredicting Reaction Performance in c‚Äìn\nCross-Coupling Using Machine Learning.‚Äù Science 360\n(6385): 186‚Äì90. https://doi.org/10.1126/science.aar5169.\n\n\nChen, Jiayuan, Kehan Guo, Zhen Liu, Olexandr Isayev, and Xiangliang\nZhang. 2024. ‚ÄúUncertainty-Aware Yield Prediction with Multimodal\nMolecular Features.‚Äù Proceedings of the AAAI Conference on\nArtificial Intelligence 38 (8): 8274‚Äì82. https://doi.org/10.1609/aaai.v38i8.28668.\n\n\nChuang, Kangway V., and Michael J. Keiser. 2018. ‚ÄúComment on\n‚ÄòPredicting Reaction Performance in c‚Äìn Cross-Coupling Using\nMachine Learning‚Äô.‚Äù Science 362 (6416). https://doi.org/10.1126/science.aat8603.\n\n\nDagdelen, John, Alexander Dunn, Sanghoon Lee, Nicholas Walker, Andrew S.\nRosen, Gerbrand Ceder, Kristin A. Persson, and Anubhav Jain. 2024.\n‚ÄúStructured Information Extraction from Scientific Text with Large\nLanguage Models.‚Äù Nature Communications 15 (1): 1418. https://doi.org/10.1038/s41467-024-45563-x.\n\n\nGal, Yarin, and Zoubin Ghahramani. 2016. ‚ÄúDropout as a Bayesian\nApproximation: Representing Model Uncertainty in Deep Learning.‚Äù\nIn International Conference on Machine Learning, 1050‚Äì59. PMLR.\n\n\nGuo, Jiang, A. Santiago Ibanez-Lopez, Hanyu Gao, Victor Quach, Connor W.\nColey, Klavs F. Jensen, and Regina Barzilay. 2021. ‚ÄúAutomated\nChemical Reaction Extraction from Scientific Literature.‚Äù\nJournal of Chemical Information and Modeling 62 (9): 2035‚Äì45.\nhttps://doi.org/10.1021/acs.jcim.1c00284.\n\n\nHuo, Haoyan, Ziqin Rong, Olga Kononova, Wenhao Sun, Tiago Botari, Tanjin\nHe, Vahe Tshitoyan, and Gerbrand Ceder. 2019. ‚ÄúSemi-Supervised\nMachine-Learning Classification of Materials Synthesis\nProcedures.‚Äù Npj Computational Materials 5 (1). https://doi.org/10.1038/s41524-019-0204-1.\n\n\nJiang, Albert Q., Alexandre Sablayrolles, Antoine Roux, Arthur Mensch,\nBlanche Savary, Chris Bamford, Devendra Singh Chaplot, et al. 2024.\n‚ÄúMixtral of Experts.‚Äù https://arxiv.org/abs/2401.04088.\n\n\nKim, Edward, Kevin Huang, Adam Saunders, Andrew McCallum, Gerbrand\nCeder, and Elsa Olivetti. 2017. ‚ÄúMaterials Synthesis Insights from\nScientific Literature via Text Extraction and Machine Learning.‚Äù\nChemistry of Materials 29 (21): 9436‚Äì44. https://doi.org/10.1021/acs.chemmater.7b03500.\n\n\nKim, Edward, Zach Jensen, Alexander van Grootel, Kevin Huang, Matthew\nStaib, Sheshera Mysore, Haw-Shiuan Chang, et al. 2020. ‚ÄúInorganic\nMaterials Synthesis Planning with Literature-Trained Neural\nNetworks.‚Äù Journal of Chemical Information and Modeling\n60 (3): 1194‚Äì1201. https://doi.org/10.1021/acs.jcim.9b00995.\n\n\nKingma, Diederik P., Tim Salimans, and Max Welling. 2015.\n‚ÄúVariational Dropout and the Local Reparameterization\nTrick.‚Äù https://arxiv.org/abs/1506.02557.\n\n\nKononova, Olga, Haoyan Huo, Tanjin He, Ziqin Rong, Tiago Botari, Wenhao\nSun, Vahe Tshitoyan, and Gerbrand Ceder. 2019. ‚ÄúText-Mined Dataset\nof Inorganic Materials Synthesis Recipes.‚Äù Scientific\nData 6 (1). https://doi.org/10.1038/s41597-019-0224-1.\n\n\nKwon, Youngchun, Dongseon Lee, Youn-Suk Choi, and Seokho Kang. 2022.\n‚ÄúUncertainty-Aware Prediction of Chemical Reaction Yields with\nGraph Neural Networks.‚Äù Journal of Cheminformatics 14\n(1). https://doi.org/10.1186/s13321-021-00579-z.\n\n\nMavraƒçiƒá, Juraj, Callum J. Court, Taketomo Isazawa, Stephen R. Elliott,\nand Jacqueline M. Cole. 2021. ‚ÄúChemDataExtractor 2.0:\nAutopopulated Ontologies for Materials Science.‚Äù Journal of\nChemical Information and Modeling 61 (9): 4280‚Äì89. https://doi.org/10.1021/acs.jcim.1c00446.\n\n\nMysore, Sheshera, Zach Jensen, Edward Kim, Kevin Huang, Haw-Shiuan\nChang, Emma Strubell, Jeffrey Flanigan, Andrew McCallum, and Elsa\nOlivetti. 2019. ‚ÄúThe Materials Science Procedural Text Corpus:\nAnnotating Materials Synthesis Procedures with Shallow Semantic\nStructures.‚Äù https://arxiv.org/abs/1905.06939.\n\n\nSchwaller, Philippe, Alain C Vaucher, Teodoro Laino, and Jean-Louis\nReymond. 2020. ‚ÄúData Augmentation Strategies to Improve Reaction\nYield Predictions and Estimate Uncertainty.‚Äù Chemrxiv\nPreprint.\n\n\n‚Äî‚Äî‚Äî. 2021. ‚ÄúPrediction of Chemical Reaction Yields Using Deep\nLearning.‚Äù Machine Learning: Science and Technology 2\n(1): 015016.\n\n\nShazeer, Noam, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc\nLe, Geoffrey Hinton, and Jeff Dean. 2017. ‚ÄúOutrageously Large\nNeural Networks: The Sparsely-Gated Mixture-of-Experts Layer.‚Äù https://arxiv.org/abs/1701.06538.\n\n\nSwain, Matthew C., and Jacqueline M. Cole. 2016.\n‚ÄúChemDataExtractor: A Toolkit for Automated Extraction of Chemical\nInformation from the Scientific Literature.‚Äù Journal of\nChemical Information and Modeling 56 (10): 1894‚Äì904. https://doi.org/10.1021/acs.jcim.6b00207.\n\n\nTrewartha, Amalie, Nicholas Walker, Haoyan Huo, Sanghoon Lee, Kevin\nCruse, John Dagdelen, Alexander Dunn, Kristin A. Persson, Gerbrand\nCeder, and Anubhav Jain. 2022. ‚ÄúQuantifying the Advantage of\nDomain-Specific Pre-Training on Named Entity Recognition Tasks in\nMaterials Science.‚Äù Patterns 3 (4): 100488. https://doi.org/10.1016/j.patter.2022.100488.\n\n\nVinyals, Oriol, Samy Bengio, and Manjunath Kudlur. 2016. ‚ÄúOrder\nMatters: Sequence to Sequence for Sets.‚Äù https://arxiv.org/abs/1511.06391.",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Uncertainty-Aware Yield Prediction with Multimodal Molecular Features</span>"
    ]
  },
  {
    "objectID": "papers/dagdelen_data_extraction.html",
    "href": "papers/dagdelen_data_extraction.html",
    "title": "Structured information extraction from scientific text with large language models",
    "section": "",
    "text": "Why discussing this paper?\nI chose Dagdelen et al.‚Äôs paper (Dagdelen et al. 2024) for our journal club because:",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Structured information extraction from scientific text with large language models</span>"
    ]
  },
  {
    "objectID": "papers/dagdelen_data_extraction.html#why-discussing-this-paper",
    "href": "papers/dagdelen_data_extraction.html#why-discussing-this-paper",
    "title": "Structured information extraction from scientific text with large language models",
    "section": "",
    "text": "Dagdelen, John, Alexander Dunn, Sanghoon Lee, Nicholas Walker, Andrew S. Rosen, Gerbrand Ceder, Kristin A. Persson, and Anubhav Jain. 2024. ‚ÄúStructured Information Extraction from Scientific Text with Large Language Models.‚Äù Nature Communications 15 (1): 1418. https://doi.org/10.1038/s41467-024-45563-x.\n\nIt is one of the last published papers to fine-tune a model for the data extraction task for materials science.\nIt presents a very robust fine-tuning and evaluation process.\nFurthermore, they show how the current models can help with a tedious task such as it is annotating data.",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Structured information extraction from scientific text with large language models</span>"
    ]
  },
  {
    "objectID": "papers/dagdelen_data_extraction.html#context",
    "href": "papers/dagdelen_data_extraction.html#context",
    "title": "Structured information extraction from scientific text with large language models",
    "section": "Context",
    "text": "Context\nExtracting the unstructured scientific information from the articles that contain it can be a really arduos and time-consuming task. In the recent years, several works have shown the great potential that LLMs have to greatly accelerate this task. However, for some research fields or harder extraction schemas, the general pre-training of these models might not be enough to archieve the desired results. For such cases, fine-tuning have shown to be the adequate technique.",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Structured information extraction from scientific text with large language models</span>"
    ]
  },
  {
    "objectID": "papers/dagdelen_data_extraction.html#prior-work",
    "href": "papers/dagdelen_data_extraction.html#prior-work",
    "title": "Structured information extraction from scientific text with large language models",
    "section": "Prior work",
    "text": "Prior work\n\nOld ages\nSeveral works from the Ceder group showed the complete tedious process. First, Huo et al. (Huo et al. 2019) use LDA + RF to classify text. To train the RF model they had to manually label 6000 materials paragraphs as they contain synthesis information or not.\n\nHuo, Haoyan, Ziqin Rong, Olga Kononova, Wenhao Sun, Tiago Botari, Tanjin He, Vahe Tshitoyan, and Gerbrand Ceder. 2019. ‚ÄúSemi-Supervised Machine-Learning Classification of Materials Synthesis Procedures.‚Äù Npj Computational Materials 5 (1). https://doi.org/10.1038/s41524-019-0204-1.\n\nKononova, Olga, Haoyan Huo, Tanjin He, Ziqin Rong, Tiago Botari, Wenhao Sun, Vahe Tshitoyan, and Gerbrand Ceder. 2019. ‚ÄúText-Mined Dataset of Inorganic Materials Synthesis Recipes.‚Äù Scientific Data 6 (1). https://doi.org/10.1038/s41597-019-0224-1.\nIn a following work, Kononova et al. (Kononova et al. 2019) trained a Word2Vec model, to then feed the embeddings to a BiLSTM-CRF. To train this NN they manually annotated more than 800 paragraphs word-by-word with tags about solid-state synthesis role (material, target, precursor or other). Furthermore, to classify the synthesis operations (NOT OPERATION, MIXING, HEATING, etc) they trained another NN with more annotated data. To this step they also had to LEMMATIZED the sentences and obtain each token‚Äôs POS. Amazing hard work!\nSimilar works by Kim et al. (Kim et al. 2017, 2020; Mysore et al. 2019) in which they applied similar techniques such as word embeddings from language models, then fed to a named entity recognition model.\n\nKim, Edward, Kevin Huang, Adam Saunders, Andrew McCallum, Gerbrand Ceder, and Elsa Olivetti. 2017. ‚ÄúMaterials Synthesis Insights from Scientific Literature via Text Extraction and Machine Learning.‚Äù Chemistry of Materials 29 (21): 9436‚Äì44. https://doi.org/10.1021/acs.chemmater.7b03500.\n\nKim, Edward, Zach Jensen, Alexander van Grootel, Kevin Huang, Matthew Staib, Sheshera Mysore, Haw-Shiuan Chang, et al. 2020. ‚ÄúInorganic Materials Synthesis Planning with Literature-Trained Neural Networks.‚Äù Journal of Chemical Information and Modeling 60 (3): 1194‚Äì1201. https://doi.org/10.1021/acs.jcim.9b00995.\n\n\n\nFigure taken from Mysore et al.¬†paper (Mysore et al. 2019) illustrating how they labeled the data for the NER task.\n\nMysore, Sheshera, Zach Jensen, Edward Kim, Kevin Huang, Haw-Shiuan Chang, Emma Strubell, Jeffrey Flanigan, Andrew McCallum, and Elsa Olivetti. 2019. ‚ÄúThe Materials Science Procedural Text Corpus: Annotating Materials Synthesis Procedures with Shallow Semantic Structures.‚Äù https://arxiv.org/abs/1905.06939.\n\n\n\n\nChemDataExtractor 1.0 and 2.0\nCole et al. (Swain and Cole 2016; Mavraƒçiƒá et al. 2021) developed ChemDataExtractor which is build from the combination of traditional ML techniques for each NLP task such as lemmatazion, tokenization, POS tagging, it even include Table Parsing. All of these models trained in chemical text, which made this tool a really good option for extracting chemical data from text.\n\nSwain, Matthew C., and Jacqueline M. Cole. 2016. ‚ÄúChemDataExtractor: A Toolkit for Automated Extraction of Chemical Information from the Scientific Literature.‚Äù Journal of Chemical Information and Modeling 56 (10): 1894‚Äì904. https://doi.org/10.1021/acs.jcim.6b00207.\n\nMavraƒçiƒá, Juraj, Callum J. Court, Taketomo Isazawa, Stephen R. Elliott, and Jacqueline M. Cole. 2021. ‚ÄúChemDataExtractor 2.0: Autopopulated Ontologies for Materials Science.‚Äù Journal of Chemical Information and Modeling 61 (9): 4280‚Äì89. https://doi.org/10.1021/acs.jcim.1c00446.\n\n\nTrewartha et al.¬†(2022)\nTrewartha el al. (Trewartha et al. 2022) compared the performance of a simpler model such as a BiLSTM RNN with three more complex transformer models, BERT, SciBERT and MatBERT for the NER task. For that, they used data from three different NER datasets, each one related with different materials synthesis.\nThe results, showed that the more specialized BERT models were able to better recognize the different entities. However, it is important to remark that the BERT models were fine-tuned for the task.\n\n\n\nFigure taken from Trewartha et al. (Trewartha et al. 2022) summarizing the results that they obtained with each model.\n\nTrewartha, Amalie, Nicholas Walker, Haoyan Huo, Sanghoon Lee, Kevin Cruse, John Dagdelen, Alexander Dunn, Kristin A. Persson, Gerbrand Ceder, and Anubhav Jain. 2022. ‚ÄúQuantifying the Advantage of Domain-Specific Pre-Training on Named Entity Recognition Tasks in Materials Science.‚Äù Patterns 3 (4): 100488. https://doi.org/10.1016/j.patter.2022.100488.",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Structured information extraction from scientific text with large language models</span>"
    ]
  },
  {
    "objectID": "papers/dagdelen_data_extraction.html#problem-setting",
    "href": "papers/dagdelen_data_extraction.html#problem-setting",
    "title": "Structured information extraction from scientific text with large language models",
    "section": "Problem setting",
    "text": "Problem setting\n\nAlmost all the scientific knowledge is contained in scientific texts in an unstructured way.\nThe classical approaches include a lot of different techniques, each of them has to be trained independently.\nFor those classical techniques, a lot manually labeled data is needed for each task and technique.\nLLMs appear to simplify a lot all the previous options by allowing to perform all the different NLP tasks with one unique model.",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Structured information extraction from scientific text with large language models</span>"
    ]
  },
  {
    "objectID": "papers/dagdelen_data_extraction.html#approach",
    "href": "papers/dagdelen_data_extraction.html#approach",
    "title": "Structured information extraction from scientific text with large language models",
    "section": "Approach",
    "text": "Approach\nThey proposed to fine-tune two models, one open-source Llama-2 70B model and a close-source one such as GPT-3, for the NER and RE tasks applied to solid-state materials. As output, they compared two different options: JSON and plain text. They proposed this for three different specificities of data: Doping, MOF and general materials data.\n\n\n\nTask\nTraining samples\nCompletion format\n\n\n\n\nDoping\n413 sentences\nJSON\n\n\nDoping\n413 sentences\nEnglish sentences\n\n\nMOFs\n507 abstracts\nJSON\n\n\nGeneral materials\n634 abstracts\nJSON",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Structured information extraction from scientific text with large language models</span>"
    ]
  },
  {
    "objectID": "papers/dagdelen_data_extraction.html#results",
    "href": "papers/dagdelen_data_extraction.html#results",
    "title": "Structured information extraction from scientific text with large language models",
    "section": "Results",
    "text": "Results\nThe results showed first of all that both models performed similar for the tasks. For the exact match, GPT-3 performed slightly better than the Llama-2 model, with overall results for both models around 50% considering all the tasks.\n\n\n\nTask\nRelation\nE.M. F1 GPT-3\nE.M. Llama-3\n\n\n\n\nDoping\nhost-dopant\n0.726\n0.821\n\n\nGeneral\nformula-name\n0.456\n0.367\n\n\nGeneral\nformula-acronym\n0.333\n0.286\n\n\nGeneral\nformula-structure/phase\n0.482\n0.470\n\n\nGeneral\nformula-application\n0.537\n0.516\n\n\nGeneral\nformula-description\n0.354\n0.340\n\n\nMOFs\nname-formula\n0.483\n0.276\n\n\nMOFs\nname-guest specie\n0.616\n0.408\n\n\nMOFs\nname-application\n0.573\n0.531\n\n\nMOFs\nname-description\n0.404\n0.389\n\n\n\n\n\n\nE.M. stands for exact match\n\n\n\nThe results presented in this table include for both NER and RE NLP tasks.\n\n\nIt is important to comment that the exact match is an approximate lower bound on information extraction performance, since it not consider some cases such as ‚ÄúLithium ion‚Äù named as ‚ÄúLi-ion‚Äù, or MOF names such as ‚ÄúZIF-8‚Äù that are described as ‚Äúmesostructured MOFs formed by Cu2+ and 5hydroxy-1,3-benzenedicarboxylic acid‚Äù.\nFor correctly measure those ambiguities, they did a manual evaluation on a randomly sampled 10% of the test set. These results showed that the score for the extraction was much better than the showed by the exact match. This also showed that some kind of normallization proccess is needed to correctly evaluate this type of extraction tasks.\nFor the Doping task, three different output schema were consider, DopingEnglish, DopingJSON and DopingExtra-English. They compared the results for the three schema GPT-3 and Llama-2 fine-tuned models with other older models such as MatBERT and Seq2rel.\n\n\nThe difference between DopingEnglish and DopingExtra-English is that the last one include some additional information and not only the host-entity relation.\nThe results showed that the Llama-2 model return the best results for this task, which are slightly better than the GPT-3 ones. Both LLMs improved by far the other two models.\n\n\n\n\n\n\n\n\n\n\nModel\nSchema\nE.M. Precision\nE.M. Recall\nE.M. F1\n\n\n\n\nMatBERT\nn/a\n0.377\n0.403\n0.390\n\n\nSeq2rel\nn/a\n0.420\n0.605\n0.496\n\n\nGPT-3\nDoping-JSON\n0.772\n0.684\n0.725\n\n\nGPT-3\nDoping-English\n0.803\n0.754\n0.778\n\n\nGPT-3\nDopingExtra-English\n0.820\n0.798\n0.809\n\n\nLlama-2\nDoping-JSON\n0.836\n0.807\n0.821\n\n\nLlama-2\nDoping-English\n0.787\n0.842\n0.814\n\n\nLlama-2\nDopingExtra-English\n0.694\n0.815\n0.750\n\n\n\nA limitation of the method could be that for each of the three extractions tasks, they have to annotate between 100 and 500 text passages. This can be a tedious work. However, to overcome this limitation, they proposed to include human-in-the-loop annotation.\n\nHuman-in-the-loop\nTo overcome the limitation of having to manually annotate all the data needed for the fine-tuning process, they sucesfully implemented human-in-the-loop annotation. For that, they fine-tune the model with a small amount of manually labelled data. Then the model is asked to extract data from the other text passages. The returned data by the model is corrected by an human annotator and is feed into the model to further fine-tune it.\n\n\n\nFigure showing the process used to implement human-in-the-loop annotation.\n\n\nBy using this technique, they greatly reduce the amount of time needed to annotate the last pieces of text compared with the first ones.\n\n\n\nFigure showing the time reduction across the process of annotation using the human-in-the-loop technique.\n\n\nBy using this annotation method they greatly improve the annotation time solving one of the main drawbacks of fine-tuning an LLM. This great limitation can be seen in another works such as the one by Guo el al. (Guo et al. 2021) in which they employed 13 graduate and postdoc students to annotate about chemical reactions. After that, they have to even check all the annotation. They estimate that this process took them almost 300 hours.\n\nGuo, Jiang, A. Santiago Ibanez-Lopez, Hanyu Gao, Victor Quach, Connor W. Coley, Klavs F. Jensen, and Regina Barzilay. 2021. ‚ÄúAutomated Chemical Reaction Extraction from Scientific Literature.‚Äù Journal of Chemical Information and Modeling 62 (9): 2035‚Äì45. https://doi.org/10.1021/acs.jcim.1c00284.",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Structured information extraction from scientific text with large language models</span>"
    ]
  },
  {
    "objectID": "papers/dagdelen_data_extraction.html#take-aways",
    "href": "papers/dagdelen_data_extraction.html#take-aways",
    "title": "Structured information extraction from scientific text with large language models",
    "section": "Take aways",
    "text": "Take aways\n\nOpen source models with proper tuning can yield high-quality results similar to those of closed source models.\nDespite some labeled data is needed, the process is simplified a lot with the use of LLMs.\nWith the fasst and continuous development of the current models, maybe fine-tuning for a simpler task such as data extraction is no furhter needed.",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Structured information extraction from scientific text with large language models</span>"
    ]
  },
  {
    "objectID": "papers/MolCLR_2024.html",
    "href": "papers/MolCLR_2024.html",
    "title": "Molecular contrastive learning of representations (MolCLR) via graph neural networks",
    "section": "",
    "text": "Why discussing this paper?\nI chose Wang‚Äôs et al.‚Äôs paper (Wang et al. 2022) for our journal club because",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Molecular contrastive learning of representations (MolCLR) via graph neural networks</span>"
    ]
  },
  {
    "objectID": "papers/MolCLR_2024.html#why-discussing-this-paper",
    "href": "papers/MolCLR_2024.html#why-discussing-this-paper",
    "title": "Molecular contrastive learning of representations (MolCLR) via graph neural networks",
    "section": "",
    "text": "Supervised learning relies heavily on labeled training data, which can be expensive and time-consuming to obtain.\nLabeled datasets are limited; training models on such datasets might lead to overfitting and poor generalization.\nSelf-Supervised Learning (SSL) provides a promising alternative. It enables learning from unlabeled data, which is much easier to acquire in real-world applications and is part of a large research effort.\nSSL learns the inherent structure and patterns in unlabeled data. By doing so, self-supervised models can acquire rich representations and knowledge that can be transferred to downstream tasks, even with limited labeled data.\nContrastive self-supervised learning, as the name implies, is a self-supervised method that learns representations by contrasting positive and negative pairs.",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Molecular contrastive learning of representations (MolCLR) via graph neural networks</span>"
    ]
  },
  {
    "objectID": "papers/MolCLR_2024.html#context",
    "href": "papers/MolCLR_2024.html#context",
    "title": "Molecular contrastive learning of representations (MolCLR) via graph neural networks",
    "section": "Context",
    "text": "Context\nThe paper introduces MolCLR, a self-supervised learning framework that uses graph encoders to learn effective molecular representations.\nThe authors used a large unlabeled dataset with 10 million unique molecule SMILES from ChemBERTa and PubChem. This framework involves two important steps. First, in the pre-training phase, they build molecule graphs and develop graph neural network (GNN) encoders to learn differentiable representations. Second, the pretrained GNN backbone is used for supervised learning tasks.",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Molecular contrastive learning of representations (MolCLR) via graph neural networks</span>"
    ]
  },
  {
    "objectID": "papers/MolCLR_2024.html#main-idea-behind-contrastive-learning",
    "href": "papers/MolCLR_2024.html#main-idea-behind-contrastive-learning",
    "title": "Molecular contrastive learning of representations (MolCLR) via graph neural networks",
    "section": "Main idea behind contrastive learning",
    "text": "Main idea behind contrastive learning\nThe aim of contrastive representation learning is to develop an embedding space where similar samples are positioned closely together, whereas dissimilar samples are kept distant from each other.\n\n\n\nFigure taken from the blog post by Ekin Tiu illustrating the idea behind contrastive learning.",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Molecular contrastive learning of representations (MolCLR) via graph neural networks</span>"
    ]
  },
  {
    "objectID": "papers/MolCLR_2024.html#nt-xent-loss",
    "href": "papers/MolCLR_2024.html#nt-xent-loss",
    "title": "Molecular contrastive learning of representations (MolCLR) via graph neural networks",
    "section": "NT-Xent loss",
    "text": "NT-Xent loss\nThe authors use a NT-Xent loss to train the graph encoders.\n\\[\nL_{ij} = -\\log \\left( \\frac{\\exp(\\text{sim}(z_i, z_j)/\\tau)}{\\sum_{k=1}^{2N} \\mathbf{1}_{\\{k \\neq i\\}} \\exp(\\text{sim}(z_i, z_k)/\\tau)} \\right)\n\\]\n\nz_i and z_j: Latent vectors extracted from a positive data pair.\nN: Batch size, indicating the number of data pairs used.\nsim(‚ãÖ): Function measuring the similarity between two vectors.\nœÑ: The temperature parameter, used to scale the similarity measures in the function.\n\nTo get more understanding, (Le-Khac, Healy, and Smeaton 2020) is a review paper on contrastive learning.",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Molecular contrastive learning of representations (MolCLR) via graph neural networks</span>"
    ]
  },
  {
    "objectID": "papers/MolCLR_2024.html#overview-of-molclr-framework",
    "href": "papers/MolCLR_2024.html#overview-of-molclr-framework",
    "title": "Molecular contrastive learning of representations (MolCLR) via graph neural networks",
    "section": "Overview of MolCLR framework",
    "text": "Overview of MolCLR framework\n\nThe SMILES are converted into graphs in a batch. The graphs consist of nodes and edges. Nodes represent atoms, and edges represent the chemical bonds of a molecule.\nThe authors studied three types of augmentations: atom masking, bond deletion, and sub-graph removal. These augmentations introduce variance to the model, allowing it to learn different substructures/topologies of the same molecule and generalize well to unseen molecules.\nThe GNNs operate based on a message-passing framework. At each node, the local neighborhood information is aggregated and updated iteratively, resulting in node embeddings of the molecule.\nThese embeddings are fed into a Multi-Layer Perceptron (MLP) with hidden units to obtain a latent representation of the molecule. The MLP serves as a projection head, mapping the high-dimensional representations from the encoder (GNN) to a lower-dimensional latent space. This projection helps in learning more compact and discriminative representations for the contrastive loss.\nThe key difference between supervised GNNs and self-supervised contrastive GNNs is the training objective. In a supervised setup, where we have access to ground truth labels, we can train the network to optimize standard loss functions such as Binary Cross Entropy (BCE) for classification or Mean Absolute Error/Mean Squared Error (MAE/MSE) for regression. In self-supervised contrastive learning, we aim to learn a latent space where positive pairs are closer and negative pairs are further apart.\nThe goal is to score the agreement between positive pairs higher than that of negative pairs. For a given graph, its positive pair is constructed using data augmentations, while all other graphs in the batch constitute negative pairs.\n\n\n\n\nOverview figure for the MolCLR framework\n\n\nFor more information about GNNs and their applications, check out this review paper [zhou2020graph].",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Molecular contrastive learning of representations (MolCLR) via graph neural networks</span>"
    ]
  },
  {
    "objectID": "papers/MolCLR_2024.html#results",
    "href": "papers/MolCLR_2024.html#results",
    "title": "Molecular contrastive learning of representations (MolCLR) via graph neural networks",
    "section": "Results",
    "text": "Results\nThe authors tested the performance of MolCLR framework on seven benchmarks for classification tasks and six benchmarks for regression tasks.\nThe authors claim that on classification tasks with other self-supervised learning or pre-training strategies, their MolCLR framework achieves the best performance on five out of seven benchmarks, with an average improvement of 4.0 percent in (ROC-AUC (%)).\n\n\n\nTable 1 shows the test performance on seven benchmarks on classifiaction task\n\n\nThe authors also claim that on regression tasks, MolCLR surpasses other pre-training baselines in five out of six benchmarks and achieves almost the same performance on the remaining ESOL benchmark. For the QM9 dataset, MolCLR does not have comparable performancee with SchNet and MGCN supervised models.\n\n\n\nTable 2 shows the test performance on seven benchmarks on classifiaction task",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Molecular contrastive learning of representations (MolCLR) via graph neural networks</span>"
    ]
  },
  {
    "objectID": "papers/MolCLR_2024.html#analysis-of-molecule-graph-augmentations",
    "href": "papers/MolCLR_2024.html#analysis-of-molecule-graph-augmentations",
    "title": "Molecular contrastive learning of representations (MolCLR) via graph neural networks",
    "section": "Analysis of molecule graph augmentations",
    "text": "Analysis of molecule graph augmentations\nThe authors employed four augmentation strategies on the classification benchmarks. Out of four augmentations, sub-graph removal with probability 0.25 achieved good (ROC-AUC %) on all of the classification benchmarks except in the BBBP benchmark as shown in the figure 2. The reason might be that model structures are sensitive in BBBP benchmark.\nThey also tested GIN supervised models with and without molecular graph augmentations. With augmentations (sub-graph removal with a probability of 0.25), the model achieved superior performance compared to without augmentations.\n\n\n\nFigure 2 shows the investigation of molecule graph augmentations on classification benchmarks",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Molecular contrastive learning of representations (MolCLR) via graph neural networks</span>"
    ]
  },
  {
    "objectID": "papers/MolCLR_2024.html#investigation-of-molclr-representation",
    "href": "papers/MolCLR_2024.html#investigation-of-molclr-representation",
    "title": "Molecular contrastive learning of representations (MolCLR) via graph neural networks",
    "section": "Investigation of MOlCLR representation",
    "text": "Investigation of MOlCLR representation\nThe authors examined the representations learned by pretrained MOlCLR using t-SNE embedding. This method groups similar chemicals together in two-dimensional space. In Figure 3, they show a picture of 100,000 molecules from the validation set of PubChem database showing similar/dissimilar molecules learned by MolCLR pretraining.\n\n\n\nFigure 3 Visualization of molecular representations learned by MolClr via t-sne\n\n\nFor instance, the three molecules on the top possess carbonyl groups connected with aryls. The two molecules on the bottom left have similar structures, where a halogen atom (fluorine or chlorine) is connected to benzene.\nTherefore, learned representations are not random but they are meaningful.\nIn addition to this, the authors also query molecule from the PubcChem with ID 42953211 and the closest nine similar molecules are retrevied along with RDKFP and ECFP similarities labelled. It is observed that these selected molecules share the same functional groups, including alkyl halides (chlorine), tertiary amines, ketones and aromatics. A thiophene structure can also be found in all the molecules.\nMore examples of query molecules can be found in supplementary information of this paper.\n\n\n\nFigure shows Comparison of MolCLR-learned representations and conventional FPs using the query molecule (PubChem ID 42953211)",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Molecular contrastive learning of representations (MolCLR) via graph neural networks</span>"
    ]
  },
  {
    "objectID": "papers/MolCLR_2024.html#take-aways",
    "href": "papers/MolCLR_2024.html#take-aways",
    "title": "Molecular contrastive learning of representations (MolCLR) via graph neural networks",
    "section": "Take aways",
    "text": "Take aways\n\nGood molecular representations are important for better predictions\nSelf-supervised contrastive learning would be an advantage for generalizable machine learning over supervised learning",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Molecular contrastive learning of representations (MolCLR) via graph neural networks</span>"
    ]
  },
  {
    "objectID": "papers/MolCLR_2024.html#references",
    "href": "papers/MolCLR_2024.html#references",
    "title": "Molecular contrastive learning of representations (MolCLR) via graph neural networks",
    "section": "References",
    "text": "References\n\n\nLe-Khac, Phuc H, Graham Healy, and Alan F Smeaton. 2020. ‚ÄúContrastive Representation Learning: A Framework and Review.‚Äù Ieee Access 8: 193907‚Äì34.\n\n\nWang, Yuyang, Jianren Wang, Zhonglin Cao, and Amir Barati Farimani. 2022. ‚ÄúMolecular Contrastive Learning of Representations via Graph Neural Networks.‚Äù Nature Machine Intelligence 4 (3): 279‚Äì87.",
    "crumbs": [
      "Papers",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Molecular contrastive learning of representations (MolCLR) via graph neural networks</span>"
    ]
  }
]
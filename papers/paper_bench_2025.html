<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.31">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Martiño Ríos García">
<meta name="dcterms.date" content="2025-04-16">

<title>PaperBench: Evaluating AI’s Ability to Replicate AI Research – Lamalab Tool and Paper Notes</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../papers/NaFM_2025.html" rel="next">
<link href="../papers/llm_confidence_2025.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-e1a5c8363afafaef2c763b6775fbf3ca.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-6e4858b4921ffe5a9c7d897cee0e5447.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>

      .quarto-title-block .quarto-title-banner {
        background-image: url(paper_bench_images/judges.png);
background-size: cover;
      }
</style>


</head>

<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../papers/chemical_reaction_acceleration.html">Papers</a></li><li class="breadcrumb-item"><a href="../papers/paper_bench_2025.html"><span class="chapter-title">PaperBench: Evaluating AI’s Ability to Replicate AI Research</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../papers/chemical_reaction_acceleration.html">Papers</a></li><li class="breadcrumb-item"><a href="../papers/paper_bench_2025.html"><span class="chapter-title">PaperBench: Evaluating AI’s Ability to Replicate AI Research</span></a></li></ol></nav>
      <h1 class="title"><span class="chapter-title">PaperBench: Evaluating AI’s Ability to Replicate AI Research</span></h1>
                      </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Martiño Ríos García </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">April 16, 2025</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Lamalab Tool and Paper Notes</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Tool and paper minutes</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Tools</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../tools/hydra.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Hydra</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../tools/ip_rotator.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">IP Rotator</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../tools/polars.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Polars</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../tools/thunder_client.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Thunder Client</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../tools/tmux.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">tmux</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../tools/trimean.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Robust statistics and Trimean</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../tools/pandarallel.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Easy fast <code>.apply</code> for pandas</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../tools/bfg.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">BFG Repo-Cleaner</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../tools/showyourwork.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">showyourwork</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Papers</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../papers/chemical_reaction_acceleration.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Accelerating the inference of string generation-based chemical reaction models for industrial applications</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../papers/llms_review.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">LLMs in Chemistry</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../papers/llm4mat.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Leveraging language representation for materials exploration and discovery</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../papers/chem_yield_prediction_2024.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Uncertainty-Aware Yield Prediction with Multimodal Molecular Features</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../papers/dagdelen_data_extraction.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Structured information extraction from scientific text with large language models</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../papers/MolCLR_2024.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Molecular contrastive learning of representations (MolCLR) via graph neural networks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../papers/PaCh.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">PaCh (Packed Chemicals): Computationally Effective Binary Format for Chemical Structure Encoding</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../papers/ms_prediction_graph_transformers/tandem_ms_prediction_graph_transformers.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Tandem mass spectrum prediction for small molecules using graph transformers</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../papers/open-source-cheminformatics.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Open-Source Software Development in Cheminformatics: A Qualitative Analysis of Rationales</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../papers/universal_chiral_fp_2024.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">MAP4C: One chiral fingerprint to find them all <span class="citation" data-cites="orsi2024one">(</span></span></a><a href="#ref-orsi2024one" role="doc-biblioref">Orsi and Reymond 2024</a>)
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../papers/lwreg_2024.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">lwreg: A Lightweight System for Chemical Registration and Data Storage <span class="citation" data-cites="landrum_lwreg_2024">(</span></span></a><a href="#ref-landrum_lwreg_2024" role="doc-biblioref">Landrum et al. 2024</a>)
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../papers/tango.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">It Takes Two to Tango: Directly Optimizing for Constrained Synthesizability in Generative Molecular Design <span class="citation" data-cites="https://doi.org/10.48550/arxiv.2410.11527">(</span></span></a><a href="#ref-https://doi.org/10.48550/arxiv.2410.11527" role="doc-biblioref">Guo and Schwaller 2024</a>)
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../papers/core_bench.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">CORE-Bench: Fostering the Credibility of Published Research Through a Computational Reproducibility Agent Benchmark</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../papers/bpm_L.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">A Bond-Based Machine Learning Model for Molecular Polarizabilities and A Priori Raman Spectra</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../papers/coconut.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Training Large Language Models to Reason in a Continuous Latent Space</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../papers/emnpd.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">🧬 EMNPD Database</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../papers/light-rag.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">🚀 LightRAG</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../papers/nmr-shift-prediction-from-small-data-quantities.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">NMR shift prediction from small data quantities</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../papers/llm_confidence_2025.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">What large language models know and what people think they know</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../papers/paper_bench_2025.html" class="sidebar-item-text sidebar-link active"><span class="chapter-title">PaperBench: Evaluating AI’s Ability to Replicate AI Research</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../papers/NaFM_2025.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">NaFM: Pre-training a Foundation Model for Small-Molecule Natural Products <span class="citation" data-cites="ding_nafm_2025">(</span></span></a><a href="#ref-ding_nafm_2025" role="doc-biblioref">Ding et al. 2025</a>)
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../papers/chen_reasoning.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Reasoning Models Don’t Always Say What They Think</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../papers/chemdfm.html" class="sidebar-item-text sidebar-link"><span class="chapter-title"></span></a><a href="https://arxiv.org/pdf/2401.14818">ChemDFM: A Large Language Foundation Model for Chemistry</a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#why-discussing-this-paper" id="toc-why-discussing-this-paper" class="nav-link active" data-scroll-target="#why-discussing-this-paper">Why discussing this paper?</a></li>
  <li><a href="#context" id="toc-context" class="nav-link" data-scroll-target="#context">Context</a></li>
  <li><a href="#prior-work" id="toc-prior-work" class="nav-link" data-scroll-target="#prior-work">Prior Work</a>
  <ul class="collapse">
  <li><a href="#scicode-a-research-coding-benchmark-curated-by-scientists" id="toc-scicode-a-research-coding-benchmark-curated-by-scientists" class="nav-link" data-scroll-target="#scicode-a-research-coding-benchmark-curated-by-scientists">SciCode: A Research Coding Benchmark Curated by Scientists</a></li>
  <li><a href="#mlagentbench-evaluating-language-agents-on-machine-learning-experimentation" id="toc-mlagentbench-evaluating-language-agents-on-machine-learning-experimentation" class="nav-link" data-scroll-target="#mlagentbench-evaluating-language-agents-on-machine-learning-experimentation">MLAgentBench: Evaluating Language Agents on Machine Learning Experimentation</a></li>
  <li><a href="#mle-bench-evaluating-machine-learning-agents-on-machine-learning-engineering" id="toc-mle-bench-evaluating-machine-learning-agents-on-machine-learning-engineering" class="nav-link" data-scroll-target="#mle-bench-evaluating-machine-learning-agents-on-machine-learning-engineering">MLE-Bench: Evaluating Machine Learning Agents on Machine Learning Engineering</a></li>
  <li><a href="#core-bench" id="toc-core-bench" class="nav-link" data-scroll-target="#core-bench">Core-Bench</a></li>
  </ul></li>
  <li><a href="#paperbench-problem-setting" id="toc-paperbench-problem-setting" class="nav-link" data-scroll-target="#paperbench-problem-setting">PaperBench problem setting</a></li>
  <li><a href="#approach" id="toc-approach" class="nav-link" data-scroll-target="#approach">Approach</a>
  <ul class="collapse">
  <li><a href="#overall-workflow" id="toc-overall-workflow" class="nav-link" data-scroll-target="#overall-workflow">Overall workflow</a></li>
  <li><a href="#rules" id="toc-rules" class="nav-link" data-scroll-target="#rules">Rules</a></li>
  <li><a href="#dev-paperbench" id="toc-dev-paperbench" class="nav-link" data-scroll-target="#dev-paperbench">Dev PaperBench</a></li>
  <li><a href="#llm-judge" id="toc-llm-judge" class="nav-link" data-scroll-target="#llm-judge">LLM Judge</a></li>
  </ul></li>
  <li><a href="#results" id="toc-results" class="nav-link" data-scroll-target="#results">Results</a>
  <ul class="collapse">
  <li><a href="#human-baseline-performance" id="toc-human-baseline-performance" class="nav-link" data-scroll-target="#human-baseline-performance">Human Baseline Performance</a></li>
  </ul></li>
  <li><a href="#takeaways" id="toc-takeaways" class="nav-link" data-scroll-target="#takeaways">Takeaways</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<section id="why-discussing-this-paper" class="level2">
<h2 class="anchored" data-anchor-id="why-discussing-this-paper">Why discussing this paper?</h2>
<p>I chose PaperBench from OpenAI <span class="citation" data-cites="starace2025paperbench0">(<a href="#ref-starace2025paperbench0" role="doc-biblioref">Starace et al. 2025</a>)</span> because:</p>
<ul>
<li>It is a recent paper that proposes a new benchmark for evaluating LLM-powered agents that work in developing AI research.</li>
<li>Specifically, the task of reproducing existing work should be the first milestone in the development of LLM-powered agents that can autonomously conduct research.<br>
</li>
<li>They use LLM-as-Judge for evaluating some of the tasks.</li>
</ul>
</section>
<section id="context" class="level2">
<h2 class="anchored" data-anchor-id="context">Context</h2>
<p>Agents are becoming increasingly capable of performing complex tasks, including some sparks of autonomous independent research <span class="citation" data-cites="lu2024ai">(<a href="#ref-lu2024ai" role="doc-biblioref">Lu et al. 2024</a>)</span>. However, perhaps one of the first steps in AI and coding research is the ability to reproduce existing work. This is a crucial step in validating and building upon previous research findings. While there are many benchmarks for evaluating LLMs in various domains, there are few that focus specifically on the task of reproducing AI research, and more specifically, only having access to the paper.</p>
</section>
<section id="prior-work" class="level2">
<h2 class="anchored" data-anchor-id="prior-work">Prior Work</h2>
<section id="scicode-a-research-coding-benchmark-curated-by-scientists" class="level3">
<h3 class="anchored" data-anchor-id="scicode-a-research-coding-benchmark-curated-by-scientists">SciCode: A Research Coding Benchmark Curated by Scientists</h3>
<p>SciCode <span class="citation" data-cites="tian2024scicode0">(<a href="#ref-tian2024scicode0" role="doc-biblioref">Tian et al. 2024</a>)</span> is a benchmark designed to evaluate the ability of LLMs to reproduce scientific code. It consists of a set of tasks based on reproducing AI scientific scripts. Interestingly, the scripts are not synthetic, but rather real-world scripts that have been used in actual research papers. The benchmark is curated by scientists and includes a diverse set of tasks from various domains, including biology, chemistry, and physics.</p>
<p>The benchmark is very well designed since each of the scripts is divided into subtasks, which allows for a more detailed evaluation of the agents. The agents are provided only with the task and the different docstrings explaining the subtasks. The benchmark is designed to be challenging, with a focus on evaluating the ability of LLMs to understand and reproduce complex scientific code. The authors also introduce a novel evaluation framework that uses LLMs-as-Judge to assess the quality of the agent’s output.</p>
<div id="fig-id:paperbench_figure_1" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-id:paperbench_figure_1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="paper_bench_images/figure_1.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-id:paperbench_figure_1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;30.1: Figure taken from SciCode <span class="citation" data-cites="tian2024scicode0">(<a href="#ref-tian2024scicode0" role="doc-biblioref">Tian et al. 2024</a>)</span> illustrating how the main task and subtasks are described to the agents.
</figcaption>
</figure>
</div>
<p>Interestingly, by the time of the paper, the authors had already evaluated several LLMs, including OpenAI’s GPT-4o and Anthropic’s Claude 3.5 Sonnet. The results show that the models struggle in general to reproduce the scripts, with a success rate lower than 5% for all models for the main tasks, and around 25% the best results for reproducing subtasks.</p>
</section>
<section id="mlagentbench-evaluating-language-agents-on-machine-learning-experimentation" class="level3">
<h3 class="anchored" data-anchor-id="mlagentbench-evaluating-language-agents-on-machine-learning-experimentation">MLAgentBench: Evaluating Language Agents on Machine Learning Experimentation</h3>
<p>MLAgentBench <span class="citation" data-cites="huang2023mlagentbench0">(<a href="#ref-huang2023mlagentbench0" role="doc-biblioref">Huang et al. 2023</a>)</span> is a benchmark designed to evaluate the ability of LLM-powered agents to reproduce machine learning experiments. The benchmark consists of a set of tasks that expands around Kaggle competitions and other ML-related tasks. Each of the tasks includes their own evaluator, which can be checking if the Kaggle submission is correct or if the proposed code improves the predictive performance of the model. <img src="paper_bench_images/figure_2.png" id="fig-id:paperbench_figure_2" class="img-fluid" alt="Figure taken from MLAgentBench (Huang et al. 2023) illustrating the different tasks in the benchmark."></p>
<p>For each of the tasks they provide the agents with some initial files and a task description and the agent has to build from there. One of the problems of this paper is that they only evaluated one agent (only ReAct) and only thirteen tasks which limits the evaluation. <img src="paper_bench_images/figure_3.png" id="fig-id:paperbench_figure_3" class="img-fluid" alt="Figure taken from MLAgentBench (Huang et al. 2023) illustrating the results for the different tasks in the benchmark."></p>
<p>The results vary a lot between the different tasks despite being averaged over 8 trials (pass@k). While for some the score is perfect, for others the score is 0.</p>
</section>
<section id="mle-bench-evaluating-machine-learning-agents-on-machine-learning-engineering" class="level3">
<h3 class="anchored" data-anchor-id="mle-bench-evaluating-machine-learning-agents-on-machine-learning-engineering">MLE-Bench: Evaluating Machine Learning Agents on Machine Learning Engineering</h3>
<p>Chen et al. <span class="citation" data-cites="chan2024mle0bench0">(<a href="#ref-chan2024mle0bench0" role="doc-biblioref">Chan et al. 2024</a>)</span> in previous work from OpenAI focused in the Kaggle competition as the origin of tasks, and curate 75 machine-learning related tasks from there. To improve the evaluation, they also run the human baseline that helps to evaluate the performance of the agents. Similarly as the proposed in MLAgentBench for the Kaggle tasks, the agents need to provide a submission according to the Kaggle rules. Thus, each of the tasks provides the agents and humans with the description of the task and the competition dataset. Interestingly, the authors also use a source code plagiarism detection tool to check that the code that either the models or the humans submitted is not plagiarized from the original Kaggle competition.</p>
<div id="tbl-id:paperbench_table_1" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-id:paperbench_table_1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;30.1: Accuracy in submitting and winning a medal (top submission)
</figcaption>
<div aria-describedby="tbl-id:paperbench_table_1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 24%">
<col style="width: 21%">
<col style="width: 20%">
<col style="width: 16%">
<col style="width: 17%">
</colgroup>
<thead>
<tr class="header">
<th>Model</th>
<th>Made Submission (%)</th>
<th>Valid Submission (%)</th>
<th>Gold (%)</th>
<th>Any Medal (%)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>AIDE</strong></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>o1-preview</td>
<td><strong>98.4 ± 0.4</strong></td>
<td><strong>82.8 ± 1.1</strong></td>
<td><strong>9.4 ± 0.8</strong></td>
<td><strong>16.9 ± 1.1</strong></td>
</tr>
<tr class="odd">
<td>gpt-4o-2024-08-06</td>
<td>70.7 ± 0.9</td>
<td>54.9 ± 1.0</td>
<td>5.0 ± 0.4</td>
<td>8.7 ± 0.5</td>
</tr>
<tr class="even">
<td>llama-3.1-405b-instruct</td>
<td>46.3 ± 2.9</td>
<td>27.3 ± 2.6</td>
<td>1.7 ± 0.7</td>
<td>3.0 ± 1.0</td>
</tr>
<tr class="odd">
<td>claude-3.5-sonnet-20240620</td>
<td>68.9 ± 3.1</td>
<td>51.1 ± 3.3</td>
<td>4.4 ± 1.4</td>
<td>7.6 ± 1.8</td>
</tr>
<tr class="even">
<td><strong>MLAB</strong></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>gpt-4o-2024-08-06</td>
<td>65.6 ± 2.5</td>
<td>44.3 ± 2.6</td>
<td>0.8 ± 0.5</td>
<td>0.8 ± 0.5</td>
</tr>
<tr class="even">
<td><strong>OpenHands</strong></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>gpt-4o-2024-08-06</td>
<td>59.1 ± 3.3</td>
<td>52.0 ± 3.3</td>
<td>2.7 ± 1.1</td>
<td>4.4 ± 1.4</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>The results show that the models can prepare a correct submission in a good amount of times. However, of all of these times, only a small fraction—around 15% for the best agent—win a medal in the competition. While this results show promising results, still they struggle to produce high-quality submissions.</p>
</section>
<section id="core-bench" class="level3">
<h3 class="anchored" data-anchor-id="core-bench">Core-Bench</h3>
<p>Core-Bench is a benchmark designed to evaluate the ability of LLM-powered agents to reproduce scientific results. The benchmark consists of a set of tasks that require the agent to read and understand a research paper and its codebase, extract relevant information, and implement the proposed methods.</p>
<div id="fig-id:paperbench_figure_4" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-id:paperbench_figure_4-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="paper_bench_images/figure_4.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-id:paperbench_figure_4-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;30.2: Figure taken from Core Bench <span class="citation" data-cites="siegel2024corebenchfosteringcredibilitypublished">(<a href="#ref-siegel2024corebenchfosteringcredibilitypublished" role="doc-biblioref">Siegel et al. 2024</a>)</span> illustrating how each task is organized.
</figcaption>
</figure>
</div>
<p>For a more in depth evaluation, they divided the benchmark by levels of difficulty in easy, medium, and hard. The differences between the different levels are the amount of information provided to the agent. For the easy level, the agent is provided with the results of executing the code. For the medium level, the agent is provided with a dockerfile that can be used to reproduce the results. Finally, for the hard level, only a README file is provided and the agent has to create and run the dockerfile by itself.</p>
<p>Core Bench design allows the agents to have access to the codebases. Thus the agents can read the code and understand how it works resulting in a benchmark that is more focused in code understanding that in code reproduction. To focus on the second Starace et al.&nbsp;proposed PaperBench <span class="citation" data-cites="starace2025paperbench0">(<a href="#ref-starace2025paperbench0" role="doc-biblioref">Starace et al. 2025</a>)</span>.</p>
<p>Refer to <span class="citation" data-cites="siegel2024corebenchfosteringcredibilitypublished">Siegel et al. (<a href="#ref-siegel2024corebenchfosteringcredibilitypublished" role="doc-biblioref">2024</a>)</span> for further discussion on Core Bench.</p>
</section>
</section>
<section id="paperbench-problem-setting" class="level2">
<h2 class="anchored" data-anchor-id="paperbench-problem-setting">PaperBench problem setting</h2>
<ul>
<li>Reproducing existing work can be a challenging and interesting task for LLM-based agents.</li>
<li>Grading the replication attemps can be really time-consuming.</li>
<li>At the same time, if you use an LLM to grade, how do you evaluate the LLM?</li>
</ul>
</section>
<section id="approach" class="level2">
<h2 class="anchored" data-anchor-id="approach">Approach</h2>
<p>The authors compile a dataset of 20 articles published as Spotlight and Oral papers from ICML 2024. For each of the papers they compile a set of rubrics that are used to evaluate the replication attempts. Each of the rubrics is structured as a hierarchical tree, with the top-level nodes representing the main categories of evaluation and the lower-level or leaf-nodes nodes representing the specific criteria within each category. This decomposition allows to do a very robust scoring which allows a more insightful evaluation of the agents. <img src="paper_bench_images/figure_5.png" id="fig-id:paperbench_figure_5" class="img-fluid" alt="Figure taken from PaperBench (Starace et al. 2025) summarizing all tasks original article, topic and the number of nodes in their evaluation rubric."></p>
<section id="overall-workflow" class="level3">
<h3 class="anchored" data-anchor-id="overall-workflow">Overall workflow</h3>
<p>For each of the tasks, the agents are given with the original paper to replicate and some clarifications compiled by the authors of the paper to better understand what was done in the paper. To avoid reward hacking, the agents are not allowed to see the rubric used to evaluate the replication attempts. The agents are asked to produce a <em>reproduce.sh</em> script that when run reproduces the results of the paper. To check if the results are correctly reproduced, the produced script is run in a fresh environment in a virtual machine with access to one GPU. The agent is not constrained in time and it has $1000 of OpenAI credits and a HuggingFace API key.</p>
<div id="tbl-id:paperbench_table_2" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-id:paperbench_table_2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;30.2: Evaluation requirements for each file type in PaperBench
</figcaption>
<div aria-describedby="tbl-id:paperbench_table_2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<thead>
<tr class="header">
<th></th>
<th>Code Dev.</th>
<th>Execution</th>
<th>Res. Match</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>READMEs &amp; Docs</td>
<td>✓</td>
<td>✓</td>
<td>✓</td>
</tr>
<tr class="even">
<td>Source code</td>
<td>✓</td>
<td>✓</td>
<td>✗</td>
</tr>
<tr class="odd">
<td>reproduce.sh</td>
<td>✓</td>
<td>✓</td>
<td>✓</td>
</tr>
<tr class="even">
<td>reproduce.log</td>
<td>✗</td>
<td>✓</td>
<td>✓</td>
</tr>
<tr class="odd">
<td>Repro outputs</td>
<td>✗</td>
<td>✗</td>
<td>✓</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>The grading for each of the tasks is done by evaluating each of the leaf nodes of the rubric. Depending on the leaf node, there are different evaluation requirements:</p>
<ul>
<li><strong>Result Match</strong>: in these leaf nodes is checked if the specific results are reproduced.</li>
<li><strong>Execution</strong>: if the script is executable and runs without errors.</li>
<li><strong>Code Development</strong>: checks if the code seems to can reproduce the results.</li>
</ul>
<div id="fig-id:paperbench_figure_6" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-id:paperbench_figure_6-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="paper_bench_images/figure_6.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-id:paperbench_figure_6-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;30.3: Figure from PaperBench <span class="citation" data-cites="starace2025paperbench0">(<a href="#ref-starace2025paperbench0" role="doc-biblioref">Starace et al. 2025</a>)</span> explaining how the rubric evaluation is done.
</figcaption>
</figure>
</div>
<p>The evaluation following the rubric is done by checking each of the leaf nodes. The score in the leaf nodes is always binary, either 0 for not matching or 1 for matching. The score in the leaf nodes is then aggregated to the parent node, and so on until the root node resulting in very descriptive score for each of the tasks.</p>
</section>
<section id="rules" class="level3">
<h3 class="anchored" data-anchor-id="rules">Rules</h3>
<p>The agent is allowed to do almost anything, including:</p>
<ul>
<li>Browsing the web.</li>
<li>No computing constraints.</li>
</ul>
<p>However, there are two limitations. The first is all API keys need to be provided to the agents. Second, the agents, despite they can browse the web, have a black list of websites that they are not allowed to use. This is done to avoid the agents to check the original codebases of the papers or related replications.</p>
</section>
<section id="dev-paperbench" class="level3">
<h3 class="anchored" data-anchor-id="dev-paperbench">Dev PaperBench</h3>
<p>Since the entire benchmark is expensive to run, the authors also compile a smaller version of the benchmark called PaperBench Code-Dev which consideraly reduces the costs. However, the results in this benchmark are not directly comparable to the full PaperBench benchmark.</p>
</section>
<section id="llm-judge" class="level3">
<h3 class="anchored" data-anchor-id="llm-judge">LLM Judge</h3>
<p>Since they arrived at such large rubrics, the human evaluation became non-scalable during the first stages of the study. Therefore, they proposed to evaluate using LLMs as judges. However, for this evaluation to be robust, the accuracy needs to be assesed <span class="citation" data-cites="shankar2024validates">(<a href="#ref-shankar2024validates" role="doc-biblioref">Shankar et al. 2024</a>)</span>. To do so, they compile a set of partial modified replications of the articles, and manually graded them using the rubrics. The authors then use these scores to build <em>JudgeEval</em> which is a benchmark for evaluating the performance of LLMs as judges.</p>
<div id="tbl-id:paperbench_table_3" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-id:paperbench_table_3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;30.3: Results of the <em>JudgeEval</em> benchmark.
</figcaption>
<div aria-describedby="tbl-id:paperbench_table_3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<thead>
<tr class="header">
<th></th>
<th>ACC.</th>
<th>PREC.</th>
<th>REC.</th>
<th>F1</th>
<th>COST</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>RANDOM</strong></td>
<td>0.48</td>
<td>0.49</td>
<td>0.49</td>
<td>0.49</td>
<td>0</td>
</tr>
<tr class="even">
<td><strong>SIMPLEJUDGE</strong></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>GPT-4o-mini</td>
<td>0.63</td>
<td>0.64</td>
<td>0.60</td>
<td>0.59</td>
<td><strong>8</strong></td>
</tr>
<tr class="even">
<td>GPT-4o</td>
<td>0.74</td>
<td>0.74</td>
<td>0.72</td>
<td>0.73</td>
<td>120</td>
</tr>
<tr class="odd">
<td>o1-mini</td>
<td>0.81</td>
<td><strong>0.85</strong></td>
<td>0.76</td>
<td>0.78</td>
<td>72</td>
</tr>
<tr class="even">
<td>o1</td>
<td><strong>0.84</strong></td>
<td>0.84</td>
<td><strong>0.84</strong></td>
<td><strong>0.84</strong></td>
<td>830</td>
</tr>
<tr class="odd">
<td>o3-mini</td>
<td>0.83</td>
<td>0.83</td>
<td>0.83</td>
<td>0.83</td>
<td>66</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>By evaluating different leading LLMs, the authors show that the performance of the LLMs as judges is good and they can be trusted as evaluators. The best model in terms of accuracy versus cost is the o3-mini model. However, despite this being the best model, the authors report a cost of $66 per task. While the cost reduces to only $10 in the PaperBench Code-Dev benchmark, this is still a very expensive evaluation.</p>
</section>
</section>
<section id="results" class="level2">
<h2 class="anchored" data-anchor-id="results">Results</h2>
<p>They evaluate the performance of some of the leading models using a simple agent scaffolding that uses the available tools until the agent decides to end the run. The tools available to the agent are: a bash shell command execution tool, a Python code execution tool, a web browser tool, and a paginated file reader tool for reading long documents.</p>
<div id="tbl-id:paperbench_table_4" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-id:paperbench_table_4-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;30.4: Average Replication Scores (%) of the evaluation of <em>PaperBench</em> benchmark.
</figcaption>
<div aria-describedby="tbl-id:paperbench_table_4-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<thead>
<tr class="header">
<th>Model</th>
<th>PaperBench</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>o3-mini-high</td>
<td>2.6 ± 0.2</td>
</tr>
<tr class="even">
<td>GPT-4o</td>
<td>4.1 ± 0.1</td>
</tr>
<tr class="odd">
<td>Gemini-2.0-Flash</td>
<td>3.2 ± 0.2</td>
</tr>
<tr class="even">
<td>DeepSeek-R1</td>
<td>6.0 ± 0.3</td>
</tr>
<tr class="odd">
<td>o1-high</td>
<td>13.2 ± 0.3</td>
</tr>
<tr class="even">
<td>Claude-3.5-Sonnet</td>
<td>21.0 ± 0.8</td>
</tr>
<tr class="odd">
<td><strong>IterativeAgent</strong></td>
<td></td>
</tr>
<tr class="even">
<td>o3-mini-high</td>
<td>8.5 ± 0.8</td>
</tr>
<tr class="odd">
<td>Claude-3.5-Sonnet</td>
<td>16.1 ± 0.1</td>
</tr>
<tr class="even">
<td>o1-high</td>
<td>24.4 ± 0.7</td>
</tr>
<tr class="odd">
<td><strong>IterativeAgent with an extended 36 hour limit</strong></td>
<td></td>
</tr>
<tr class="even">
<td>o1-high</td>
<td>24.4 ± 0.7</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>The results show that the best model is (ironically) Claude 3.5 Sonnet, with an average replication score of 21.0% in the basic agent scaffolding. The authors claim that by manual inspection of the log files, the most common failure case was due to early termination by the agents, while some other model such as o3-mini struggle with tool usage. These results show the weakness of the current LLMs in carrying long-horizon tasks.</p>
<p>Since in the basic agent implementation the models tend to finish prematurely, the authors propose the <em>IterativeAgent</em> which consists of forcing the agent to run the full available time by removing their ability to finishing earlier and uses prompts to encourage the agents to work in a step-based manner. By using this agent, while the scores decrease for Claude 3.5 Sonnet, o1-high model improved more than 10% that gets increased another 2% by extending the time limit to 36 hours.</p>
<section id="human-baseline-performance" class="level3">
<h3 class="anchored" data-anchor-id="human-baseline-performance">Human Baseline Performance</h3>
<p>The authors also evaluate the performance of humans in the benchmark. For that they recruited 8 PhDs in machine learning. To evaluate the participands, they provided them with the same resources as the agents, including the original paper and the clarifications, and access to the same computing resources. The humans were allowed to work during four weeks in the tasks using tracked hours to compare them with the agents.</p>
<div id="fig-id:paperbench_figure_7" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-id:paperbench_figure_7-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="paper_bench_images/figure_7.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-id:paperbench_figure_7-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;30.4: Figure from PaperBench <span class="citation" data-cites="starace2025paperbench0">(<a href="#ref-starace2025paperbench0" role="doc-biblioref">Starace et al. 2025</a>)</span> comparing humans and agents.
</figcaption>
</figure>
</div>
<p>The results show that while the agents clearly outperform the humans during the first hours, the humans are able to catch up and even outperform the agents after 24 hours. This is mainly due to the fact that the agents are not able to improve significantly their performance after the first hour of work.</p>
</section>
</section>
<section id="takeaways" class="level2">
<h2 class="anchored" data-anchor-id="takeaways">Takeaways</h2>
<ul>
<li>Current models still fail to follow long-horizon tasks.</li>
<li>The use of LLMs as judges is a promising approach to evaluate the performance of LLM-powered agents.</li>
<li>Reproducing existing work is a challenging task for LLM-powered agents (and can result in very high cost attemps).</li>
</ul>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-chan2024mle0bench0" class="csl-entry" role="listitem">
Chan, Jun Shern, Neil Chowdhury, Oliver Jaffe, James Aung, Dane Sherburn, Evan Mays, Giulio Starace, et al. 2024. <span>“MLE-Bench: Evaluating Machine Learning Agents on Machine Learning Engineering.”</span> <em>arXiv Preprint arXiv: 2410.07095</em>.
</div>
<div id="ref-ding_nafm_2025" class="csl-entry" role="listitem">
Ding, Yuheng, Yusong Wang, Bo Qiang, Jie Yu, Qi Li, Yiran Zhou, and Zhenmin Liu. 2025. <span>“<span>NaFM</span>: <span>Pre</span>-Training a <span>Foundation</span> <span>Model</span> for <span>Small</span>-<span>Molecule</span> <span>Natural</span> <span>Products</span>.”</span> <a href="https://doi.org/10.48550/ARXIV.2503.17656">https://doi.org/10.48550/ARXIV.2503.17656</a>.
</div>
<div id="ref-https://doi.org/10.48550/arxiv.2410.11527" class="csl-entry" role="listitem">
Guo, Jeff, and Philippe Schwaller. 2024. <span>“It Takes Two to Tango: Directly Optimizing for Constrained Synthesizability in Generative Molecular Design.”</span> arXiv. <a href="https://doi.org/10.48550/ARXIV.2410.11527">https://doi.org/10.48550/ARXIV.2410.11527</a>.
</div>
<div id="ref-huang2023mlagentbench0" class="csl-entry" role="listitem">
Huang, Qian, Jian Vora, Percy Liang, and J. Leskovec. 2023. <span>“MLAgentBench: Evaluating Language Agents on Machine Learning Experimentation.”</span> <em>International Conference on Machine Learning</em>.
</div>
<div id="ref-landrum_lwreg_2024" class="csl-entry" role="listitem">
Landrum, Gregory A., Jessica Braun, Paul Katzberger, Marc T. Lehner, and Sereina Riniker. 2024. <span>“Lwreg: <span>A</span> <span>Lightweight</span> <span>System</span> for <span>Chemical</span> <span>Registration</span> and <span>Data</span> <span>Storage</span>.”</span> <em>Journal of Chemical Information and Modeling</em> 64 (16): 6247–52. <a href="https://doi.org/10.1021/acs.jcim.4c01133">https://doi.org/10.1021/acs.jcim.4c01133</a>.
</div>
<div id="ref-lu2024ai" class="csl-entry" role="listitem">
Lu, Chris, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, and David Ha. 2024. <span>“The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery.”</span> <em>arXiv Preprint arXiv: 2408.06292</em>.
</div>
<div id="ref-orsi2024one" class="csl-entry" role="listitem">
Orsi, Markus, and Jean-Louis Reymond. 2024. <span>“One Chiral Fingerprint to Find Them All.”</span> <em>Journal of Cheminformatics</em> 16 (1): 53.
</div>
<div id="ref-shankar2024validates" class="csl-entry" role="listitem">
Shankar, Shreya, J. D. Zamfirescu-Pereira, Bjorn Hartmann, Aditya G. Parameswaran, and Ian Arawjo. 2024. <span>“Who Validates the Validators? Aligning LLM-Assisted Evaluation of LLM Outputs with Human Preferences.”</span> <em>ACM Symposium on User Interface Software and Technology</em>. <a href="https://doi.org/10.48550/arXiv.2404.12272">https://doi.org/10.48550/arXiv.2404.12272</a>.
</div>
<div id="ref-siegel2024corebenchfosteringcredibilitypublished" class="csl-entry" role="listitem">
Siegel, Zachary S., Sayash Kapoor, Nitya Nagdir, Benedikt Stroebl, and Arvind Narayanan. 2024. <span>“CORE-Bench: Fostering the Credibility of Published Research Through a Computational Reproducibility Agent Benchmark.”</span> <a href="https://arxiv.org/abs/2409.11363">https://arxiv.org/abs/2409.11363</a>.
</div>
<div id="ref-starace2025paperbench0" class="csl-entry" role="listitem">
Starace, Giulio, Oliver Jaffe, Dane Sherburn, James Aung, Jun Shern Chan, Leon Maksin, Rachel Dias, et al. 2025. <span>“PaperBench: Evaluating AI’s Ability to Replicate AI Research.”</span> <em>arXiv Preprint arXiv: 2504.01848</em>.
</div>
<div id="ref-tian2024scicode0" class="csl-entry" role="listitem">
Tian, Minyang, Luyu Gao, Shizhuo Dylan Zhang, Xinan Chen, Cunwei Fan, Xuefei Guo, Roland Haas, et al. 2024. <span>“SciCode: A Research Coding Benchmark Curated by Scientists.”</span> <em>Neural Information Processing Systems</em>. <a href="https://doi.org/10.48550/arXiv.2407.13168">https://doi.org/10.48550/arXiv.2407.13168</a>.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../papers/llm_confidence_2025.html" class="pagination-link" aria-label="What large language models know and what people think they know">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-title">What large language models know and what people think they know</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../papers/NaFM_2025.html" class="pagination-link" aria-label="NaFM: Pre-training a Foundation Model for Small-Molecule Natural Products [@ding_nafm_2025]">
        <span class="nav-page-text"><span class="chapter-title">NaFM: Pre-training a Foundation Model for Small-Molecule Natural Products <span class="citation" data-cites="ding_nafm_2025">(</span></span></span></a><a href="#ref-ding_nafm_2025" role="doc-biblioref">Ding et al. 2025</a>) <i class="bi bi-arrow-right-short"></i>
      
  </div>
</nav>
</div> <!-- /content -->




</body></html>